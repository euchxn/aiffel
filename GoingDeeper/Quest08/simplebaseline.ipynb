{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gE5LeVc5ORa",
        "outputId": "68df7176-cea2-4643-ac94-b20086fb1eaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-03 06:08:22--  https://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz\n",
            "Resolving datasets.d2.mpi-inf.mpg.de (datasets.d2.mpi-inf.mpg.de)... 139.19.206.177\n",
            "Connecting to datasets.d2.mpi-inf.mpg.de (datasets.d2.mpi-inf.mpg.de)|139.19.206.177|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12088943206 (11G) [application/x-gzip]\n",
            "Saving to: ‘/content/work/mpii/mpii_human_pose_v1.tar.gz’\n",
            "\n",
            "mpii_human_pose_v1. 100%[===================>]  11.26G  24.3MB/s    in 9m 20s  \n",
            "\n",
            "2025-12-03 06:17:43 (20.6 MB/s) - ‘/content/work/mpii/mpii_human_pose_v1.tar.gz’ saved [12088943206/12088943206]\n",
            "\n",
            "--2025-12-03 06:17:43--  https://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1_u12_2.zip\n",
            "Resolving datasets.d2.mpi-inf.mpg.de (datasets.d2.mpi-inf.mpg.de)... 139.19.206.177\n",
            "Connecting to datasets.d2.mpi-inf.mpg.de (datasets.d2.mpi-inf.mpg.de)|139.19.206.177|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12340483 (12M) [application/zip]\n",
            "Saving to: ‘/content/work/mpii/mpii_human_pose_v1_u12_2.zip’\n",
            "\n",
            "mpii_human_pose_v1_ 100%[===================>]  11.77M  7.89MB/s    in 1.5s    \n",
            "\n",
            "2025-12-03 06:17:45 (7.89 MB/s) - ‘/content/work/mpii/mpii_human_pose_v1_u12_2.zip’ saved [12340483/12340483]\n",
            "\n",
            "다운로드 및 압축 해제가 완료되었습니다.\n"
          ]
        }
      ],
      "source": [
        "# 1. 경로를 명확하게 지정 (Colab의 기본 폴더는 /content 입니다)\n",
        "base_path = '/content/work/mpii'\n",
        "\n",
        "# 폴더가 혹시 없으면 생성 (에러 방지)\n",
        "!mkdir -p {base_path}\n",
        "\n",
        "# 2. 데이터셋 다운로드 (경로를 base_path로 변경)\n",
        "!wget https://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz -P {base_path}\n",
        "!wget https://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1_u12_2.zip -P {base_path}\n",
        "\n",
        "# 3. 압축 해제\n",
        "# tar 압축 해제\n",
        "!tar -xf {base_path}/mpii_human_pose_v1.tar.gz -C {base_path}\n",
        "\n",
        "# zip 압축 해제\n",
        "!unzip -q {base_path}/mpii_human_pose_v1_u12_2.zip -d {base_path}\n",
        "\n",
        "print(\"다운로드 및 압축 해제가 완료되었습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일이 저장될 목표 경로 설정 (/content로 시작하는 절대 경로)\n",
        "target_path = '/content/work/mpii/mpii_human_pose_v1_u12_2'\n",
        "\n",
        "# 폴더가 없을 경우를 대비해 생성 (이미 있다면 무시됨)\n",
        "!mkdir -p {target_path}\n",
        "\n",
        "# -P 옵션을 사용하여 다운로드와 동시에 해당 폴더로 저장합니다. (mv 불필요)\n",
        "!wget https://d3s0tskafalll9.cloudfront.net/media/documents/train.json -P {target_path}\n",
        "!wget https://d3s0tskafalll9.cloudfront.net/media/documents/validation.json -P {target_path}\n",
        "\n",
        "print(f\"{target_path} 경로에 json 파일 다운로드가 완료되었습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qK-ZQvhw_OWF",
        "outputId": "e8bfa900-f493-482f-edad-08c770df8ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-03 06:26:36--  https://d3s0tskafalll9.cloudfront.net/media/documents/train.json\n",
            "Resolving d3s0tskafalll9.cloudfront.net (d3s0tskafalll9.cloudfront.net)... 3.165.160.100, 3.165.160.58, 3.165.160.93, ...\n",
            "Connecting to d3s0tskafalll9.cloudfront.net (d3s0tskafalll9.cloudfront.net)|3.165.160.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31043010 (30M) [application/json]\n",
            "Saving to: ‘/content/work/mpii/mpii_human_pose_v1_u12_2/train.json’\n",
            "\n",
            "train.json          100%[===================>]  29.60M  16.7MB/s    in 1.8s    \n",
            "\n",
            "2025-12-03 06:26:38 (16.7 MB/s) - ‘/content/work/mpii/mpii_human_pose_v1_u12_2/train.json’ saved [31043010/31043010]\n",
            "\n",
            "--2025-12-03 06:26:38--  https://d3s0tskafalll9.cloudfront.net/media/documents/validation.json\n",
            "Resolving d3s0tskafalll9.cloudfront.net (d3s0tskafalll9.cloudfront.net)... 3.165.160.100, 3.165.160.58, 3.165.160.93, ...\n",
            "Connecting to d3s0tskafalll9.cloudfront.net (d3s0tskafalll9.cloudfront.net)|3.165.160.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4127396 (3.9M) [application/json]\n",
            "Saving to: ‘/content/work/mpii/mpii_human_pose_v1_u12_2/validation.json’\n",
            "\n",
            "validation.json     100%[===================>]   3.94M  4.50MB/s    in 0.9s    \n",
            "\n",
            "2025-12-03 06:26:40 (4.50 MB/s) - ‘/content/work/mpii/mpii_human_pose_v1_u12_2/validation.json’ saved [4127396/4127396]\n",
            "\n",
            "/content/work/mpii/mpii_human_pose_v1_u12_2 경로에 json 파일 다운로드가 완료되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 압축 파일 다운로드\n",
        "# -O 옵션을 사용해 /content 폴더 바로 아래에 'mpii.zip'이라는 이름으로 저장합니다.\n",
        "!wget https://d3s0tskafalll9.cloudfront.net/media/documents/mpii_gtuV0hd.zip -O /content/mpii.zip\n",
        "\n",
        "# 2. 압축 해제\n",
        "# 다운로드 받은 파일을 찾아서(/content/mpii.zip), 목표 폴더(/content/work/mpii)에 풉니다.\n",
        "!unzip -q /content/mpii.zip -d /content/work/mpii\n",
        "\n",
        "print(\"mpii.zip 다운로드 및 압축 해제가 완료되었습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_iv_Y3Z_Qh1",
        "outputId": "72e48a1d-9b55-46d9-ab3c-6760cdd13b71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-03 06:27:39--  https://d3s0tskafalll9.cloudfront.net/media/documents/mpii_gtuV0hd.zip\n",
            "Resolving d3s0tskafalll9.cloudfront.net (d3s0tskafalll9.cloudfront.net)... 3.165.160.58, 3.165.160.93, 3.165.160.100, ...\n",
            "Connecting to d3s0tskafalll9.cloudfront.net (d3s0tskafalll9.cloudfront.net)|3.165.160.58|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12488 (12K) [application/zip]\n",
            "Saving to: ‘/content/mpii.zip’\n",
            "\n",
            "/content/mpii.zip   100%[===================>]  12.20K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-12-03 06:27:40 (295 MB/s) - ‘/content/mpii.zip’ saved [12488/12488]\n",
            "\n",
            "mpii.zip 다운로드 및 압축 해제가 완료되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ray"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy3mzdKNAETT",
        "outputId": "fe2600f9-60c0-497c-8c40-b3b9b055fe87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ray\n",
            "  Downloading ray-2.52.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting click!=8.3.*,>=7.0 (from ray)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from ray) (3.20.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from ray) (4.25.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray) (1.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ray) (25.0)\n",
            "Requirement already satisfied: protobuf>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from ray) (5.29.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from ray) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from ray) (2.32.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray) (0.29.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->ray) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->ray) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->ray) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->ray) (2025.11.12)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema->ray) (4.15.0)\n",
            "Downloading ray-2.52.1-cp312-cp312-manylinux2014_x86_64.whl (72.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.3/72.3 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: click, ray\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.1\n",
            "    Uninstalling click-8.3.1:\n",
            "      Successfully uninstalled click-8.3.1\n",
            "Successfully installed click-8.2.1 ray-2.52.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 주의! ray를 pytorch보다 먼저 import하면 오류가 발생할 수 있습니다\n",
        "import io, json, os, math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import ray\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 프로젝트 경로 설정\n",
        "# os.getenv(\"HOME\") 대신 '/content'를 직접 지정합니다.\n",
        "PROJECT_PATH = '/content/work/mpii'\n",
        "\n",
        "IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n",
        "MODEL_PATH = os.path.join(PROJECT_PATH, 'models')\n",
        "PT_RECORD_PATH = os.path.join(PROJECT_PATH, 'ptrecords_mpii')\n",
        "\n",
        "# 데이터셋 압축 풀 때 생성된 폴더명(mpii_human_pose_v1_u12_2)과 일치해야 합니다.\n",
        "TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n",
        "VALID_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')\n",
        "\n",
        "print('슝=3')\n",
        "\n",
        "# (선택 사항) 경로가 올바르게 잡혔는지 확인용 출력\n",
        "print(f\"Project Path: {PROJECT_PATH}\")\n",
        "print(f\"Train JSON Path: {TRAIN_JSON}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pkt6kq06AHhg",
        "outputId": "c47169ee-7efa-47af-b0cf-aa1dce43b1d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n",
            "Project Path: /content/work/mpii\n",
            "Train JSON Path: /content/work/mpii/mpii_human_pose_v1_u12_2/train.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# TRAIN_JSON 변수를 target_path를 사용하여 올바른 경로로 재정의합니다.\n",
        "# target_path는 이미 /content/work/mpii/mpii_human_pose_v1_u12_2로 정의되어 있습니다.\n",
        "TRAIN_JSON = os.path.join(target_path, 'train.json')\n",
        "\n",
        "with open(TRAIN_JSON) as train_json:\n",
        "    train_annos = json.load(train_json)\n",
        "    json_formatted_str = json.dumps(train_annos[0], indent=2)\n",
        "    print(json_formatted_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqU63A74ANFp",
        "outputId": "8db5c796-fbd1-4bb9-ccd0-10f056275126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"joints_vis\": [\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"joints\": [\n",
            "    [\n",
            "      620.0,\n",
            "      394.0\n",
            "    ],\n",
            "    [\n",
            "      616.0,\n",
            "      269.0\n",
            "    ],\n",
            "    [\n",
            "      573.0,\n",
            "      185.0\n",
            "    ],\n",
            "    [\n",
            "      647.0,\n",
            "      188.0\n",
            "    ],\n",
            "    [\n",
            "      661.0,\n",
            "      221.0\n",
            "    ],\n",
            "    [\n",
            "      656.0,\n",
            "      231.0\n",
            "    ],\n",
            "    [\n",
            "      610.0,\n",
            "      187.0\n",
            "    ],\n",
            "    [\n",
            "      647.0,\n",
            "      176.0\n",
            "    ],\n",
            "    [\n",
            "      637.0201,\n",
            "      189.8183\n",
            "    ],\n",
            "    [\n",
            "      695.9799,\n",
            "      108.1817\n",
            "    ],\n",
            "    [\n",
            "      606.0,\n",
            "      217.0\n",
            "    ],\n",
            "    [\n",
            "      553.0,\n",
            "      161.0\n",
            "    ],\n",
            "    [\n",
            "      601.0,\n",
            "      167.0\n",
            "    ],\n",
            "    [\n",
            "      692.0,\n",
            "      185.0\n",
            "    ],\n",
            "    [\n",
            "      693.0,\n",
            "      240.0\n",
            "    ],\n",
            "    [\n",
            "      688.0,\n",
            "      313.0\n",
            "    ]\n",
            "  ],\n",
            "  \"image\": \"015601864.jpg\",\n",
            "  \"scale\": 3.021046,\n",
            "  \"center\": [\n",
            "    594.0,\n",
            "    257.0\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_one_annotation(anno, image_dir):\n",
        "    filename = anno['image']\n",
        "    joints = anno['joints']\n",
        "    joints_visibility = anno['joints_vis']\n",
        "    annotation = {\n",
        "        'filename': filename,\n",
        "        'filepath': os.path.join(image_dir, filename),\n",
        "        'joints_visibility': joints_visibility,\n",
        "        'joints': joints,\n",
        "        'center': anno['center'],\n",
        "        'scale' : anno['scale']\n",
        "    }\n",
        "    return annotation\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4qn9IBZArXb",
        "outputId": "a9a84b2f-0f4e-4f25-ee14-eb6141605cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(TRAIN_JSON) as train_json:\n",
        "    train_annos = json.load(train_json)\n",
        "    test = parse_one_annotation(train_annos[0], IMAGE_PATH)\n",
        "    print(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fR-fnTABGcW",
        "outputId": "31d49c4a-562b-487b-8ddb-164d9794e6b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'filename': '015601864.jpg', 'filepath': '/content/work/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ptexample(anno):\n",
        "    filename = anno['filename']\n",
        "    filepath = anno['filepath']\n",
        "\n",
        "    # 이미지 파일 읽기\n",
        "    with open(filepath, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    image = Image.open(filepath)\n",
        "    # JPEG 형식 및 RGB 모드가 아니면 변환\n",
        "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
        "        image_rgb = image.convert('RGB')\n",
        "        with io.BytesIO() as output:\n",
        "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
        "            content = output.getvalue()\n",
        "\n",
        "    width, height = image.size\n",
        "    depth = 3\n",
        "\n",
        "    c_x = int(anno['center'][0])\n",
        "    c_y = int(anno['center'][1])\n",
        "    scale = anno['scale']\n",
        "\n",
        "    x = [int(joint[0]) if joint[0] >= 0 else int(joint[0])\n",
        "         for joint in anno['joints']]\n",
        "    y = [int(joint[1]) if joint[1] >= 0 else int(joint[0])\n",
        "         for joint in anno['joints']]\n",
        "\n",
        "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
        "\n",
        "    feature = {\n",
        "        'image/height': height,\n",
        "        'image/width': width,\n",
        "        'image/depth': depth,\n",
        "        'image/object/parts/x': x,\n",
        "        'image/object/parts/y': y,\n",
        "        'image/object/center/x': c_x,\n",
        "        'image/object/center/y': c_y,\n",
        "        'image/object/scale': scale,\n",
        "        'image/object/parts/v': v,\n",
        "        'image/encoded': content,\n",
        "        'image/filename': filename.encode()  # bytes로 저장\n",
        "    }\n",
        "\n",
        "    return feature\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51s2mVekSgUU",
        "outputId": "37cf1a8c-02b7-427e-ba3e-e95550b57c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chunkify(l, n):\n",
        "    size = len(l) // n\n",
        "    start = 0\n",
        "    results = []\n",
        "    for i in range(n):\n",
        "        results.append(l[start:start + size])\n",
        "        start += size\n",
        "    return results\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuvxeQ8rSlv_",
        "outputId": "d1f83e4f-3e53-4058-8f41-509e5d7a93ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_chunks = chunkify([0] * 1000, 64)\n",
        "print(test_chunks)\n",
        "print(len(test_chunks))\n",
        "print(len(test_chunks[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx3CcfGhTKRt",
        "outputId": "8fe2fbd0-8219-4a25-c1b3-bacc646cc2eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "64\n",
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "@ray.remote\n",
        "def build_single_ptrecord(chunk, path):\n",
        "    print('start to build ptrecord for ' + path)\n",
        "\n",
        "    with open(path, 'wb') as writer:\n",
        "        for anno in chunk:\n",
        "            ptexample = generate_ptexample(anno)\n",
        "            pickle.dump(ptexample, writer)\n",
        "\n",
        "    print('finished building ptrecord for ' + path)\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRC9pf05Tkvu",
        "outputId": "4f61734f-b62a-4704-d563-113637aeb6b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_pt_records(annotations, total_shards, split):\n",
        "    chunks = chunkify(annotations, total_shards)\n",
        "    futures = [\n",
        "        build_single_ptrecord.remote(\n",
        "            chunk, '{}/{}_{}_of_{}.ptrecords'.format(\n",
        "                PT_RECORD_PATH,\n",
        "                split,\n",
        "                str(i + 1).zfill(4),\n",
        "                str(total_shards).zfill(4),\n",
        "            )\n",
        "        ) for i, chunk in enumerate(chunks)\n",
        "    ]\n",
        "    ray.get(futures)\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swaGpucAToPa",
        "outputId": "03c5a11c-2b1f-475e-c3ae-1391478df720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MPIIDataset(Dataset):\n",
        "    def __init__(self, annotation_file, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # JSON 파일을 읽어 annotations 리스트 생성\n",
        "        with open(annotation_file, 'r') as f:\n",
        "            annotations = json.load(f)\n",
        "\n",
        "        # 각 annotation을 파싱하여 리스트에 저장\n",
        "        self.annotations = [parse_one_annotation(anno, image_dir) for anno in annotations]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anno = self.annotations[idx]\n",
        "        # 이미지 파일 경로로부터 이미지를 로드 (RGB 모드로 변환)\n",
        "        image = Image.open(anno['filepath']).convert('RGB')\n",
        "\n",
        "        # transform이 있으면 적용\n",
        "        if self.transform:\n",
        "            image, heatmaps = self.transform({'image': image, 'annotation': anno})\n",
        "            return image, heatmaps\n",
        "        else:\n",
        "            # transform이 없으면 원본 이미지와 annotation dict 반환\n",
        "            return image, anno"
      ],
      "metadata": {
        "id": "AUgovoXATrPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(os.path.join(PT_RECORD_PATH, 'train'), exist_ok=True)\n",
        "os.makedirs(os.path.join(PT_RECORD_PATH, 'val'), exist_ok=True)"
      ],
      "metadata": {
        "id": "MvDZQxmErizg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "num_train_shards = 64\n",
        "num_val_shards = 8\n",
        "\n",
        "ray.init()\n",
        "\n",
        "print('Start to parse annotations.')\n",
        "if not os.path.exists(PT_RECORD_PATH):\n",
        "    os.makedirs(PT_RECORD_PATH)\n",
        "\n",
        "with open(TRAIN_JSON) as train_json:\n",
        "    train_annos = json.load(train_json)\n",
        "    train_annotations = [\n",
        "        parse_one_annotation(anno, IMAGE_PATH)\n",
        "        for anno in train_annos\n",
        "    ]\n",
        "    print('First train annotation: ', train_annotations[0])\n",
        "\n",
        "with open(VALID_JSON) as val_json:\n",
        "    val_annos = json.load(val_json)\n",
        "    val_annotations = [\n",
        "        parse_one_annotation(anno, IMAGE_PATH)\n",
        "        for anno in val_annos\n",
        "    ]\n",
        "    print('First val annotation: ', val_annotations[0])\n",
        "\n",
        "print('Start to build PT Records.')\n",
        "build_pt_records(train_annotations, num_train_shards, 'train')\n",
        "build_pt_records(val_annotations, num_val_shards, 'val')\n",
        "\n",
        "print('Successfully wrote {} annotations to PT Records.'.format(\n",
        "    len(train_annotations) + len(val_annotations)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_E_CbFkNrnZT",
        "outputId": "d3bf2e8c-f7c0-4dc1-99f2-121b615caa0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-12-03 06:31:07,255\tINFO worker.py:2023 -- Started a local Ray instance.\n",
            "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py:2062: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start to parse annotations.\n",
            "First train annotation:  {'filename': '015601864.jpg', 'filepath': '/content/work/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n",
            "First val annotation:  {'filename': '005808361.jpg', 'filepath': '/content/work/mpii/images/005808361.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[804.0, 711.0], [816.0, 510.0], [908.0, 438.0], [1040.0, 454.0], [906.0, 528.0], [883.0, 707.0], [974.0, 446.0], [985.0, 253.0], [982.7591, 235.9694], [962.2409, 80.0306], [869.0, 214.0], [798.0, 340.0], [902.0, 253.0], [1067.0, 253.0], [1167.0, 353.0], [1142.0, 478.0]], 'center': [966.0, 340.0], 'scale': 4.718488}\n",
            "Start to build PT Records.\n",
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0001_of_0064.ptrecords\n",
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/train_0001_of_0064.ptrecords\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0006_of_0064.ptrecords\u001b[32m [repeated 5x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/train_0008_of_0064.ptrecords\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0010_of_0064.ptrecords\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(pid=gcs_server)\u001b[0m [2025-12-03 06:31:32,020 E 5993 5993] (gcs_server) gcs_server.cc:303: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/train_0011_of_0064.ptrecords\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0012_of_0064.ptrecords\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m(raylet)\u001b[0m [2025-12-03 06:31:38,017 E 6092 6092] (raylet) main.cc:979: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/train_0015_of_0064.ptrecords\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0016_of_0064.ptrecords\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m [2025-12-03 06:31:46,025 E 6130 6235] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/train_0018_of_0064.ptrecords\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0019_of_0064.ptrecords\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/train_0020_of_0064.ptrecords\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0021_of_0064.ptrecords\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/train_0023_of_0064.ptrecords\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0025_of_0064.ptrecords\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/train_0025_of_0064.ptrecords\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0026_of_0064.ptrecords\n",
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0027_of_0064.ptrecords\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/train_0026_of_0064.ptrecords\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0030_of_0064.ptrecords\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/train_0029_of_0064.ptrecords\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0033_of_0064.ptrecords\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/train_0033_of_0064.ptrecords\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0037_of_0064.ptrecords\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/train_0035_of_0064.ptrecords\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0040_of_0064.ptrecords\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/train_0040_of_0064.ptrecords\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0044_of_0064.ptrecords\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/train_0043_of_0064.ptrecords\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0047_of_0064.ptrecords\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/train_0046_of_0064.ptrecords\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0050_of_0064.ptrecords\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/train_0049_of_0064.ptrecords\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0053_of_0064.ptrecords\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/train_0052_of_0064.ptrecords\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0056_of_0064.ptrecords\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/train_0055_of_0064.ptrecords\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0058_of_0064.ptrecords\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/train_0057_of_0064.ptrecords\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0061_of_0064.ptrecords\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/train_0059_of_0064.ptrecords\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/train_0063_of_0064.ptrecords\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/train_0062_of_0064.ptrecords\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/val_0002_of_0008.ptrecords\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/val_0001_of_0008.ptrecords\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/val_0004_of_0008.ptrecords\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/val_0003_of_0008.ptrecords\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6131)\u001b[0m start to build ptrecord for /content/work/mpii/ptrecords_mpii/val_0008_of_0008.ptrecords\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(build_single_ptrecord pid=6130)\u001b[0m finished building ptrecord for /content/work/mpii/ptrecords_mpii/val_0007_of_0008.ptrecords\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Successfully wrote 25204 annotations to PT Records.\n",
            "CPU times: user 2.27 s, sys: 461 ms, total: 2.73 s\n",
            "Wall time: 2min 46s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tfrecord"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6XVSH-SsljB",
        "outputId": "23c4c952-b6fd-43c5-ee55-2358877b4900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tfrecord\n",
            "  Downloading tfrecord-1.14.6.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from tfrecord) (2.0.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from tfrecord) (5.29.5)\n",
            "Collecting crc32c (from tfrecord)\n",
            "  Downloading crc32c-2.8-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (7.8 kB)\n",
            "Downloading crc32c-2.8-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.0/80.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: tfrecord\n",
            "  Building wheel for tfrecord (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tfrecord: filename=tfrecord-1.14.6-py3-none-any.whl size=14834 sha256=d0fe24c468533f7ee2c2353a6711b50be71631bfe164b67b1ee00825f51bee37\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/00/32/db9abfb84508d182806ca91e7391ca865d5037e585c088e8e6\n",
            "Successfully built tfrecord\n",
            "Installing collected packages: crc32c, tfrecord\n",
            "Successfully installed crc32c-2.8 tfrecord-1.14.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tfrecord.torch.dataset import TFRecordDataset\n",
        "\n",
        "feature_description = {\n",
        "    'image/height': 'int',\n",
        "    'image/width': 'int',\n",
        "    'image/depth': 'int',\n",
        "    'image/object/parts/x': 'int',\n",
        "    'image/object/parts/y': 'int',\n",
        "    'image/object/parts/v': 'int',\n",
        "    'image/object/center/x': 'int',\n",
        "    'image/object/center/y': 'int',\n",
        "    'image/object/scale': 'float',\n",
        "    'image/encoded': 'byte',\n",
        "    'image/filename': 'byte',\n",
        "}\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7mLaClpvVYi",
        "outputId": "817a5b86-aa14-42da-b44f-8b22008d4cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def crop_roi(image, features, margin=0.2):\n",
        "    img_height, img_width, img_depth = image.shape\n",
        "\n",
        "    keypoint_x = torch.tensor(features['image/object/parts/x'], dtype=torch.int32)\n",
        "    keypoint_y = torch.tensor(features['image/object/parts/y'], dtype=torch.int32)\n",
        "    center_x = features['image/object/center/x']\n",
        "    center_y = features['image/object/center/y']\n",
        "    body_height = features['image/object/scale'] * 200.0\n",
        "\n",
        "    # 유효한 keypoint (값이 0보다 큰 값)만 선택합니다.\n",
        "    masked_keypoint_x = keypoint_x[keypoint_x > 0]\n",
        "    masked_keypoint_y = keypoint_y[keypoint_y > 0]\n",
        "\n",
        "    # 최소, 최대 값 계산 (유효한 keypoint가 하나 이상 있다고 가정)\n",
        "    keypoint_xmin = masked_keypoint_x.min()\n",
        "    keypoint_xmax = masked_keypoint_x.max()\n",
        "    keypoint_ymin = masked_keypoint_y.min()\n",
        "    keypoint_ymax = masked_keypoint_y.max()\n",
        "\n",
        "    # margin을 적용하여 경계를 확장 (body_height * margin 값을 정수로 캐스팅)\n",
        "    extra = int(body_height * margin)\n",
        "    xmin = int(keypoint_xmin.item()) - extra\n",
        "    xmax = int(keypoint_xmax.item()) + extra\n",
        "    ymin = int(keypoint_ymin.item()) - extra\n",
        "    ymax = int(keypoint_ymax.item()) + extra\n",
        "\n",
        "    # 이미지 경계를 벗어나지 않도록 조정\n",
        "    effective_xmin = xmin if xmin > 0 else 0\n",
        "    effective_ymin = ymin if ymin > 0 else 0\n",
        "    effective_xmax = xmax if xmax < img_width else img_width\n",
        "    effective_ymax = ymax if ymax < img_height else img_height\n",
        "\n",
        "    # 이미지 크기 재조정\n",
        "    cropped_image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
        "    new_height, new_width, _ = cropped_image.shape\n",
        "\n",
        "    # keypoint 좌표를 정규화 (0~1 범위)\n",
        "    effective_keypoint_x = (keypoint_x.float() - effective_xmin) / new_width\n",
        "    effective_keypoint_y = (keypoint_y.float() - effective_ymin) / new_height\n",
        "\n",
        "    return cropped_image, effective_keypoint_x, effective_keypoint_y\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Krol1h_fvWCG",
        "outputId": "3fc74c4e-0c7a-4a82-bdf1-f64ea31e9759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def generate_2d_gaussian(height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
        "    # (height, width) 크기의 0으로 채워진 heatmap 생성\n",
        "    heatmap = torch.zeros((height, width), dtype=torch.float32)\n",
        "\n",
        "    xmin = x0 - 3 * sigma\n",
        "    ymin = y0 - 3 * sigma\n",
        "    xmax = x0 + 3 * sigma\n",
        "    ymax = y0 + 3 * sigma\n",
        "\n",
        "    # 범위가 이미지 내에 없거나, visibility가 0이면 heatmap 그대로 반환\n",
        "    if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
        "        return heatmap\n",
        "\n",
        "    size = int(6 * sigma + 1)\n",
        "    grid_range = torch.arange(0, size, dtype=torch.float32)\n",
        "    x_grid, y_grid = torch.meshgrid(grid_range, grid_range, indexing='xy')\n",
        "    center_x = size // 2\n",
        "    center_y = size // 2\n",
        "\n",
        "    # 가우시안 patch 계산\n",
        "    gaussian_patch = torch.exp(-(((x_grid - center_x)**2 + (y_grid - center_y)**2) / (sigma**2 * 2))) * scale\n",
        "\n",
        "    # 이미지와 patch 간의 겹치는 영역 계산\n",
        "    patch_xmin = max(0, -xmin)\n",
        "    patch_ymin = max(0, -ymin)\n",
        "    patch_xmax = min(xmax, width) - xmin\n",
        "    patch_ymax = min(ymax, height) - ymin\n",
        "\n",
        "    heatmap_xmin = max(0, xmin)\n",
        "    heatmap_ymin = max(0, ymin)\n",
        "    heatmap_xmax = min(xmax, width)\n",
        "    heatmap_ymax = min(ymax, height)\n",
        "\n",
        "    # 계산된 영역에 gaussian_patch 값을 할당\n",
        "    heatmap[heatmap_ymin:heatmap_ymax, heatmap_xmin:heatmap_xmax] = \\\n",
        "        gaussian_patch[int(patch_ymin):int(patch_ymax), int(patch_xmin):int(patch_xmax)]\n",
        "\n",
        "    return heatmap\n",
        "\n",
        "def make_heatmaps(features, keypoint_x, keypoint_y, heatmap_shape):\n",
        "    v = torch.tensor(features['image/object/parts/v'], dtype=torch.float32)\n",
        "    x = torch.round(torch.tensor(keypoint_x, dtype=torch.float32) * heatmap_shape[0]).to(torch.int32)\n",
        "    y = torch.round(torch.tensor(keypoint_y, dtype=torch.float32) * heatmap_shape[1]).to(torch.int32)\n",
        "\n",
        "    num_heatmap = heatmap_shape[2]\n",
        "    heatmaps_list = []\n",
        "    for i in range(num_heatmap):\n",
        "        # generate_2d_gaussian 함수 호출 시, height=heatmap_shape[1], width=heatmap_shape[0]\n",
        "        gaussian = generate_2d_gaussian(\n",
        "            heatmap_shape[1],\n",
        "            heatmap_shape[0],\n",
        "            int(y[i].item()),\n",
        "            int(x[i].item()),\n",
        "            visibility=int(v[i].item())\n",
        "        )\n",
        "        heatmaps_list.append(gaussian)\n",
        "\n",
        "    # (num_heatmap, height, width) 텐서를 생성한 후, (height, width, num_heatmap)로 전치\n",
        "    heatmaps = torch.stack(heatmaps_list, dim=0)\n",
        "    heatmaps = heatmaps.permute(1, 2, 0)\n",
        "\n",
        "    return heatmaps\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frz2ag79vXLS",
        "outputId": "043fb530-59d4-4294-bb3d-732a42b2c695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def generate_2d_gaussian(height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
        "    # (height, width) 크기의 0으로 채워진 heatmap 생성\n",
        "    heatmap = torch.zeros((height, width), dtype=torch.float32)\n",
        "\n",
        "    xmin = x0 - 3 * sigma\n",
        "    ymin = y0 - 3 * sigma\n",
        "    xmax = x0 + 3 * sigma\n",
        "    ymax = y0 + 3 * sigma\n",
        "\n",
        "    # 범위가 이미지 내에 없거나, visibility가 0이면 heatmap 그대로 반환\n",
        "    if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
        "        return heatmap\n",
        "\n",
        "    size = int(6 * sigma + 1)\n",
        "    grid_range = torch.arange(0, size, dtype=torch.float32)\n",
        "    x_grid, y_grid = torch.meshgrid(grid_range, grid_range, indexing='xy')\n",
        "    center_x = size // 2\n",
        "    center_y = size // 2\n",
        "\n",
        "    # 가우시안 patch 계산\n",
        "    gaussian_patch = torch.exp(-(((x_grid - center_x)**2 + (y_grid - center_y)**2) / (sigma**2 * 2))) * scale\n",
        "\n",
        "    # 이미지와 patch 간의 겹치는 영역 계산\n",
        "    patch_xmin = max(0, -xmin)\n",
        "    patch_ymin = max(0, -ymin)\n",
        "    patch_xmax = min(xmax, width) - xmin\n",
        "    patch_ymax = min(ymax, height) - ymin\n",
        "\n",
        "    heatmap_xmin = max(0, xmin)\n",
        "    heatmap_ymin = max(0, ymin)\n",
        "    heatmap_xmax = min(xmax, width)\n",
        "    heatmap_ymax = min(ymax, height)\n",
        "\n",
        "    # 계산된 영역에 gaussian_patch 값을 할당\n",
        "    heatmap[heatmap_ymin:heatmap_ymax, heatmap_xmin:heatmap_xmax] = \\\n",
        "        gaussian_patch[int(patch_ymin):int(patch_ymax), int(patch_xmin):int(patch_xmax)]\n",
        "\n",
        "    return heatmap\n",
        "\n",
        "def make_heatmaps(features, keypoint_x, keypoint_y, heatmap_shape):\n",
        "    v = torch.tensor(features['image/object/parts/v'], dtype=torch.float32)\n",
        "    x = torch.round(torch.tensor(keypoint_x, dtype=torch.float32) * heatmap_shape[0]).to(torch.int32)\n",
        "    y = torch.round(torch.tensor(keypoint_y, dtype=torch.float32) * heatmap_shape[1]).to(torch.int32)\n",
        "\n",
        "    num_heatmap = heatmap_shape[2]\n",
        "    heatmaps_list = []\n",
        "    for i in range(num_heatmap):\n",
        "        # generate_2d_gaussian 함수 호출 시, height=heatmap_shape[1], width=heatmap_shape[0]\n",
        "        gaussian = generate_2d_gaussian(\n",
        "            heatmap_shape[1],\n",
        "            heatmap_shape[0],\n",
        "            int(y[i].item()),\n",
        "            int(x[i].item()),\n",
        "            visibility=int(v[i].item())\n",
        "        )\n",
        "        heatmaps_list.append(gaussian)\n",
        "\n",
        "    # (num_heatmap, height, width) 텐서를 생성한 후, (height, width, num_heatmap)로 전치\n",
        "    heatmaps = torch.stack(heatmaps_list, dim=0)\n",
        "    heatmaps = heatmaps.permute(1, 2, 0)\n",
        "\n",
        "    return heatmaps\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCijkc5Hvf3u",
        "outputId": "3a08925e-0951-4386-b031-bcac9e87e3bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import io\n",
        "import numpy as np\n",
        "\n",
        "class Preprocessor(object):\n",
        "    def __init__(self,\n",
        "                 image_shape=(256, 256, 3),\n",
        "                 heatmap_shape=(64, 64, 16),\n",
        "                 is_train=False):\n",
        "        self.is_train = is_train\n",
        "        self.image_shape = image_shape      # (height, width, channels)\n",
        "        self.heatmap_shape = heatmap_shape  # (height, width, num_heatmap)\n",
        "\n",
        "    def __call__(self, example):\n",
        "        features = self.parse_tfexample(example)\n",
        "        # image 데이터를 다시 bytes로 디코딩 후 재로딩 (JPEG 형식)\n",
        "        image = Image.open(io.BytesIO(features['image/encoded']))\n",
        "\n",
        "        if self.is_train:\n",
        "            # 0.1 ~ 0.3 사이의 random margin 생성\n",
        "            random_margin = torch.empty(1).uniform_(0.1, 0.3).item()\n",
        "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
        "            image = image.resize((self.image_shape[1], self.image_shape[0]))\n",
        "        else:\n",
        "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
        "            image = image.resize((self.image_shape[1], self.image_shape[0]))\n",
        "\n",
        "        # 이미지 정규화: uint8 → [0,255] → [-1, 1]\n",
        "        image_np = np.array(image).astype(np.float32)\n",
        "        image_np = image_np / 127.5 - 1.0\n",
        "        # 채널 우선순서로 변환: (H, W, C) -> (C, H, W)\n",
        "        image_tensor = torch.from_numpy(image_np).permute(2, 0, 1)\n",
        "\n",
        "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y, self.heatmap_shape)\n",
        "\n",
        "        return image_tensor, heatmaps\n",
        "\n",
        "    def parse_tfexample(self, example):\n",
        "        \"\"\"\n",
        "        MPIIDataset에서 전달한 예제를 받아, Preprocessor가 처리할 수 있도록 features dict를 구성합니다.\n",
        "        예제 형식: {'image': PIL.Image, 'annotation': anno}\n",
        "        \"\"\"\n",
        "        annotation = example['annotation']\n",
        "        # joints: list of [x, y]\n",
        "        joints = annotation['joints']\n",
        "        keypoint_x = [joint[0] for joint in joints]\n",
        "        keypoint_y = [joint[1] for joint in joints]\n",
        "\n",
        "        # joints_vis가 없으면 모든 관절이 가시적이라고 가정 (1)\n",
        "        joints_vis = annotation.get('joints_vis', [1] * len(joints))\n",
        "\n",
        "        features = {\n",
        "            'image/encoded': self.image_to_bytes(example['image']),\n",
        "            'image/object/parts/x': keypoint_x,\n",
        "            'image/object/parts/y': keypoint_y,\n",
        "            'image/object/parts/v': joints_vis,\n",
        "            'image/object/center/x': annotation['center'][0],\n",
        "            'image/object/center/y': annotation['center'][1],\n",
        "            'image/object/scale': annotation['scale'],\n",
        "        }\n",
        "        return features\n",
        "\n",
        "    def image_to_bytes(self, image):\n",
        "        \"\"\"\n",
        "        PIL.Image 객체를 JPEG 인코딩된 bytes로 변환합니다.\n",
        "        \"\"\"\n",
        "        buffer = io.BytesIO()\n",
        "        image.save(buffer, format=\"JPEG\")\n",
        "        return buffer.getvalue()\n",
        "\n",
        "    def crop_roi(self, image, features, margin=0.2):\n",
        "        # image: PIL.Image, features: dict\n",
        "        img_width, img_height = image.size  # PIL: (width, height)\n",
        "\n",
        "        keypoint_x = torch.tensor(features['image/object/parts/x'], dtype=torch.int32)\n",
        "        keypoint_y = torch.tensor(features['image/object/parts/y'], dtype=torch.int32)\n",
        "        body_height = features['image/object/scale'] * 200.0\n",
        "\n",
        "        # 유효한 keypoint (값 > 0)만 선택\n",
        "        masked_keypoint_x = keypoint_x[keypoint_x > 0]\n",
        "        masked_keypoint_y = keypoint_y[keypoint_y > 0]\n",
        "\n",
        "        keypoint_xmin = int(masked_keypoint_x.min().item())\n",
        "        keypoint_xmax = int(masked_keypoint_x.max().item())\n",
        "        keypoint_ymin = int(masked_keypoint_y.min().item())\n",
        "        keypoint_ymax = int(masked_keypoint_y.max().item())\n",
        "\n",
        "        extra = int(body_height * margin)\n",
        "        xmin = keypoint_xmin - extra\n",
        "        xmax = keypoint_xmax + extra\n",
        "        ymin = keypoint_ymin - extra\n",
        "        ymax = keypoint_ymax + extra\n",
        "\n",
        "        effective_xmin = max(xmin, 0)\n",
        "        effective_ymin = max(ymin, 0)\n",
        "        effective_xmax = min(xmax, img_width)\n",
        "        effective_ymax = min(ymax, img_height)\n",
        "\n",
        "        cropped_image = image.crop((effective_xmin, effective_ymin, effective_xmax, effective_ymax))\n",
        "\n",
        "        new_width = effective_xmax - effective_xmin\n",
        "        new_height = effective_ymax - effective_ymin\n",
        "\n",
        "        effective_keypoint_x = (keypoint_x.float() - effective_xmin) / new_width\n",
        "        effective_keypoint_y = (keypoint_y.float() - effective_ymin) / new_height\n",
        "\n",
        "        return cropped_image, effective_keypoint_x, effective_keypoint_y\n",
        "\n",
        "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
        "        heatmap = torch.zeros((height, width), dtype=torch.float32)\n",
        "\n",
        "        xmin = x0 - 3 * sigma\n",
        "        ymin = y0 - 3 * sigma\n",
        "        xmax = x0 + 3 * sigma\n",
        "        ymax = y0 + 3 * sigma\n",
        "\n",
        "        if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
        "            return heatmap\n",
        "\n",
        "        size = int(6 * sigma + 1)\n",
        "        grid_range = torch.arange(0, size, dtype=torch.float32)\n",
        "        x_grid, y_grid = torch.meshgrid(grid_range, grid_range, indexing='xy')\n",
        "        center_x = size // 2\n",
        "        center_y = size // 2\n",
        "\n",
        "        gaussian_patch = torch.exp(-(((x_grid - center_x)**2 + (y_grid - center_y)**2) / (2 * sigma**2))) * scale\n",
        "\n",
        "        patch_xmin = max(0, -xmin)\n",
        "        patch_ymin = max(0, -ymin)\n",
        "        patch_xmax = min(xmax, width) - xmin\n",
        "        patch_ymax = min(ymax, height) - ymin\n",
        "\n",
        "        heatmap_xmin = max(0, xmin)\n",
        "        heatmap_ymin = max(0, ymin)\n",
        "        heatmap_xmax = min(xmax, width)\n",
        "        heatmap_ymax = min(ymax, height)\n",
        "\n",
        "        heatmap[heatmap_ymin:heatmap_ymax, heatmap_xmin:heatmap_xmax] = \\\n",
        "            gaussian_patch[int(patch_ymin):int(patch_ymax), int(patch_xmin):int(patch_xmax)]\n",
        "\n",
        "        return heatmap\n",
        "\n",
        "    def make_heatmaps(self, features, keypoint_x, keypoint_y, heatmap_shape):\n",
        "        # heatmap_shape: (height, width, num_heatmap)\n",
        "        v = torch.tensor(features['image/object/parts/v'], dtype=torch.float32)\n",
        "        x = torch.round(keypoint_x * heatmap_shape[1]).to(torch.int32)  # width: heatmap_shape[1]\n",
        "        y = torch.round(keypoint_y * heatmap_shape[0]).to(torch.int32)  # height: heatmap_shape[0]\n",
        "\n",
        "        num_heatmap = heatmap_shape[2]\n",
        "        heatmaps_list = []\n",
        "\n",
        "        for i in range(num_heatmap):\n",
        "            gaussian = self.generate_2d_guassian(\n",
        "                height=heatmap_shape[0],\n",
        "                width=heatmap_shape[1],\n",
        "                y0=int(y[i].item()),\n",
        "                x0=int(x[i].item()),\n",
        "                visibility=int(v[i].item())\n",
        "            )\n",
        "            heatmaps_list.append(gaussian)\n",
        "\n",
        "        # 스택 후, (num_heatmap, height, width) 형태로 반환\n",
        "        heatmaps = torch.stack(heatmaps_list, dim=0)\n",
        "        return heatmaps\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6NR-vdgvhDT",
        "outputId": "d3259bdb-442e-46fa-c9bf-3533ab81ceeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BottleneckBlock(nn.Module):\n",
        "    def __init__(self, in_channels, filters, stride=1, downsample=False):\n",
        "        super(BottleneckBlock, self).__init__()\n",
        "        self.downsample = downsample\n",
        "        # 만약 downsample이라면 identity branch에 1x1 conv 적용하여 채널 수와 spatial size 조정\n",
        "        if self.downsample:\n",
        "            self.downsample_conv = nn.Conv2d(in_channels, filters, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "        # main branch\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels, momentum=0.9)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_channels, filters // 2, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "\n",
        "        self.bn2 = nn.BatchNorm2d(filters // 2, momentum=0.9)\n",
        "        # kernel_size=3, padding=1로 'same' padding 효과\n",
        "        self.conv2 = nn.Conv2d(filters // 2, filters // 2, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "        self.bn3 = nn.BatchNorm2d(filters // 2, momentum=0.9)\n",
        "        self.conv3 = nn.Conv2d(filters // 2, filters, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        if self.downsample:\n",
        "            identity = self.downsample_conv(x)\n",
        "\n",
        "        out = self.bn1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        out = self.bn3(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "\n",
        "        out += identity\n",
        "        return out\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuCDicFhv5G3",
        "outputId": "92ae5cfc-703e-4b3c-fddd-52b102167667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class HourglassModule(nn.Module):\n",
        "    def __init__(self, order, filters, num_residual):\n",
        "        super(HourglassModule, self).__init__()\n",
        "        self.order = order\n",
        "\n",
        "        # Up branch: BottleneckBlock 1회 + num_residual회 반복\n",
        "        self.up1_0 = BottleneckBlock(in_channels=filters, filters=filters, stride=1, downsample=False)\n",
        "        self.up1_blocks = nn.Sequential(*[\n",
        "            BottleneckBlock(in_channels=filters, filters=filters, stride=1, downsample=False)\n",
        "            for _ in range(num_residual)\n",
        "        ])\n",
        "\n",
        "        # Low branch: MaxPool + num_residual BottleneckBlock\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.low1_blocks = nn.Sequential(*[\n",
        "            BottleneckBlock(in_channels=filters, filters=filters, stride=1, downsample=False)\n",
        "            for _ in range(num_residual)\n",
        "        ])\n",
        "\n",
        "        # Recursive hourglass or additional BottleneckBlocks\n",
        "        if order > 1:\n",
        "            self.low2 = HourglassModule(order - 1, filters, num_residual)\n",
        "        else:\n",
        "            self.low2_blocks = nn.Sequential(*[\n",
        "                BottleneckBlock(in_channels=filters, filters=filters, stride=1, downsample=False)\n",
        "                for _ in range(num_residual)\n",
        "            ])\n",
        "\n",
        "        # 후처리 BottleneckBlock 반복\n",
        "        self.low3_blocks = nn.Sequential(*[\n",
        "            BottleneckBlock(in_channels=filters, filters=filters, stride=1, downsample=False)\n",
        "            for _ in range(num_residual)\n",
        "        ])\n",
        "\n",
        "        # UpSampling (최근접 보간법)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "\n",
        "    def forward(self, x):\n",
        "        # up branch\n",
        "        up1 = self.up1_0(x)\n",
        "        up1 = self.up1_blocks(up1)\n",
        "\n",
        "        # low branch\n",
        "        low1 = self.pool(x)\n",
        "        low1 = self.low1_blocks(low1)\n",
        "        if self.order > 1:\n",
        "            low2 = self.low2(low1)\n",
        "        else:\n",
        "            low2 = self.low2_blocks(low1)\n",
        "        low3 = self.low3_blocks(low2)\n",
        "        up2 = self.upsample(low3)\n",
        "\n",
        "        return up2 + up1\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZkz7_4jzTdD",
        "outputId": "4e15168c-7fdc-4ccb-f69a-91fd8a5b8c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LinearLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(LinearLayer, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels, momentum=0.9)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # He (Kaiming) 초기화 적용\n",
        "        nn.init.kaiming_normal_(self.conv.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jhplfjtu0BHE",
        "outputId": "423c94f8-8f76-4ddf-9da4-e38bb88627a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class StackedHourglassNetwork(nn.Module):\n",
        "    def __init__(self, input_shape=(256, 256, 3), num_stack=4, num_residual=1, num_heatmap=16):\n",
        "        super(StackedHourglassNetwork, self).__init__()\n",
        "        self.num_stack = num_stack\n",
        "\n",
        "        in_channels = input_shape[2]  # 3\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64, momentum=0.9)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Bottleneck blocks 초기화\n",
        "        # BottleneckBlock의 첫번째 호출: 64 → 128, downsample=True\n",
        "        self.bottleneck1 = BottleneckBlock(in_channels=64, filters=128, stride=1, downsample=True)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # 두 번째: 128 → 128, downsample=False\n",
        "        self.bottleneck2 = BottleneckBlock(in_channels=128, filters=128, stride=1, downsample=False)\n",
        "        # 세 번째: 128 → 256, downsample=True\n",
        "        self.bottleneck3 = BottleneckBlock(in_channels=128, filters=256, stride=1, downsample=True)\n",
        "\n",
        "        # 스택 구성 요소들\n",
        "        self.hourglass_modules = nn.ModuleList()\n",
        "        self.residual_modules = nn.ModuleList()  # hourglass 후 residual block들 (num_residual회)\n",
        "        self.linear_layers = nn.ModuleList()\n",
        "        self.heatmap_convs = nn.ModuleList()\n",
        "        # 마지막 스택을 제외한 중간 피쳐 결합용 1x1 conv\n",
        "        self.intermediate_convs = nn.ModuleList()\n",
        "        self.intermediate_outs = nn.ModuleList()\n",
        "\n",
        "        for i in range(num_stack):\n",
        "            # order=4인 hourglass 모듈 (앞에서 정의한 HourglassModule 사용)\n",
        "            self.hourglass_modules.append(HourglassModule(order=4, filters=256, num_residual=num_residual))\n",
        "            # hourglass 후 residual block들\n",
        "            self.residual_modules.append(nn.Sequential(*[\n",
        "                BottleneckBlock(in_channels=256, filters=256, stride=1, downsample=False)\n",
        "                for _ in range(num_residual)\n",
        "            ]))\n",
        "            # Linear layer: 1x1 conv + BN + ReLU (앞에서 정의한 LinearLayer 사용)\n",
        "            self.linear_layers.append(LinearLayer(in_channels=256, out_channels=256))\n",
        "            # 최종 heatmap을 생성하는 1x1 conv\n",
        "            self.heatmap_convs.append(nn.Conv2d(256, num_heatmap, kernel_size=1, stride=1, padding=0))\n",
        "\n",
        "            if i < num_stack - 1:\n",
        "                self.intermediate_convs.append(nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0))\n",
        "                self.intermediate_outs.append(nn.Conv2d(num_heatmap, 256, kernel_size=1, stride=1, padding=0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, 3, H, W)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.bottleneck1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.bottleneck2(x)\n",
        "        x = self.bottleneck3(x)\n",
        "\n",
        "        outputs = []\n",
        "        for i in range(self.num_stack):\n",
        "            hg = self.hourglass_modules[i](x)\n",
        "            res = self.residual_modules[i](hg)\n",
        "            lin = self.linear_layers[i](res)\n",
        "            heatmap = self.heatmap_convs[i](lin)\n",
        "            outputs.append(heatmap)\n",
        "\n",
        "            if i < self.num_stack - 1:\n",
        "                inter1 = self.intermediate_convs[i](lin)\n",
        "                inter2 = self.intermediate_outs[i](heatmap)\n",
        "                x = inter1 + inter2  # 다음 스택의 입력으로 사용\n",
        "\n",
        "        return outputs\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AtxxrmN0Cf3",
        "outputId": "fdf86d27-5862-4d81-aed1-c3e31d40c678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 epochs,\n",
        "                 global_batch_size,\n",
        "                 initial_learning_rate):\n",
        "        \"\"\"\n",
        "        - model: 학습시킬 PyTorch 모델(nn.Module)\n",
        "        - epochs: 전체 학습 epoch 수\n",
        "        - global_batch_size: 전체 배치 크기 (loss 계산 시 사용)\n",
        "        - initial_learning_rate: 초기 학습률\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.epochs = epochs\n",
        "        self.global_batch_size = global_batch_size\n",
        "\n",
        "        # MSE loss를 reduction='none'으로 사용 (가중치 적용을 위해)\n",
        "        self.loss_object = nn.MSELoss(reduction='none')\n",
        "\n",
        "        # Adam optimizer 초기화\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=initial_learning_rate)\n",
        "\n",
        "        # 학습률 스케줄링 관련 변수들\n",
        "        self.current_learning_rate = initial_learning_rate\n",
        "        self.last_val_loss = math.inf\n",
        "        self.lowest_val_loss = math.inf\n",
        "        self.patience_count = 0\n",
        "        self.max_patience = 10\n",
        "\n",
        "        # 최적 모델 체크포인트 저장\n",
        "        self.best_model = None\n",
        "\n",
        "        # 단일 GPU/멀티 GPU(DataParallel) 설정\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            print(f\"멀티 GPU 사용 (GPU 개수: {torch.cuda.device_count()})\")\n",
        "            self.model = nn.DataParallel(self.model)\n",
        "        else:\n",
        "            print(\"단일 GPU 혹은 CPU 사용\")\n",
        "\n",
        "    def lr_decay(self):\n",
        "        \"\"\"\n",
        "        patience_count가 max_patience를 넘으면 학습률을 1/10으로 감소,\n",
        "        그렇지 않고 val_loss가 그대로면 patience_count += 1,\n",
        "        새 최저 val_loss를 달성하면 patience_count를 0으로.\n",
        "        \"\"\"\n",
        "        if self.patience_count >= self.max_patience:\n",
        "            self.current_learning_rate /= 10.0\n",
        "            self.patience_count = 0\n",
        "        elif self.last_val_loss == self.lowest_val_loss:\n",
        "            self.patience_count = 0\n",
        "\n",
        "        self.patience_count += 1\n",
        "\n",
        "        # optimizer의 learning rate 갱신\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = self.current_learning_rate\n",
        "\n",
        "    def lr_decay_step(self, epoch):\n",
        "        \"\"\"\n",
        "        25, 50, 75 epoch에서 학습률을 1/10으로 감소시키는 스케줄링.\n",
        "        \"\"\"\n",
        "        if epoch in [25, 50, 75]:\n",
        "            self.current_learning_rate /= 10.0\n",
        "\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = self.current_learning_rate\n",
        "\n",
        "    def compute_loss(self, labels, outputs):\n",
        "        \"\"\"\n",
        "        여러 스택의 heatmap 출력(outputs)에 대해 MSE를 구하되,\n",
        "        labels > 0인 위치에는 81의 추가 가중치를 적용.\n",
        "        \"\"\"\n",
        "        loss = 0\n",
        "        for output in outputs:\n",
        "            # labels > 0 이면 81 + 1 = 82, 아니면 1\n",
        "            weights = (labels > 0).float() * 81 + 1\n",
        "            squared_error = (labels - output) ** 2\n",
        "            weighted_error = squared_error * weights\n",
        "            # 전체 배치에 대한 평균 후, global_batch_size로 나눔\n",
        "            loss += weighted_error.mean() / self.global_batch_size\n",
        "        return loss\n",
        "\n",
        "    def train_step(self, images, labels, device):\n",
        "        self.model.train()\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        outputs = self.model(images)\n",
        "        loss = self.compute_loss(labels, outputs)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def val_step(self, images, labels, device):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = self.model(images)\n",
        "            loss = self.compute_loss(labels, outputs)\n",
        "        return loss.item()\n",
        "\n",
        "    def run(self, train_loader, val_loader, device):\n",
        "        \"\"\"\n",
        "        - train_loader, val_loader: PyTorch DataLoader\n",
        "        - device: torch.device('cuda' or 'cpu')\n",
        "        \"\"\"\n",
        "        for epoch in range(1, self.epochs + 1):\n",
        "            # 학습률 감소 로직\n",
        "            self.lr_decay()\n",
        "            print(f\"Start epoch {epoch} with learning rate {self.current_learning_rate:.6f}\")\n",
        "\n",
        "            # Training\n",
        "            total_train_loss = 0.0\n",
        "            num_train_batches = 0\n",
        "            for images, labels in train_loader:\n",
        "                batch_loss = self.train_step(images, labels, device)\n",
        "                total_train_loss += batch_loss\n",
        "                num_train_batches += 1\n",
        "                print(f\"[Train] batch {num_train_batches} loss {batch_loss:.4f} \"\n",
        "                      f\"avg_loss {total_train_loss/num_train_batches:.4f}\")\n",
        "            train_loss = total_train_loss / num_train_batches\n",
        "            print(f\"Epoch {epoch} train loss {train_loss:.4f}\")\n",
        "\n",
        "            # Validation\n",
        "            total_val_loss = 0.0\n",
        "            num_val_batches = 0\n",
        "            for images, labels in val_loader:\n",
        "                batch_loss = self.val_step(images, labels, device)\n",
        "                num_val_batches += 1\n",
        "                print(f\"[Val] batch {num_val_batches} loss {batch_loss:.4f}\")\n",
        "                # NaN이 아닌 경우만 합산\n",
        "                if not math.isnan(batch_loss):\n",
        "                    total_val_loss += batch_loss\n",
        "                else:\n",
        "                    num_val_batches -= 1\n",
        "\n",
        "            if num_val_batches > 0:\n",
        "                val_loss = total_val_loss / num_val_batches\n",
        "            else:\n",
        "                val_loss = float('nan')\n",
        "\n",
        "            print(f\"Epoch {epoch} val loss {val_loss:.4f}\")\n",
        "\n",
        "            # 새로운 최저 val_loss 달성 시 모델 저장\n",
        "            if val_loss < self.lowest_val_loss:\n",
        "                self.save_model(epoch, val_loss)\n",
        "                self.lowest_val_loss = val_loss\n",
        "            self.last_val_loss = val_loss\n",
        "\n",
        "        return self.best_model\n",
        "\n",
        "    def save_model(self, epoch, loss):\n",
        "        model_name = os.path.join(MODEL_PATH, f'model-epoch-{epoch}-loss-{loss:.4f}.pt')\n",
        "        torch.save(self.model.state_dict(), model_name)\n",
        "        self.best_model = model_name\n",
        "        print(f\"Model {model_name} saved.\")\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjZeqcsO0MzH",
        "outputId": "364155ec-475e-497a-9c1e-88c368b2df65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "IMAGE_SHAPE = (256, 256, 3)\n",
        "HEATMAP_SIZE = (64, 64)\n",
        "\n",
        "def create_dataloader(annotation_file, image_dir, batch_size, num_heatmap, is_train=True):\n",
        "    \"\"\"\n",
        "    annotation_file: JSON 파일 경로 (예: train.json)\n",
        "    image_dir: 이미지 파일들이 저장된 디렉토리 경로\n",
        "    batch_size: 배치 크기\n",
        "    num_heatmap: 생성할 heatmap 개수\n",
        "    is_train: True이면 shuffle 적용\n",
        "    \"\"\"\n",
        "\n",
        "    preprocess = Preprocessor(\n",
        "        image_shape=IMAGE_SHAPE,\n",
        "        heatmap_shape=(HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap),\n",
        "        is_train=is_train\n",
        "    )\n",
        "\n",
        "    dataset = MPIIDataset(annotation_file=annotation_file, image_dir=image_dir, transform=preprocess)\n",
        "\n",
        "    # DataLoader 생성\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=is_train,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        drop_last=False,\n",
        "        prefetch_factor=2\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "IDoL04eY0em0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs, learning_rate, num_heatmap, batch_size, train_annotation_file, val_annotation_file, image_dir):\n",
        "    \"\"\"\n",
        "    - epochs: 전체 학습 epoch 수\n",
        "    - learning_rate: 초기 학습률\n",
        "    - num_heatmap: 생성할 heatmap 개수\n",
        "    - batch_size: 배치 크기\n",
        "    - train_annotation_file: train.json 파일 경로\n",
        "    - val_annotation_file: validation.json 파일 경로\n",
        "    - image_dir: 이미지 파일들이 저장된 디렉토리 경로\n",
        "    \"\"\"\n",
        "    global_batch_size = batch_size\n",
        "\n",
        "    train_loader = create_dataloader(train_annotation_file, image_dir, batch_size, num_heatmap, is_train=True)\n",
        "    val_loader = create_dataloader(val_annotation_file, image_dir, batch_size, num_heatmap, is_train=False)\n",
        "\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        os.makedirs(MODEL_PATH)\n",
        "\n",
        "    model = StackedHourglassNetwork(IMAGE_SHAPE, num_stack=4, num_residual=1, num_heatmap=num_heatmap)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model,\n",
        "        epochs,\n",
        "        global_batch_size,\n",
        "        initial_learning_rate=learning_rate\n",
        "    )\n",
        "\n",
        "    print(\"Start training...\")\n",
        "    return trainer.run(train_loader, val_loader, device)\n",
        "\n",
        "print('슝=3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fm4oLGI0gBO",
        "outputId": "ac4865c7-5b5b-42cd-bf81-4964bc17ff13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# 아래 코드를 실행하면 직접 학습을 해볼 수 있습니다.\n",
        "import os\n",
        "\n",
        "TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n",
        "VAL_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')\n",
        "IMAGE_DIR = os.path.join(PROJECT_PATH, 'images')\n",
        "\n",
        "epochs = 2\n",
        "batch_size = 2\n",
        "num_heatmap = 16\n",
        "learning_rate = 0.0007\n",
        "\n",
        "best_model_file = train(epochs, learning_rate, num_heatmap, batch_size, TRAIN_JSON, VAL_JSON, IMAGE_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oss17fio0o8C",
        "outputId": "065bb387-3ce6-48a5-e7d1-717f74cc3a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단일 GPU 혹은 CPU 사용\n",
            "Start training...\n",
            "Start epoch 1 with learning rate 0.000700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] batch 1 loss 16.6857 avg_loss 16.6857\n",
            "[Train] batch 2 loss 18.6459 avg_loss 17.6658\n",
            "[Train] batch 3 loss 17.4044 avg_loss 17.5786\n",
            "[Train] batch 4 loss 18.0607 avg_loss 17.6992\n",
            "[Train] batch 5 loss 17.2038 avg_loss 17.6001\n",
            "[Train] batch 6 loss 18.1601 avg_loss 17.6934\n",
            "[Train] batch 7 loss 17.4299 avg_loss 17.6558\n",
            "[Train] batch 8 loss 16.7896 avg_loss 17.5475\n",
            "[Train] batch 9 loss 16.8555 avg_loss 17.4706\n",
            "[Train] batch 10 loss 14.8046 avg_loss 17.2040\n",
            "[Train] batch 11 loss 15.2930 avg_loss 17.0303\n",
            "[Train] batch 12 loss 16.9962 avg_loss 17.0275\n",
            "[Train] batch 13 loss 17.0763 avg_loss 17.0312\n",
            "[Train] batch 14 loss 15.3272 avg_loss 16.9095\n",
            "[Train] batch 15 loss 16.0041 avg_loss 16.8491\n",
            "[Train] batch 16 loss 16.2050 avg_loss 16.8089\n",
            "[Train] batch 17 loss 15.7702 avg_loss 16.7478\n",
            "[Train] batch 18 loss 15.8207 avg_loss 16.6963\n",
            "[Train] batch 19 loss 13.9334 avg_loss 16.5509\n",
            "[Train] batch 20 loss 16.1596 avg_loss 16.5313\n",
            "[Train] batch 21 loss 12.8936 avg_loss 16.3581\n",
            "[Train] batch 22 loss 13.0383 avg_loss 16.2072\n",
            "[Train] batch 23 loss 15.5450 avg_loss 16.1784\n",
            "[Train] batch 24 loss 15.3749 avg_loss 16.1449\n",
            "[Train] batch 25 loss 16.1510 avg_loss 16.1451\n",
            "[Train] batch 26 loss 14.4237 avg_loss 16.0789\n",
            "[Train] batch 27 loss 15.2558 avg_loss 16.0485\n",
            "[Train] batch 28 loss 14.2074 avg_loss 15.9827\n",
            "[Train] batch 29 loss 13.4499 avg_loss 15.8954\n",
            "[Train] batch 30 loss 13.5508 avg_loss 15.8172\n",
            "[Train] batch 31 loss 14.9318 avg_loss 15.7886\n",
            "[Train] batch 32 loss 13.8865 avg_loss 15.7292\n",
            "[Train] batch 33 loss 15.1364 avg_loss 15.7112\n",
            "[Train] batch 34 loss 14.4691 avg_loss 15.6747\n",
            "[Train] batch 35 loss 12.9882 avg_loss 15.5980\n",
            "[Train] batch 36 loss 13.8988 avg_loss 15.5508\n",
            "[Train] batch 37 loss 14.8428 avg_loss 15.5316\n",
            "[Train] batch 38 loss 14.4527 avg_loss 15.5032\n",
            "[Train] batch 39 loss 12.9877 avg_loss 15.4387\n",
            "[Train] batch 40 loss 13.7985 avg_loss 15.3977\n",
            "[Train] batch 41 loss 11.6257 avg_loss 15.3057\n",
            "[Train] batch 42 loss 14.6113 avg_loss 15.2892\n",
            "[Train] batch 43 loss 14.0455 avg_loss 15.2603\n",
            "[Train] batch 44 loss 13.0196 avg_loss 15.2093\n",
            "[Train] batch 45 loss 13.8193 avg_loss 15.1785\n",
            "[Train] batch 46 loss 15.5018 avg_loss 15.1855\n",
            "[Train] batch 47 loss 13.7352 avg_loss 15.1546\n",
            "[Train] batch 48 loss 14.7650 avg_loss 15.1465\n",
            "[Train] batch 49 loss 12.0672 avg_loss 15.0837\n",
            "[Train] batch 50 loss 13.5325 avg_loss 15.0526\n",
            "[Train] batch 51 loss 15.2552 avg_loss 15.0566\n",
            "[Train] batch 52 loss 11.1279 avg_loss 14.9811\n",
            "[Train] batch 53 loss 14.3140 avg_loss 14.9685\n",
            "[Train] batch 54 loss 13.0365 avg_loss 14.9327\n",
            "[Train] batch 55 loss 13.5316 avg_loss 14.9072\n",
            "[Train] batch 56 loss 14.7944 avg_loss 14.9052\n",
            "[Train] batch 57 loss 11.4082 avg_loss 14.8439\n",
            "[Train] batch 58 loss 14.2349 avg_loss 14.8334\n",
            "[Train] batch 59 loss 14.1232 avg_loss 14.8213\n",
            "[Train] batch 60 loss 13.7479 avg_loss 14.8034\n",
            "[Train] batch 61 loss 13.7846 avg_loss 14.7867\n",
            "[Train] batch 62 loss 14.7763 avg_loss 14.7866\n",
            "[Train] batch 63 loss 15.0513 avg_loss 14.7908\n",
            "[Train] batch 64 loss 13.1162 avg_loss 14.7646\n",
            "[Train] batch 65 loss 13.6255 avg_loss 14.7471\n",
            "[Train] batch 66 loss 13.3597 avg_loss 14.7261\n",
            "[Train] batch 67 loss 15.4719 avg_loss 14.7372\n",
            "[Train] batch 68 loss 11.8312 avg_loss 14.6944\n",
            "[Train] batch 69 loss 13.8816 avg_loss 14.6827\n",
            "[Train] batch 70 loss 14.0330 avg_loss 14.6734\n",
            "[Train] batch 71 loss 15.3785 avg_loss 14.6833\n",
            "[Train] batch 72 loss 12.0096 avg_loss 14.6462\n",
            "[Train] batch 73 loss 14.8002 avg_loss 14.6483\n",
            "[Train] batch 74 loss 13.1836 avg_loss 14.6285\n",
            "[Train] batch 75 loss 14.0564 avg_loss 14.6209\n",
            "[Train] batch 76 loss 14.2038 avg_loss 14.6154\n",
            "[Train] batch 77 loss 15.1846 avg_loss 14.6228\n",
            "[Train] batch 78 loss 13.2625 avg_loss 14.6053\n",
            "[Train] batch 79 loss 11.3110 avg_loss 14.5636\n",
            "[Train] batch 80 loss 14.8789 avg_loss 14.5676\n",
            "[Train] batch 81 loss 14.2009 avg_loss 14.5631\n",
            "[Train] batch 82 loss 13.7489 avg_loss 14.5531\n",
            "[Train] batch 83 loss 13.8180 avg_loss 14.5443\n",
            "[Train] batch 84 loss 13.7714 avg_loss 14.5351\n",
            "[Train] batch 85 loss 14.3457 avg_loss 14.5328\n",
            "[Train] batch 86 loss 14.0753 avg_loss 14.5275\n",
            "[Train] batch 87 loss 13.8162 avg_loss 14.5193\n",
            "[Train] batch 88 loss 13.4101 avg_loss 14.5067\n",
            "[Train] batch 89 loss 11.9870 avg_loss 14.4784\n",
            "[Train] batch 90 loss 13.6174 avg_loss 14.4689\n",
            "[Train] batch 91 loss 14.1066 avg_loss 14.4649\n",
            "[Train] batch 92 loss 14.4718 avg_loss 14.4650\n",
            "[Train] batch 93 loss 13.7587 avg_loss 14.4574\n",
            "[Train] batch 94 loss 14.5967 avg_loss 14.4588\n",
            "[Train] batch 95 loss 11.7629 avg_loss 14.4305\n",
            "[Train] batch 96 loss 12.1628 avg_loss 14.4068\n",
            "[Train] batch 97 loss 12.5488 avg_loss 14.3877\n",
            "[Train] batch 98 loss 14.1217 avg_loss 14.3850\n",
            "[Train] batch 99 loss 13.7642 avg_loss 14.3787\n",
            "[Train] batch 100 loss 14.5663 avg_loss 14.3806\n",
            "[Train] batch 101 loss 13.8150 avg_loss 14.3750\n",
            "[Train] batch 102 loss 13.0342 avg_loss 14.3618\n",
            "[Train] batch 103 loss 13.7461 avg_loss 14.3559\n",
            "[Train] batch 104 loss 14.7880 avg_loss 14.3600\n",
            "[Train] batch 105 loss 13.1798 avg_loss 14.3488\n",
            "[Train] batch 106 loss 12.3149 avg_loss 14.3296\n",
            "[Train] batch 107 loss 13.6563 avg_loss 14.3233\n",
            "[Train] batch 108 loss 13.8447 avg_loss 14.3189\n",
            "[Train] batch 109 loss 14.1713 avg_loss 14.3175\n",
            "[Train] batch 110 loss 12.4455 avg_loss 14.3005\n",
            "[Train] batch 111 loss 12.7410 avg_loss 14.2864\n",
            "[Train] batch 112 loss 12.3924 avg_loss 14.2695\n",
            "[Train] batch 113 loss 14.5269 avg_loss 14.2718\n",
            "[Train] batch 114 loss 13.6830 avg_loss 14.2666\n",
            "[Train] batch 115 loss 13.7864 avg_loss 14.2625\n",
            "[Train] batch 116 loss 10.5211 avg_loss 14.2302\n",
            "[Train] batch 117 loss 13.5072 avg_loss 14.2240\n",
            "[Train] batch 118 loss 13.4969 avg_loss 14.2179\n",
            "[Train] batch 119 loss 13.9179 avg_loss 14.2153\n",
            "[Train] batch 120 loss 13.4284 avg_loss 14.2088\n",
            "[Train] batch 121 loss 11.5530 avg_loss 14.1868\n",
            "[Train] batch 122 loss 12.2333 avg_loss 14.1708\n",
            "[Train] batch 123 loss 13.4794 avg_loss 14.1652\n",
            "[Train] batch 124 loss 10.6894 avg_loss 14.1372\n",
            "[Train] batch 125 loss 12.1215 avg_loss 14.1211\n",
            "[Train] batch 126 loss 14.0602 avg_loss 14.1206\n",
            "[Train] batch 127 loss 13.1605 avg_loss 14.1130\n",
            "[Train] batch 128 loss 12.9650 avg_loss 14.1040\n",
            "[Train] batch 129 loss 12.5725 avg_loss 14.0922\n",
            "[Train] batch 130 loss 12.1439 avg_loss 14.0772\n",
            "[Train] batch 131 loss 12.4307 avg_loss 14.0646\n",
            "[Train] batch 132 loss 14.3278 avg_loss 14.0666\n",
            "[Train] batch 133 loss 15.4025 avg_loss 14.0766\n",
            "[Train] batch 134 loss 11.9184 avg_loss 14.0605\n",
            "[Train] batch 135 loss 12.1470 avg_loss 14.0464\n",
            "[Train] batch 136 loss 13.8731 avg_loss 14.0451\n",
            "[Train] batch 137 loss 13.5606 avg_loss 14.0416\n",
            "[Train] batch 138 loss 14.2175 avg_loss 14.0428\n",
            "[Train] batch 139 loss 9.6174 avg_loss 14.0110\n",
            "[Train] batch 140 loss 13.3279 avg_loss 14.0061\n",
            "[Train] batch 141 loss 14.2388 avg_loss 14.0078\n",
            "[Train] batch 142 loss 13.2211 avg_loss 14.0022\n",
            "[Train] batch 143 loss 12.9604 avg_loss 13.9949\n",
            "[Train] batch 144 loss 13.5541 avg_loss 13.9919\n",
            "[Train] batch 145 loss 12.9561 avg_loss 13.9847\n",
            "[Train] batch 146 loss 13.2445 avg_loss 13.9797\n",
            "[Train] batch 147 loss 13.2069 avg_loss 13.9744\n",
            "[Train] batch 148 loss 13.9203 avg_loss 13.9740\n",
            "[Train] batch 149 loss 13.0797 avg_loss 13.9680\n",
            "[Train] batch 150 loss 13.1698 avg_loss 13.9627\n",
            "[Train] batch 151 loss 12.7397 avg_loss 13.9546\n",
            "[Train] batch 152 loss 12.6538 avg_loss 13.9461\n",
            "[Train] batch 153 loss 12.9651 avg_loss 13.9397\n",
            "[Train] batch 154 loss 12.4425 avg_loss 13.9299\n",
            "[Train] batch 155 loss 13.3106 avg_loss 13.9259\n",
            "[Train] batch 156 loss 11.8147 avg_loss 13.9124\n",
            "[Train] batch 157 loss 13.2555 avg_loss 13.9082\n",
            "[Train] batch 158 loss 9.5686 avg_loss 13.8807\n",
            "[Train] batch 159 loss 13.3212 avg_loss 13.8772\n",
            "[Train] batch 160 loss 13.3274 avg_loss 13.8738\n",
            "[Train] batch 161 loss 13.5570 avg_loss 13.8718\n",
            "[Train] batch 162 loss 13.1560 avg_loss 13.8674\n",
            "[Train] batch 163 loss 13.4397 avg_loss 13.8648\n",
            "[Train] batch 164 loss 11.9743 avg_loss 13.8533\n",
            "[Train] batch 165 loss 13.5772 avg_loss 13.8516\n",
            "[Train] batch 166 loss 11.7164 avg_loss 13.8387\n",
            "[Train] batch 167 loss 13.9472 avg_loss 13.8394\n",
            "[Train] batch 168 loss 14.1838 avg_loss 13.8414\n",
            "[Train] batch 169 loss 14.1820 avg_loss 13.8434\n",
            "[Train] batch 170 loss 13.1766 avg_loss 13.8395\n",
            "[Train] batch 171 loss 12.0590 avg_loss 13.8291\n",
            "[Train] batch 172 loss 13.5447 avg_loss 13.8274\n",
            "[Train] batch 173 loss 12.1056 avg_loss 13.8175\n",
            "[Train] batch 174 loss 12.4093 avg_loss 13.8094\n",
            "[Train] batch 175 loss 12.2910 avg_loss 13.8007\n",
            "[Train] batch 176 loss 12.7727 avg_loss 13.7949\n",
            "[Train] batch 177 loss 13.0840 avg_loss 13.7909\n",
            "[Train] batch 178 loss 12.2861 avg_loss 13.7824\n",
            "[Train] batch 179 loss 12.9806 avg_loss 13.7779\n",
            "[Train] batch 180 loss 13.5463 avg_loss 13.7766\n",
            "[Train] batch 181 loss 12.9550 avg_loss 13.7721\n",
            "[Train] batch 182 loss 13.7922 avg_loss 13.7722\n",
            "[Train] batch 183 loss 11.4786 avg_loss 13.7597\n",
            "[Train] batch 184 loss 14.4453 avg_loss 13.7634\n",
            "[Train] batch 185 loss 12.0267 avg_loss 13.7540\n",
            "[Train] batch 186 loss 13.6546 avg_loss 13.7535\n",
            "[Train] batch 187 loss 13.4525 avg_loss 13.7519\n",
            "[Train] batch 188 loss 12.4753 avg_loss 13.7451\n",
            "[Train] batch 189 loss 14.0729 avg_loss 13.7468\n",
            "[Train] batch 190 loss 13.5739 avg_loss 13.7459\n",
            "[Train] batch 191 loss 14.4781 avg_loss 13.7497\n",
            "[Train] batch 192 loss 13.3411 avg_loss 13.7476\n",
            "[Train] batch 193 loss 14.6511 avg_loss 13.7523\n",
            "[Train] batch 194 loss 13.5278 avg_loss 13.7511\n",
            "[Train] batch 195 loss 13.7321 avg_loss 13.7510\n",
            "[Train] batch 196 loss 14.1718 avg_loss 13.7532\n",
            "[Train] batch 197 loss 12.1154 avg_loss 13.7449\n",
            "[Train] batch 198 loss 12.5405 avg_loss 13.7388\n",
            "[Train] batch 199 loss 13.3927 avg_loss 13.7371\n",
            "[Train] batch 200 loss 11.1349 avg_loss 13.7240\n",
            "[Train] batch 201 loss 13.7300 avg_loss 13.7241\n",
            "[Train] batch 202 loss 10.6283 avg_loss 13.7087\n",
            "[Train] batch 203 loss 12.8267 avg_loss 13.7044\n",
            "[Train] batch 204 loss 13.8683 avg_loss 13.7052\n",
            "[Train] batch 205 loss 11.5381 avg_loss 13.6946\n",
            "[Train] batch 206 loss 13.3485 avg_loss 13.6930\n",
            "[Train] batch 207 loss 12.6132 avg_loss 13.6877\n",
            "[Train] batch 208 loss 13.5865 avg_loss 13.6873\n",
            "[Train] batch 209 loss 13.7586 avg_loss 13.6876\n",
            "[Train] batch 210 loss 11.9480 avg_loss 13.6793\n",
            "[Train] batch 211 loss 12.5179 avg_loss 13.6738\n",
            "[Train] batch 212 loss 13.2301 avg_loss 13.6717\n",
            "[Train] batch 213 loss 12.9416 avg_loss 13.6683\n",
            "[Train] batch 214 loss 13.2323 avg_loss 13.6662\n",
            "[Train] batch 215 loss 13.8224 avg_loss 13.6670\n",
            "[Train] batch 216 loss 13.1809 avg_loss 13.6647\n",
            "[Train] batch 217 loss 14.0194 avg_loss 13.6664\n",
            "[Train] batch 218 loss 13.9050 avg_loss 13.6675\n",
            "[Train] batch 219 loss 13.3694 avg_loss 13.6661\n",
            "[Train] batch 220 loss 12.9758 avg_loss 13.6630\n",
            "[Train] batch 221 loss 13.6370 avg_loss 13.6628\n",
            "[Train] batch 222 loss 13.1394 avg_loss 13.6605\n",
            "[Train] batch 223 loss 12.7846 avg_loss 13.6565\n",
            "[Train] batch 224 loss 13.5826 avg_loss 13.6562\n",
            "[Train] batch 225 loss 13.6016 avg_loss 13.6560\n",
            "[Train] batch 226 loss 13.9600 avg_loss 13.6573\n",
            "[Train] batch 227 loss 13.7185 avg_loss 13.6576\n",
            "[Train] batch 228 loss 13.5847 avg_loss 13.6573\n",
            "[Train] batch 229 loss 13.3156 avg_loss 13.6558\n",
            "[Train] batch 230 loss 12.8987 avg_loss 13.6525\n",
            "[Train] batch 231 loss 12.1248 avg_loss 13.6459\n",
            "[Train] batch 232 loss 11.9633 avg_loss 13.6386\n",
            "[Train] batch 233 loss 11.6019 avg_loss 13.6299\n",
            "[Train] batch 234 loss 13.3511 avg_loss 13.6287\n",
            "[Train] batch 235 loss 11.0056 avg_loss 13.6175\n",
            "[Train] batch 236 loss 11.6563 avg_loss 13.6092\n",
            "[Train] batch 237 loss 14.1403 avg_loss 13.6115\n",
            "[Train] batch 238 loss 13.0217 avg_loss 13.6090\n",
            "[Train] batch 239 loss 13.5133 avg_loss 13.6086\n",
            "[Train] batch 240 loss 12.9611 avg_loss 13.6059\n",
            "[Train] batch 241 loss 14.3576 avg_loss 13.6090\n",
            "[Train] batch 242 loss 13.8238 avg_loss 13.6099\n",
            "[Train] batch 243 loss 13.7770 avg_loss 13.6106\n",
            "[Train] batch 244 loss 12.3422 avg_loss 13.6054\n",
            "[Train] batch 245 loss 10.9909 avg_loss 13.5947\n",
            "[Train] batch 246 loss 11.9509 avg_loss 13.5880\n",
            "[Train] batch 247 loss 13.5185 avg_loss 13.5877\n",
            "[Train] batch 248 loss 12.1911 avg_loss 13.5821\n",
            "[Train] batch 249 loss 12.2711 avg_loss 13.5768\n",
            "[Train] batch 250 loss 12.0271 avg_loss 13.5706\n",
            "[Train] batch 251 loss 12.3629 avg_loss 13.5658\n",
            "[Train] batch 252 loss 13.7085 avg_loss 13.5664\n",
            "[Train] batch 253 loss 13.0125 avg_loss 13.5642\n",
            "[Train] batch 254 loss 12.4583 avg_loss 13.5599\n",
            "[Train] batch 255 loss 13.7699 avg_loss 13.5607\n",
            "[Train] batch 256 loss 13.6142 avg_loss 13.5609\n",
            "[Train] batch 257 loss 13.8858 avg_loss 13.5622\n",
            "[Train] batch 258 loss 13.7228 avg_loss 13.5628\n",
            "[Train] batch 259 loss 12.6719 avg_loss 13.5593\n",
            "[Train] batch 260 loss 13.6169 avg_loss 13.5596\n",
            "[Train] batch 261 loss 11.8851 avg_loss 13.5531\n",
            "[Train] batch 262 loss 10.6520 avg_loss 13.5421\n",
            "[Train] batch 263 loss 12.4961 avg_loss 13.5381\n",
            "[Train] batch 264 loss 12.4903 avg_loss 13.5341\n",
            "[Train] batch 265 loss 13.8604 avg_loss 13.5354\n",
            "[Train] batch 266 loss 13.9004 avg_loss 13.5367\n",
            "[Train] batch 267 loss 12.8731 avg_loss 13.5342\n",
            "[Train] batch 268 loss 13.8380 avg_loss 13.5354\n",
            "[Train] batch 269 loss 11.8591 avg_loss 13.5291\n",
            "[Train] batch 270 loss 13.9174 avg_loss 13.5306\n",
            "[Train] batch 271 loss 13.6818 avg_loss 13.5311\n",
            "[Train] batch 272 loss 12.1506 avg_loss 13.5261\n",
            "[Train] batch 273 loss 13.2398 avg_loss 13.5250\n",
            "[Train] batch 274 loss 13.5507 avg_loss 13.5251\n",
            "[Train] batch 275 loss 13.9059 avg_loss 13.5265\n",
            "[Train] batch 276 loss 12.6088 avg_loss 13.5232\n",
            "[Train] batch 277 loss 9.4471 avg_loss 13.5085\n",
            "[Train] batch 278 loss 13.3466 avg_loss 13.5079\n",
            "[Train] batch 279 loss 11.8664 avg_loss 13.5020\n",
            "[Train] batch 280 loss 12.0725 avg_loss 13.4969\n",
            "[Train] batch 281 loss 11.7853 avg_loss 13.4908\n",
            "[Train] batch 282 loss 13.7539 avg_loss 13.4917\n",
            "[Train] batch 283 loss 11.8613 avg_loss 13.4860\n",
            "[Train] batch 284 loss 11.5131 avg_loss 13.4790\n",
            "[Train] batch 285 loss 11.9502 avg_loss 13.4737\n",
            "[Train] batch 286 loss 12.9296 avg_loss 13.4718\n",
            "[Train] batch 287 loss 13.5996 avg_loss 13.4722\n",
            "[Train] batch 288 loss 13.2156 avg_loss 13.4713\n",
            "[Train] batch 289 loss 13.8244 avg_loss 13.4725\n",
            "[Train] batch 290 loss 11.9061 avg_loss 13.4671\n",
            "[Train] batch 291 loss 13.2505 avg_loss 13.4664\n",
            "[Train] batch 292 loss 10.6561 avg_loss 13.4568\n",
            "[Train] batch 293 loss 11.7194 avg_loss 13.4508\n",
            "[Train] batch 294 loss 13.0603 avg_loss 13.4495\n",
            "[Train] batch 295 loss 12.3082 avg_loss 13.4456\n",
            "[Train] batch 296 loss 12.5990 avg_loss 13.4428\n",
            "[Train] batch 297 loss 9.8653 avg_loss 13.4307\n",
            "[Train] batch 298 loss 11.8685 avg_loss 13.4255\n",
            "[Train] batch 299 loss 12.4969 avg_loss 13.4224\n",
            "[Train] batch 300 loss 12.5748 avg_loss 13.4196\n",
            "[Train] batch 301 loss 12.0451 avg_loss 13.4150\n",
            "[Train] batch 302 loss 13.1906 avg_loss 13.4142\n",
            "[Train] batch 303 loss 12.8305 avg_loss 13.4123\n",
            "[Train] batch 304 loss 12.3747 avg_loss 13.4089\n",
            "[Train] batch 305 loss 11.2461 avg_loss 13.4018\n",
            "[Train] batch 306 loss 13.1652 avg_loss 13.4010\n",
            "[Train] batch 307 loss 12.3226 avg_loss 13.3975\n",
            "[Train] batch 308 loss 12.9590 avg_loss 13.3961\n",
            "[Train] batch 309 loss 13.0037 avg_loss 13.3948\n",
            "[Train] batch 310 loss 13.9366 avg_loss 13.3966\n",
            "[Train] batch 311 loss 12.1968 avg_loss 13.3927\n",
            "[Train] batch 312 loss 14.5030 avg_loss 13.3963\n",
            "[Train] batch 313 loss 12.5347 avg_loss 13.3935\n",
            "[Train] batch 314 loss 13.1846 avg_loss 13.3929\n",
            "[Train] batch 315 loss 12.5225 avg_loss 13.3901\n",
            "[Train] batch 316 loss 14.2005 avg_loss 13.3927\n",
            "[Train] batch 317 loss 12.3201 avg_loss 13.3893\n",
            "[Train] batch 318 loss 13.4743 avg_loss 13.3895\n",
            "[Train] batch 319 loss 13.4326 avg_loss 13.3897\n",
            "[Train] batch 320 loss 12.7513 avg_loss 13.3877\n",
            "[Train] batch 321 loss 11.5943 avg_loss 13.3821\n",
            "[Train] batch 322 loss 13.8507 avg_loss 13.3836\n",
            "[Train] batch 323 loss 13.9073 avg_loss 13.3852\n",
            "[Train] batch 324 loss 12.2096 avg_loss 13.3816\n",
            "[Train] batch 325 loss 13.2702 avg_loss 13.3812\n",
            "[Train] batch 326 loss 12.0943 avg_loss 13.3773\n",
            "[Train] batch 327 loss 11.5660 avg_loss 13.3717\n",
            "[Train] batch 328 loss 13.0741 avg_loss 13.3708\n",
            "[Train] batch 329 loss 13.1029 avg_loss 13.3700\n",
            "[Train] batch 330 loss 12.6622 avg_loss 13.3679\n",
            "[Train] batch 331 loss 13.2773 avg_loss 13.3676\n",
            "[Train] batch 332 loss 12.1036 avg_loss 13.3638\n",
            "[Train] batch 333 loss 13.5845 avg_loss 13.3644\n",
            "[Train] batch 334 loss 12.0291 avg_loss 13.3604\n",
            "[Train] batch 335 loss 12.3654 avg_loss 13.3575\n",
            "[Train] batch 336 loss 12.3606 avg_loss 13.3545\n",
            "[Train] batch 337 loss 10.0891 avg_loss 13.3448\n",
            "[Train] batch 338 loss 12.1270 avg_loss 13.3412\n",
            "[Train] batch 339 loss 12.6870 avg_loss 13.3393\n",
            "[Train] batch 340 loss 12.5649 avg_loss 13.3370\n",
            "[Train] batch 341 loss 13.1900 avg_loss 13.3366\n",
            "[Train] batch 342 loss 13.5301 avg_loss 13.3371\n",
            "[Train] batch 343 loss 13.3718 avg_loss 13.3372\n",
            "[Train] batch 344 loss 12.8243 avg_loss 13.3357\n",
            "[Train] batch 345 loss 9.6855 avg_loss 13.3252\n",
            "[Train] batch 346 loss 13.6932 avg_loss 13.3262\n",
            "[Train] batch 347 loss 12.2989 avg_loss 13.3233\n",
            "[Train] batch 348 loss 10.9512 avg_loss 13.3165\n",
            "[Train] batch 349 loss 12.8559 avg_loss 13.3151\n",
            "[Train] batch 350 loss 12.6370 avg_loss 13.3132\n",
            "[Train] batch 351 loss 11.3879 avg_loss 13.3077\n",
            "[Train] batch 352 loss 13.4310 avg_loss 13.3081\n",
            "[Train] batch 353 loss 12.4899 avg_loss 13.3057\n",
            "[Train] batch 354 loss 13.2399 avg_loss 13.3056\n",
            "[Train] batch 355 loss 13.6704 avg_loss 13.3066\n",
            "[Train] batch 356 loss 12.4772 avg_loss 13.3043\n",
            "[Train] batch 357 loss 12.5690 avg_loss 13.3022\n",
            "[Train] batch 358 loss 12.4708 avg_loss 13.2999\n",
            "[Train] batch 359 loss 13.0025 avg_loss 13.2990\n",
            "[Train] batch 360 loss 13.3679 avg_loss 13.2992\n",
            "[Train] batch 361 loss 11.9024 avg_loss 13.2954\n",
            "[Train] batch 362 loss 12.7678 avg_loss 13.2939\n",
            "[Train] batch 363 loss 13.5851 avg_loss 13.2947\n",
            "[Train] batch 364 loss 12.6192 avg_loss 13.2929\n",
            "[Train] batch 365 loss 13.0223 avg_loss 13.2921\n",
            "[Train] batch 366 loss 13.4835 avg_loss 13.2926\n",
            "[Train] batch 367 loss 11.8071 avg_loss 13.2886\n",
            "[Train] batch 368 loss 11.7229 avg_loss 13.2843\n",
            "[Train] batch 369 loss 12.3304 avg_loss 13.2818\n",
            "[Train] batch 370 loss 11.8893 avg_loss 13.2780\n",
            "[Train] batch 371 loss 12.3891 avg_loss 13.2756\n",
            "[Train] batch 372 loss 13.4597 avg_loss 13.2761\n",
            "[Train] batch 373 loss 12.5979 avg_loss 13.2743\n",
            "[Train] batch 374 loss 12.3505 avg_loss 13.2718\n",
            "[Train] batch 375 loss 12.4699 avg_loss 13.2697\n",
            "[Train] batch 376 loss 10.5789 avg_loss 13.2625\n",
            "[Train] batch 377 loss 14.1800 avg_loss 13.2649\n",
            "[Train] batch 378 loss 11.9676 avg_loss 13.2615\n",
            "[Train] batch 379 loss 12.7735 avg_loss 13.2602\n",
            "[Train] batch 380 loss 13.1774 avg_loss 13.2600\n",
            "[Train] batch 381 loss 12.2019 avg_loss 13.2572\n",
            "[Train] batch 382 loss 14.3740 avg_loss 13.2601\n",
            "[Train] batch 383 loss 11.9684 avg_loss 13.2568\n",
            "[Train] batch 384 loss 12.9386 avg_loss 13.2559\n",
            "[Train] batch 385 loss 12.1126 avg_loss 13.2530\n",
            "[Train] batch 386 loss 13.6921 avg_loss 13.2541\n",
            "[Train] batch 387 loss 12.7512 avg_loss 13.2528\n",
            "[Train] batch 388 loss 13.2431 avg_loss 13.2528\n",
            "[Train] batch 389 loss 13.5065 avg_loss 13.2534\n",
            "[Train] batch 390 loss 13.4052 avg_loss 13.2538\n",
            "[Train] batch 391 loss 11.7820 avg_loss 13.2501\n",
            "[Train] batch 392 loss 13.2046 avg_loss 13.2499\n",
            "[Train] batch 393 loss 13.3493 avg_loss 13.2502\n",
            "[Train] batch 394 loss 12.1434 avg_loss 13.2474\n",
            "[Train] batch 395 loss 12.9195 avg_loss 13.2466\n",
            "[Train] batch 396 loss 13.2073 avg_loss 13.2465\n",
            "[Train] batch 397 loss 10.9266 avg_loss 13.2406\n",
            "[Train] batch 398 loss 12.4739 avg_loss 13.2387\n",
            "[Train] batch 399 loss 13.1395 avg_loss 13.2384\n",
            "[Train] batch 400 loss 12.7160 avg_loss 13.2371\n",
            "[Train] batch 401 loss 12.1965 avg_loss 13.2345\n",
            "[Train] batch 402 loss 12.8161 avg_loss 13.2335\n",
            "[Train] batch 403 loss 12.7669 avg_loss 13.2323\n",
            "[Train] batch 404 loss 11.6543 avg_loss 13.2284\n",
            "[Train] batch 405 loss 11.8930 avg_loss 13.2251\n",
            "[Train] batch 406 loss 12.9214 avg_loss 13.2244\n",
            "[Train] batch 407 loss 11.5073 avg_loss 13.2202\n",
            "[Train] batch 408 loss 10.4167 avg_loss 13.2133\n",
            "[Train] batch 409 loss 12.5664 avg_loss 13.2117\n",
            "[Train] batch 410 loss 12.5208 avg_loss 13.2100\n",
            "[Train] batch 411 loss 12.0146 avg_loss 13.2071\n",
            "[Train] batch 412 loss 13.3313 avg_loss 13.2074\n",
            "[Train] batch 413 loss 12.8659 avg_loss 13.2066\n",
            "[Train] batch 414 loss 11.7130 avg_loss 13.2030\n",
            "[Train] batch 415 loss 13.8880 avg_loss 13.2046\n",
            "[Train] batch 416 loss 11.9179 avg_loss 13.2016\n",
            "[Train] batch 417 loss 13.3677 avg_loss 13.2019\n",
            "[Train] batch 418 loss 10.6367 avg_loss 13.1958\n",
            "[Train] batch 419 loss 13.0930 avg_loss 13.1956\n",
            "[Train] batch 420 loss 11.3686 avg_loss 13.1912\n",
            "[Train] batch 421 loss 12.7623 avg_loss 13.1902\n",
            "[Train] batch 422 loss 13.6333 avg_loss 13.1912\n",
            "[Train] batch 423 loss 12.2957 avg_loss 13.1891\n",
            "[Train] batch 424 loss 12.9025 avg_loss 13.1885\n",
            "[Train] batch 425 loss 11.4396 avg_loss 13.1843\n",
            "[Train] batch 426 loss 12.3585 avg_loss 13.1824\n",
            "[Train] batch 427 loss 9.0889 avg_loss 13.1728\n",
            "[Train] batch 428 loss 12.6592 avg_loss 13.1716\n",
            "[Train] batch 429 loss 13.5062 avg_loss 13.1724\n",
            "[Train] batch 430 loss 11.3398 avg_loss 13.1681\n",
            "[Train] batch 431 loss 13.5664 avg_loss 13.1691\n",
            "[Train] batch 432 loss 11.0866 avg_loss 13.1642\n",
            "[Train] batch 433 loss 12.6838 avg_loss 13.1631\n",
            "[Train] batch 434 loss 12.2853 avg_loss 13.1611\n",
            "[Train] batch 435 loss 11.4665 avg_loss 13.1572\n",
            "[Train] batch 436 loss 13.1243 avg_loss 13.1571\n",
            "[Train] batch 437 loss 12.9869 avg_loss 13.1567\n",
            "[Train] batch 438 loss 12.1441 avg_loss 13.1544\n",
            "[Train] batch 439 loss 13.4147 avg_loss 13.1550\n",
            "[Train] batch 440 loss 12.8565 avg_loss 13.1543\n",
            "[Train] batch 441 loss 11.1288 avg_loss 13.1498\n",
            "[Train] batch 442 loss 12.4736 avg_loss 13.1482\n",
            "[Train] batch 443 loss 12.7059 avg_loss 13.1472\n",
            "[Train] batch 444 loss 12.8942 avg_loss 13.1467\n",
            "[Train] batch 445 loss 13.1200 avg_loss 13.1466\n",
            "[Train] batch 446 loss 12.5307 avg_loss 13.1452\n",
            "[Train] batch 447 loss 10.5634 avg_loss 13.1394\n",
            "[Train] batch 448 loss 10.0420 avg_loss 13.1325\n",
            "[Train] batch 449 loss 14.3794 avg_loss 13.1353\n",
            "[Train] batch 450 loss 12.8102 avg_loss 13.1346\n",
            "[Train] batch 451 loss 13.6305 avg_loss 13.1357\n",
            "[Train] batch 452 loss 12.6114 avg_loss 13.1345\n",
            "[Train] batch 453 loss 12.7914 avg_loss 13.1338\n",
            "[Train] batch 454 loss 12.7415 avg_loss 13.1329\n",
            "[Train] batch 455 loss 13.3048 avg_loss 13.1333\n",
            "[Train] batch 456 loss 12.8621 avg_loss 13.1327\n",
            "[Train] batch 457 loss 9.6875 avg_loss 13.1251\n",
            "[Train] batch 458 loss 11.5148 avg_loss 13.1216\n",
            "[Train] batch 459 loss 13.2509 avg_loss 13.1219\n",
            "[Train] batch 460 loss 12.3966 avg_loss 13.1203\n",
            "[Train] batch 461 loss 11.0640 avg_loss 13.1159\n",
            "[Train] batch 462 loss 12.8997 avg_loss 13.1154\n",
            "[Train] batch 463 loss 13.6695 avg_loss 13.1166\n",
            "[Train] batch 464 loss 13.8930 avg_loss 13.1183\n",
            "[Train] batch 465 loss 10.2510 avg_loss 13.1121\n",
            "[Train] batch 466 loss 11.4493 avg_loss 13.1085\n",
            "[Train] batch 467 loss 12.6509 avg_loss 13.1076\n",
            "[Train] batch 468 loss 12.4029 avg_loss 13.1061\n",
            "[Train] batch 469 loss 12.7060 avg_loss 13.1052\n",
            "[Train] batch 470 loss 13.8722 avg_loss 13.1068\n",
            "[Train] batch 471 loss 11.5468 avg_loss 13.1035\n",
            "[Train] batch 472 loss 13.1905 avg_loss 13.1037\n",
            "[Train] batch 473 loss 12.1146 avg_loss 13.1016\n",
            "[Train] batch 474 loss 12.6975 avg_loss 13.1008\n",
            "[Train] batch 475 loss 11.5443 avg_loss 13.0975\n",
            "[Train] batch 476 loss 13.1771 avg_loss 13.0977\n",
            "[Train] batch 477 loss 14.9748 avg_loss 13.1016\n",
            "[Train] batch 478 loss 13.1164 avg_loss 13.1016\n",
            "[Train] batch 479 loss 12.9493 avg_loss 13.1013\n",
            "[Train] batch 480 loss 12.5828 avg_loss 13.1002\n",
            "[Train] batch 481 loss 13.2395 avg_loss 13.1005\n",
            "[Train] batch 482 loss 13.4727 avg_loss 13.1013\n",
            "[Train] batch 483 loss 11.7834 avg_loss 13.0986\n",
            "[Train] batch 484 loss 13.8260 avg_loss 13.1001\n",
            "[Train] batch 485 loss 13.0277 avg_loss 13.0999\n",
            "[Train] batch 486 loss 10.3417 avg_loss 13.0942\n",
            "[Train] batch 487 loss 13.6731 avg_loss 13.0954\n",
            "[Train] batch 488 loss 11.9330 avg_loss 13.0930\n",
            "[Train] batch 489 loss 13.5216 avg_loss 13.0939\n",
            "[Train] batch 490 loss 12.0557 avg_loss 13.0918\n",
            "[Train] batch 491 loss 12.5933 avg_loss 13.0908\n",
            "[Train] batch 492 loss 12.3955 avg_loss 13.0894\n",
            "[Train] batch 493 loss 12.0922 avg_loss 13.0873\n",
            "[Train] batch 494 loss 12.6530 avg_loss 13.0865\n",
            "[Train] batch 495 loss 13.5199 avg_loss 13.0873\n",
            "[Train] batch 496 loss 11.3754 avg_loss 13.0839\n",
            "[Train] batch 497 loss 12.2767 avg_loss 13.0823\n",
            "[Train] batch 498 loss 13.9203 avg_loss 13.0839\n",
            "[Train] batch 499 loss 10.0731 avg_loss 13.0779\n",
            "[Train] batch 500 loss 11.5828 avg_loss 13.0749\n",
            "[Train] batch 501 loss 12.8585 avg_loss 13.0745\n",
            "[Train] batch 502 loss 13.8098 avg_loss 13.0760\n",
            "[Train] batch 503 loss 13.2001 avg_loss 13.0762\n",
            "[Train] batch 504 loss 12.3392 avg_loss 13.0747\n",
            "[Train] batch 505 loss 13.5007 avg_loss 13.0756\n",
            "[Train] batch 506 loss 11.0785 avg_loss 13.0716\n",
            "[Train] batch 507 loss 12.1458 avg_loss 13.0698\n",
            "[Train] batch 508 loss 11.6895 avg_loss 13.0671\n",
            "[Train] batch 509 loss 13.5796 avg_loss 13.0681\n",
            "[Train] batch 510 loss 12.5212 avg_loss 13.0670\n",
            "[Train] batch 511 loss 13.4159 avg_loss 13.0677\n",
            "[Train] batch 512 loss 12.8267 avg_loss 13.0672\n",
            "[Train] batch 513 loss 12.9957 avg_loss 13.0671\n",
            "[Train] batch 514 loss 12.4720 avg_loss 13.0659\n",
            "[Train] batch 515 loss 14.0294 avg_loss 13.0678\n",
            "[Train] batch 516 loss 13.0358 avg_loss 13.0678\n",
            "[Train] batch 517 loss 12.9083 avg_loss 13.0674\n",
            "[Train] batch 518 loss 11.3613 avg_loss 13.0642\n",
            "[Train] batch 519 loss 13.1370 avg_loss 13.0643\n",
            "[Train] batch 520 loss 13.2635 avg_loss 13.0647\n",
            "[Train] batch 521 loss 12.9164 avg_loss 13.0644\n",
            "[Train] batch 522 loss 11.8973 avg_loss 13.0622\n",
            "[Train] batch 523 loss 13.0272 avg_loss 13.0621\n",
            "[Train] batch 524 loss 12.5731 avg_loss 13.0612\n",
            "[Train] batch 525 loss 12.5912 avg_loss 13.0603\n",
            "[Train] batch 526 loss 12.3970 avg_loss 13.0590\n",
            "[Train] batch 527 loss 11.9613 avg_loss 13.0569\n",
            "[Train] batch 528 loss 13.3434 avg_loss 13.0575\n",
            "[Train] batch 529 loss 11.3375 avg_loss 13.0542\n",
            "[Train] batch 530 loss 12.7604 avg_loss 13.0537\n",
            "[Train] batch 531 loss 12.5993 avg_loss 13.0528\n",
            "[Train] batch 532 loss 12.8942 avg_loss 13.0525\n",
            "[Train] batch 533 loss 13.1649 avg_loss 13.0527\n",
            "[Train] batch 534 loss 11.8993 avg_loss 13.0505\n",
            "[Train] batch 535 loss 12.5174 avg_loss 13.0496\n",
            "[Train] batch 536 loss 13.2404 avg_loss 13.0499\n",
            "[Train] batch 537 loss 12.5760 avg_loss 13.0490\n",
            "[Train] batch 538 loss 13.0934 avg_loss 13.0491\n",
            "[Train] batch 539 loss 11.9256 avg_loss 13.0470\n",
            "[Train] batch 540 loss 12.1479 avg_loss 13.0454\n",
            "[Train] batch 541 loss 12.2261 avg_loss 13.0438\n",
            "[Train] batch 542 loss 15.0512 avg_loss 13.0475\n",
            "[Train] batch 543 loss 13.0578 avg_loss 13.0476\n",
            "[Train] batch 544 loss 13.7782 avg_loss 13.0489\n",
            "[Train] batch 545 loss 11.5805 avg_loss 13.0462\n",
            "[Train] batch 546 loss 13.7790 avg_loss 13.0476\n",
            "[Train] batch 547 loss 11.9227 avg_loss 13.0455\n",
            "[Train] batch 548 loss 12.5074 avg_loss 13.0445\n",
            "[Train] batch 549 loss 12.1879 avg_loss 13.0430\n",
            "[Train] batch 550 loss 13.2901 avg_loss 13.0434\n",
            "[Train] batch 551 loss 10.0558 avg_loss 13.0380\n",
            "[Train] batch 552 loss 11.2782 avg_loss 13.0348\n",
            "[Train] batch 553 loss 12.6592 avg_loss 13.0341\n",
            "[Train] batch 554 loss 9.2851 avg_loss 13.0274\n",
            "[Train] batch 555 loss 11.4502 avg_loss 13.0245\n",
            "[Train] batch 556 loss 11.2315 avg_loss 13.0213\n",
            "[Train] batch 557 loss 13.5116 avg_loss 13.0222\n",
            "[Train] batch 558 loss 11.8165 avg_loss 13.0200\n",
            "[Train] batch 559 loss 11.7989 avg_loss 13.0178\n",
            "[Train] batch 560 loss 12.5736 avg_loss 13.0170\n",
            "[Train] batch 561 loss 13.6550 avg_loss 13.0182\n",
            "[Train] batch 562 loss 13.5806 avg_loss 13.0192\n",
            "[Train] batch 563 loss 11.0883 avg_loss 13.0157\n",
            "[Train] batch 564 loss 12.1104 avg_loss 13.0141\n",
            "[Train] batch 565 loss 12.2024 avg_loss 13.0127\n",
            "[Train] batch 566 loss 12.3355 avg_loss 13.0115\n",
            "[Train] batch 567 loss 11.8229 avg_loss 13.0094\n",
            "[Train] batch 568 loss 11.4053 avg_loss 13.0066\n",
            "[Train] batch 569 loss 10.2405 avg_loss 13.0017\n",
            "[Train] batch 570 loss 12.9336 avg_loss 13.0016\n",
            "[Train] batch 571 loss 12.0082 avg_loss 12.9999\n",
            "[Train] batch 572 loss 13.6148 avg_loss 13.0009\n",
            "[Train] batch 573 loss 12.5965 avg_loss 13.0002\n",
            "[Train] batch 574 loss 13.8651 avg_loss 13.0017\n",
            "[Train] batch 575 loss 12.1716 avg_loss 13.0003\n",
            "[Train] batch 576 loss 12.5288 avg_loss 12.9995\n",
            "[Train] batch 577 loss 12.2410 avg_loss 12.9982\n",
            "[Train] batch 578 loss 12.3845 avg_loss 12.9971\n",
            "[Train] batch 579 loss 13.5318 avg_loss 12.9980\n",
            "[Train] batch 580 loss 13.3852 avg_loss 12.9987\n",
            "[Train] batch 581 loss 11.9783 avg_loss 12.9969\n",
            "[Train] batch 582 loss 13.9461 avg_loss 12.9986\n",
            "[Train] batch 583 loss 12.3153 avg_loss 12.9974\n",
            "[Train] batch 584 loss 12.3680 avg_loss 12.9963\n",
            "[Train] batch 585 loss 13.3329 avg_loss 12.9969\n",
            "[Train] batch 586 loss 12.0148 avg_loss 12.9952\n",
            "[Train] batch 587 loss 12.8032 avg_loss 12.9949\n",
            "[Train] batch 588 loss 12.0156 avg_loss 12.9932\n",
            "[Train] batch 589 loss 12.7416 avg_loss 12.9928\n",
            "[Train] batch 590 loss 13.7168 avg_loss 12.9940\n",
            "[Train] batch 591 loss 11.4868 avg_loss 12.9915\n",
            "[Train] batch 592 loss 10.7893 avg_loss 12.9877\n",
            "[Train] batch 593 loss 13.3973 avg_loss 12.9884\n",
            "[Train] batch 594 loss 11.6202 avg_loss 12.9861\n",
            "[Train] batch 595 loss 11.7072 avg_loss 12.9840\n",
            "[Train] batch 596 loss 13.0113 avg_loss 12.9840\n",
            "[Train] batch 597 loss 11.7927 avg_loss 12.9820\n",
            "[Train] batch 598 loss 12.5213 avg_loss 12.9813\n",
            "[Train] batch 599 loss 13.0400 avg_loss 12.9814\n",
            "[Train] batch 600 loss 10.5702 avg_loss 12.9773\n",
            "[Train] batch 601 loss 14.3225 avg_loss 12.9796\n",
            "[Train] batch 602 loss 13.7288 avg_loss 12.9808\n",
            "[Train] batch 603 loss 10.5258 avg_loss 12.9768\n",
            "[Train] batch 604 loss 12.9268 avg_loss 12.9767\n",
            "[Train] batch 605 loss 13.3260 avg_loss 12.9772\n",
            "[Train] batch 606 loss 12.7387 avg_loss 12.9769\n",
            "[Train] batch 607 loss 11.8965 avg_loss 12.9751\n",
            "[Train] batch 608 loss 11.7043 avg_loss 12.9730\n",
            "[Train] batch 609 loss 12.5275 avg_loss 12.9723\n",
            "[Train] batch 610 loss 11.3600 avg_loss 12.9696\n",
            "[Train] batch 611 loss 12.0980 avg_loss 12.9682\n",
            "[Train] batch 612 loss 11.3025 avg_loss 12.9655\n",
            "[Train] batch 613 loss 10.8554 avg_loss 12.9620\n",
            "[Train] batch 614 loss 13.2237 avg_loss 12.9624\n",
            "[Train] batch 615 loss 11.1744 avg_loss 12.9595\n",
            "[Train] batch 616 loss 11.6216 avg_loss 12.9574\n",
            "[Train] batch 617 loss 12.6692 avg_loss 12.9569\n",
            "[Train] batch 618 loss 11.9784 avg_loss 12.9553\n",
            "[Train] batch 619 loss 11.8395 avg_loss 12.9535\n",
            "[Train] batch 620 loss 12.8002 avg_loss 12.9533\n",
            "[Train] batch 621 loss 13.7397 avg_loss 12.9545\n",
            "[Train] batch 622 loss 13.3189 avg_loss 12.9551\n",
            "[Train] batch 623 loss 12.4229 avg_loss 12.9543\n",
            "[Train] batch 624 loss 11.5938 avg_loss 12.9521\n",
            "[Train] batch 625 loss 10.4500 avg_loss 12.9481\n",
            "[Train] batch 626 loss 12.6111 avg_loss 12.9475\n",
            "[Train] batch 627 loss 12.6578 avg_loss 12.9471\n",
            "[Train] batch 628 loss 13.3367 avg_loss 12.9477\n",
            "[Train] batch 629 loss 12.7888 avg_loss 12.9474\n",
            "[Train] batch 630 loss 14.4507 avg_loss 12.9498\n",
            "[Train] batch 631 loss 12.9544 avg_loss 12.9498\n",
            "[Train] batch 632 loss 12.4256 avg_loss 12.9490\n",
            "[Train] batch 633 loss 12.7073 avg_loss 12.9486\n",
            "[Train] batch 634 loss 13.1920 avg_loss 12.9490\n",
            "[Train] batch 635 loss 13.2485 avg_loss 12.9495\n",
            "[Train] batch 636 loss 12.7309 avg_loss 12.9491\n",
            "[Train] batch 637 loss 13.8570 avg_loss 12.9506\n",
            "[Train] batch 638 loss 11.4303 avg_loss 12.9482\n",
            "[Train] batch 639 loss 11.8172 avg_loss 12.9464\n",
            "[Train] batch 640 loss 13.6553 avg_loss 12.9475\n",
            "[Train] batch 641 loss 12.6518 avg_loss 12.9471\n",
            "[Train] batch 642 loss 12.7439 avg_loss 12.9467\n",
            "[Train] batch 643 loss 12.3838 avg_loss 12.9459\n",
            "[Train] batch 644 loss 12.4349 avg_loss 12.9451\n",
            "[Train] batch 645 loss 12.5496 avg_loss 12.9445\n",
            "[Train] batch 646 loss 11.6181 avg_loss 12.9424\n",
            "[Train] batch 647 loss 11.1126 avg_loss 12.9396\n",
            "[Train] batch 648 loss 13.1448 avg_loss 12.9399\n",
            "[Train] batch 649 loss 12.4178 avg_loss 12.9391\n",
            "[Train] batch 650 loss 13.4706 avg_loss 12.9399\n",
            "[Train] batch 651 loss 11.4631 avg_loss 12.9376\n",
            "[Train] batch 652 loss 14.3109 avg_loss 12.9398\n",
            "[Train] batch 653 loss 14.5271 avg_loss 12.9422\n",
            "[Train] batch 654 loss 8.8038 avg_loss 12.9359\n",
            "[Train] batch 655 loss 11.6588 avg_loss 12.9339\n",
            "[Train] batch 656 loss 13.2143 avg_loss 12.9343\n",
            "[Train] batch 657 loss 13.1897 avg_loss 12.9347\n",
            "[Train] batch 658 loss 11.3102 avg_loss 12.9323\n",
            "[Train] batch 659 loss 13.7889 avg_loss 12.9336\n",
            "[Train] batch 660 loss 12.8440 avg_loss 12.9334\n",
            "[Train] batch 661 loss 13.2240 avg_loss 12.9339\n",
            "[Train] batch 662 loss 11.8395 avg_loss 12.9322\n",
            "[Train] batch 663 loss 10.9813 avg_loss 12.9293\n",
            "[Train] batch 664 loss 11.7883 avg_loss 12.9275\n",
            "[Train] batch 665 loss 12.4848 avg_loss 12.9269\n",
            "[Train] batch 666 loss 11.6351 avg_loss 12.9249\n",
            "[Train] batch 667 loss 13.0403 avg_loss 12.9251\n",
            "[Train] batch 668 loss 12.3615 avg_loss 12.9243\n",
            "[Train] batch 669 loss 12.2717 avg_loss 12.9233\n",
            "[Train] batch 670 loss 12.5958 avg_loss 12.9228\n",
            "[Train] batch 671 loss 12.4233 avg_loss 12.9221\n",
            "[Train] batch 672 loss 11.7096 avg_loss 12.9203\n",
            "[Train] batch 673 loss 14.2047 avg_loss 12.9222\n",
            "[Train] batch 674 loss 12.4294 avg_loss 12.9214\n",
            "[Train] batch 675 loss 12.2666 avg_loss 12.9205\n",
            "[Train] batch 676 loss 10.7095 avg_loss 12.9172\n",
            "[Train] batch 677 loss 12.4616 avg_loss 12.9165\n",
            "[Train] batch 678 loss 13.2267 avg_loss 12.9170\n",
            "[Train] batch 679 loss 13.3670 avg_loss 12.9176\n",
            "[Train] batch 680 loss 12.5537 avg_loss 12.9171\n",
            "[Train] batch 681 loss 11.0241 avg_loss 12.9143\n",
            "[Train] batch 682 loss 12.8255 avg_loss 12.9142\n",
            "[Train] batch 683 loss 11.4977 avg_loss 12.9121\n",
            "[Train] batch 684 loss 13.3926 avg_loss 12.9128\n",
            "[Train] batch 685 loss 12.9530 avg_loss 12.9129\n",
            "[Train] batch 686 loss 12.6662 avg_loss 12.9125\n",
            "[Train] batch 687 loss 11.8571 avg_loss 12.9110\n",
            "[Train] batch 688 loss 11.3641 avg_loss 12.9087\n",
            "[Train] batch 689 loss 12.5350 avg_loss 12.9082\n",
            "[Train] batch 690 loss 13.7549 avg_loss 12.9094\n",
            "[Train] batch 691 loss 12.5492 avg_loss 12.9089\n",
            "[Train] batch 692 loss 12.4459 avg_loss 12.9082\n",
            "[Train] batch 693 loss 12.9540 avg_loss 12.9083\n",
            "[Train] batch 694 loss 14.6419 avg_loss 12.9108\n",
            "[Train] batch 695 loss 12.0837 avg_loss 12.9096\n",
            "[Train] batch 696 loss 11.2958 avg_loss 12.9073\n",
            "[Train] batch 697 loss 13.9910 avg_loss 12.9088\n",
            "[Train] batch 698 loss 13.8283 avg_loss 12.9102\n",
            "[Train] batch 699 loss 14.4712 avg_loss 12.9124\n",
            "[Train] batch 700 loss 13.4477 avg_loss 12.9132\n",
            "[Train] batch 701 loss 13.0866 avg_loss 12.9134\n",
            "[Train] batch 702 loss 13.8647 avg_loss 12.9148\n",
            "[Train] batch 703 loss 12.8820 avg_loss 12.9147\n",
            "[Train] batch 704 loss 13.6995 avg_loss 12.9158\n",
            "[Train] batch 705 loss 12.3215 avg_loss 12.9150\n",
            "[Train] batch 706 loss 13.0133 avg_loss 12.9151\n",
            "[Train] batch 707 loss 12.1025 avg_loss 12.9140\n",
            "[Train] batch 708 loss 12.8700 avg_loss 12.9139\n",
            "[Train] batch 709 loss 12.0257 avg_loss 12.9127\n",
            "[Train] batch 710 loss 12.1915 avg_loss 12.9116\n",
            "[Train] batch 711 loss 10.0637 avg_loss 12.9076\n",
            "[Train] batch 712 loss 12.3270 avg_loss 12.9068\n",
            "[Train] batch 713 loss 13.2463 avg_loss 12.9073\n",
            "[Train] batch 714 loss 11.9294 avg_loss 12.9059\n",
            "[Train] batch 715 loss 12.5746 avg_loss 12.9055\n",
            "[Train] batch 716 loss 13.0402 avg_loss 12.9057\n",
            "[Train] batch 717 loss 11.9683 avg_loss 12.9043\n",
            "[Train] batch 718 loss 12.1571 avg_loss 12.9033\n",
            "[Train] batch 719 loss 12.1555 avg_loss 12.9023\n",
            "[Train] batch 720 loss 12.2335 avg_loss 12.9013\n",
            "[Train] batch 721 loss 12.4338 avg_loss 12.9007\n",
            "[Train] batch 722 loss 11.7842 avg_loss 12.8991\n",
            "[Train] batch 723 loss 10.3237 avg_loss 12.8956\n",
            "[Train] batch 724 loss 10.7227 avg_loss 12.8926\n",
            "[Train] batch 725 loss 11.6862 avg_loss 12.8909\n",
            "[Train] batch 726 loss 12.9997 avg_loss 12.8911\n",
            "[Train] batch 727 loss 12.9937 avg_loss 12.8912\n",
            "[Train] batch 728 loss 13.1898 avg_loss 12.8916\n",
            "[Train] batch 729 loss 9.7557 avg_loss 12.8873\n",
            "[Train] batch 730 loss 12.1451 avg_loss 12.8863\n",
            "[Train] batch 731 loss 12.1407 avg_loss 12.8853\n",
            "[Train] batch 732 loss 10.6213 avg_loss 12.8822\n",
            "[Train] batch 733 loss 12.5455 avg_loss 12.8817\n",
            "[Train] batch 734 loss 11.8539 avg_loss 12.8803\n",
            "[Train] batch 735 loss 11.3531 avg_loss 12.8782\n",
            "[Train] batch 736 loss 10.0587 avg_loss 12.8744\n",
            "[Train] batch 737 loss 11.2871 avg_loss 12.8723\n",
            "[Train] batch 738 loss 11.9474 avg_loss 12.8710\n",
            "[Train] batch 739 loss 11.4729 avg_loss 12.8691\n",
            "[Train] batch 740 loss 13.9581 avg_loss 12.8706\n",
            "[Train] batch 741 loss 9.9618 avg_loss 12.8667\n",
            "[Train] batch 742 loss 13.7681 avg_loss 12.8679\n",
            "[Train] batch 743 loss 11.8131 avg_loss 12.8665\n",
            "[Train] batch 744 loss 12.8994 avg_loss 12.8665\n",
            "[Train] batch 745 loss 12.2813 avg_loss 12.8657\n",
            "[Train] batch 746 loss 12.4442 avg_loss 12.8651\n",
            "[Train] batch 747 loss 12.2847 avg_loss 12.8644\n",
            "[Train] batch 748 loss 12.6481 avg_loss 12.8641\n",
            "[Train] batch 749 loss 14.3198 avg_loss 12.8660\n",
            "[Train] batch 750 loss 10.8945 avg_loss 12.8634\n",
            "[Train] batch 751 loss 11.3988 avg_loss 12.8614\n",
            "[Train] batch 752 loss 11.8697 avg_loss 12.8601\n",
            "[Train] batch 753 loss 11.1223 avg_loss 12.8578\n",
            "[Train] batch 754 loss 11.5051 avg_loss 12.8560\n",
            "[Train] batch 755 loss 13.0331 avg_loss 12.8563\n",
            "[Train] batch 756 loss 14.1166 avg_loss 12.8579\n",
            "[Train] batch 757 loss 13.5295 avg_loss 12.8588\n",
            "[Train] batch 758 loss 11.9317 avg_loss 12.8576\n",
            "[Train] batch 759 loss 12.9464 avg_loss 12.8577\n",
            "[Train] batch 760 loss 11.8598 avg_loss 12.8564\n",
            "[Train] batch 761 loss 9.6478 avg_loss 12.8522\n",
            "[Train] batch 762 loss 12.0060 avg_loss 12.8511\n",
            "[Train] batch 763 loss 12.7210 avg_loss 12.8509\n",
            "[Train] batch 764 loss 10.7626 avg_loss 12.8482\n",
            "[Train] batch 765 loss 12.9571 avg_loss 12.8483\n",
            "[Train] batch 766 loss 10.0346 avg_loss 12.8446\n",
            "[Train] batch 767 loss 13.8646 avg_loss 12.8460\n",
            "[Train] batch 768 loss 12.5796 avg_loss 12.8456\n",
            "[Train] batch 769 loss 12.5195 avg_loss 12.8452\n",
            "[Train] batch 770 loss 13.1653 avg_loss 12.8456\n",
            "[Train] batch 771 loss 12.0653 avg_loss 12.8446\n",
            "[Train] batch 772 loss 12.6031 avg_loss 12.8443\n",
            "[Train] batch 773 loss 11.3569 avg_loss 12.8424\n",
            "[Train] batch 774 loss 11.6337 avg_loss 12.8408\n",
            "[Train] batch 775 loss 12.7995 avg_loss 12.8407\n",
            "[Train] batch 776 loss 12.7985 avg_loss 12.8407\n",
            "[Train] batch 777 loss 11.2159 avg_loss 12.8386\n",
            "[Train] batch 778 loss 12.9715 avg_loss 12.8388\n",
            "[Train] batch 779 loss 8.9470 avg_loss 12.8338\n",
            "[Train] batch 780 loss 12.0426 avg_loss 12.8328\n",
            "[Train] batch 781 loss 12.1283 avg_loss 12.8319\n",
            "[Train] batch 782 loss 12.8889 avg_loss 12.8319\n",
            "[Train] batch 783 loss 12.2759 avg_loss 12.8312\n",
            "[Train] batch 784 loss 12.2059 avg_loss 12.8304\n",
            "[Train] batch 785 loss 13.1917 avg_loss 12.8309\n",
            "[Train] batch 786 loss 12.9011 avg_loss 12.8310\n",
            "[Train] batch 787 loss 13.1492 avg_loss 12.8314\n",
            "[Train] batch 788 loss 10.1304 avg_loss 12.8280\n",
            "[Train] batch 789 loss 11.8167 avg_loss 12.8267\n",
            "[Train] batch 790 loss 12.0586 avg_loss 12.8257\n",
            "[Train] batch 791 loss 11.6957 avg_loss 12.8243\n",
            "[Train] batch 792 loss 12.7642 avg_loss 12.8242\n",
            "[Train] batch 793 loss 10.4631 avg_loss 12.8212\n",
            "[Train] batch 794 loss 12.3971 avg_loss 12.8207\n",
            "[Train] batch 795 loss 11.6981 avg_loss 12.8193\n",
            "[Train] batch 796 loss 12.7981 avg_loss 12.8192\n",
            "[Train] batch 797 loss 13.0804 avg_loss 12.8196\n",
            "[Train] batch 798 loss 11.8056 avg_loss 12.8183\n",
            "[Train] batch 799 loss 13.2791 avg_loss 12.8189\n",
            "[Train] batch 800 loss 13.5122 avg_loss 12.8197\n",
            "[Train] batch 801 loss 12.1217 avg_loss 12.8189\n",
            "[Train] batch 802 loss 12.4625 avg_loss 12.8184\n",
            "[Train] batch 803 loss 9.7710 avg_loss 12.8146\n",
            "[Train] batch 804 loss 13.0220 avg_loss 12.8149\n",
            "[Train] batch 805 loss 13.8955 avg_loss 12.8162\n",
            "[Train] batch 806 loss 10.6755 avg_loss 12.8136\n",
            "[Train] batch 807 loss 14.1762 avg_loss 12.8153\n",
            "[Train] batch 808 loss 11.6319 avg_loss 12.8138\n",
            "[Train] batch 809 loss 10.6393 avg_loss 12.8111\n",
            "[Train] batch 810 loss 12.4570 avg_loss 12.8107\n",
            "[Train] batch 811 loss 12.2674 avg_loss 12.8100\n",
            "[Train] batch 812 loss 12.6531 avg_loss 12.8098\n",
            "[Train] batch 813 loss 13.8531 avg_loss 12.8111\n",
            "[Train] batch 814 loss 12.7620 avg_loss 12.8110\n",
            "[Train] batch 815 loss 12.9034 avg_loss 12.8111\n",
            "[Train] batch 816 loss 13.2018 avg_loss 12.8116\n",
            "[Train] batch 817 loss 13.2328 avg_loss 12.8121\n",
            "[Train] batch 818 loss 11.2437 avg_loss 12.8102\n",
            "[Train] batch 819 loss 14.0023 avg_loss 12.8117\n",
            "[Train] batch 820 loss 13.3528 avg_loss 12.8123\n",
            "[Train] batch 821 loss 12.3986 avg_loss 12.8118\n",
            "[Train] batch 822 loss 12.9332 avg_loss 12.8120\n",
            "[Train] batch 823 loss 10.6757 avg_loss 12.8094\n",
            "[Train] batch 824 loss 14.0228 avg_loss 12.8109\n",
            "[Train] batch 825 loss 11.9873 avg_loss 12.8099\n",
            "[Train] batch 826 loss 12.8218 avg_loss 12.8099\n",
            "[Train] batch 827 loss 11.9489 avg_loss 12.8088\n",
            "[Train] batch 828 loss 12.6339 avg_loss 12.8086\n",
            "[Train] batch 829 loss 12.7030 avg_loss 12.8085\n",
            "[Train] batch 830 loss 12.8226 avg_loss 12.8085\n",
            "[Train] batch 831 loss 12.6763 avg_loss 12.8084\n",
            "[Train] batch 832 loss 12.3847 avg_loss 12.8078\n",
            "[Train] batch 833 loss 12.1972 avg_loss 12.8071\n",
            "[Train] batch 834 loss 12.4050 avg_loss 12.8066\n",
            "[Train] batch 835 loss 11.8846 avg_loss 12.8055\n",
            "[Train] batch 836 loss 12.6200 avg_loss 12.8053\n",
            "[Train] batch 837 loss 13.6042 avg_loss 12.8063\n",
            "[Train] batch 838 loss 12.7090 avg_loss 12.8061\n",
            "[Train] batch 839 loss 11.2860 avg_loss 12.8043\n",
            "[Train] batch 840 loss 13.5809 avg_loss 12.8053\n",
            "[Train] batch 841 loss 12.8085 avg_loss 12.8053\n",
            "[Train] batch 842 loss 11.7312 avg_loss 12.8040\n",
            "[Train] batch 843 loss 13.4036 avg_loss 12.8047\n",
            "[Train] batch 844 loss 14.5125 avg_loss 12.8067\n",
            "[Train] batch 845 loss 13.8141 avg_loss 12.8079\n",
            "[Train] batch 846 loss 10.4701 avg_loss 12.8051\n",
            "[Train] batch 847 loss 11.2721 avg_loss 12.8033\n",
            "[Train] batch 848 loss 11.6600 avg_loss 12.8020\n",
            "[Train] batch 849 loss 10.1370 avg_loss 12.7989\n",
            "[Train] batch 850 loss 12.3263 avg_loss 12.7983\n",
            "[Train] batch 851 loss 10.3238 avg_loss 12.7954\n",
            "[Train] batch 852 loss 11.3381 avg_loss 12.7937\n",
            "[Train] batch 853 loss 11.2712 avg_loss 12.7919\n",
            "[Train] batch 854 loss 11.3679 avg_loss 12.7902\n",
            "[Train] batch 855 loss 11.5000 avg_loss 12.7887\n",
            "[Train] batch 856 loss 9.7580 avg_loss 12.7852\n",
            "[Train] batch 857 loss 13.0335 avg_loss 12.7855\n",
            "[Train] batch 858 loss 13.3558 avg_loss 12.7861\n",
            "[Train] batch 859 loss 11.9246 avg_loss 12.7851\n",
            "[Train] batch 860 loss 10.6670 avg_loss 12.7827\n",
            "[Train] batch 861 loss 12.8177 avg_loss 12.7827\n",
            "[Train] batch 862 loss 13.9118 avg_loss 12.7840\n",
            "[Train] batch 863 loss 11.0775 avg_loss 12.7820\n",
            "[Train] batch 864 loss 12.5755 avg_loss 12.7818\n",
            "[Train] batch 865 loss 11.5534 avg_loss 12.7804\n",
            "[Train] batch 866 loss 11.9361 avg_loss 12.7794\n",
            "[Train] batch 867 loss 12.4719 avg_loss 12.7790\n",
            "[Train] batch 868 loss 13.1419 avg_loss 12.7795\n",
            "[Train] batch 869 loss 12.1805 avg_loss 12.7788\n",
            "[Train] batch 870 loss 9.7658 avg_loss 12.7753\n",
            "[Train] batch 871 loss 11.3333 avg_loss 12.7737\n",
            "[Train] batch 872 loss 10.8577 avg_loss 12.7715\n",
            "[Train] batch 873 loss 9.6463 avg_loss 12.7679\n",
            "[Train] batch 874 loss 12.8430 avg_loss 12.7680\n",
            "[Train] batch 875 loss 12.0844 avg_loss 12.7672\n",
            "[Train] batch 876 loss 14.0827 avg_loss 12.7687\n",
            "[Train] batch 877 loss 12.4388 avg_loss 12.7683\n",
            "[Train] batch 878 loss 12.3024 avg_loss 12.7678\n",
            "[Train] batch 879 loss 11.6198 avg_loss 12.7665\n",
            "[Train] batch 880 loss 12.1597 avg_loss 12.7658\n",
            "[Train] batch 881 loss 13.4284 avg_loss 12.7665\n",
            "[Train] batch 882 loss 12.4568 avg_loss 12.7662\n",
            "[Train] batch 883 loss 13.0783 avg_loss 12.7665\n",
            "[Train] batch 884 loss 12.8656 avg_loss 12.7667\n",
            "[Train] batch 885 loss 12.4021 avg_loss 12.7662\n",
            "[Train] batch 886 loss 13.3677 avg_loss 12.7669\n",
            "[Train] batch 887 loss 13.1429 avg_loss 12.7673\n",
            "[Train] batch 888 loss 13.1841 avg_loss 12.7678\n",
            "[Train] batch 889 loss 13.6898 avg_loss 12.7688\n",
            "[Train] batch 890 loss 11.2974 avg_loss 12.7672\n",
            "[Train] batch 891 loss 11.2243 avg_loss 12.7655\n",
            "[Train] batch 892 loss 12.2907 avg_loss 12.7649\n",
            "[Train] batch 893 loss 15.0164 avg_loss 12.7675\n",
            "[Train] batch 894 loss 13.0682 avg_loss 12.7678\n",
            "[Train] batch 895 loss 11.7866 avg_loss 12.7667\n",
            "[Train] batch 896 loss 12.6340 avg_loss 12.7665\n",
            "[Train] batch 897 loss 14.3486 avg_loss 12.7683\n",
            "[Train] batch 898 loss 12.6091 avg_loss 12.7681\n",
            "[Train] batch 899 loss 13.3251 avg_loss 12.7687\n",
            "[Train] batch 900 loss 12.0703 avg_loss 12.7680\n",
            "[Train] batch 901 loss 11.1748 avg_loss 12.7662\n",
            "[Train] batch 902 loss 12.0309 avg_loss 12.7654\n",
            "[Train] batch 903 loss 11.4860 avg_loss 12.7640\n",
            "[Train] batch 904 loss 12.3220 avg_loss 12.7635\n",
            "[Train] batch 905 loss 9.8252 avg_loss 12.7602\n",
            "[Train] batch 906 loss 13.4924 avg_loss 12.7610\n",
            "[Train] batch 907 loss 11.9230 avg_loss 12.7601\n",
            "[Train] batch 908 loss 10.3634 avg_loss 12.7575\n",
            "[Train] batch 909 loss 12.0766 avg_loss 12.7567\n",
            "[Train] batch 910 loss 11.2718 avg_loss 12.7551\n",
            "[Train] batch 911 loss 12.8398 avg_loss 12.7552\n",
            "[Train] batch 912 loss 12.2019 avg_loss 12.7546\n",
            "[Train] batch 913 loss 12.6873 avg_loss 12.7545\n",
            "[Train] batch 914 loss 12.1812 avg_loss 12.7539\n",
            "[Train] batch 915 loss 12.4125 avg_loss 12.7535\n",
            "[Train] batch 916 loss 13.8575 avg_loss 12.7547\n",
            "[Train] batch 917 loss 13.4275 avg_loss 12.7555\n",
            "[Train] batch 918 loss 11.3761 avg_loss 12.7540\n",
            "[Train] batch 919 loss 12.2927 avg_loss 12.7534\n",
            "[Train] batch 920 loss 13.0225 avg_loss 12.7537\n",
            "[Train] batch 921 loss 12.0699 avg_loss 12.7530\n",
            "[Train] batch 922 loss 11.1726 avg_loss 12.7513\n",
            "[Train] batch 923 loss 14.2108 avg_loss 12.7529\n",
            "[Train] batch 924 loss 13.1724 avg_loss 12.7533\n",
            "[Train] batch 925 loss 11.8989 avg_loss 12.7524\n",
            "[Train] batch 926 loss 12.2753 avg_loss 12.7519\n",
            "[Train] batch 927 loss 12.0186 avg_loss 12.7511\n",
            "[Train] batch 928 loss 11.9822 avg_loss 12.7503\n",
            "[Train] batch 929 loss 12.8740 avg_loss 12.7504\n",
            "[Train] batch 930 loss 11.7862 avg_loss 12.7494\n",
            "[Train] batch 931 loss 10.3857 avg_loss 12.7468\n",
            "[Train] batch 932 loss 10.7848 avg_loss 12.7447\n",
            "[Train] batch 933 loss 12.9611 avg_loss 12.7449\n",
            "[Train] batch 934 loss 12.1887 avg_loss 12.7443\n",
            "[Train] batch 935 loss 12.7127 avg_loss 12.7443\n",
            "[Train] batch 936 loss 11.9985 avg_loss 12.7435\n",
            "[Train] batch 937 loss 12.1481 avg_loss 12.7429\n",
            "[Train] batch 938 loss 11.5721 avg_loss 12.7416\n",
            "[Train] batch 939 loss 9.2467 avg_loss 12.7379\n",
            "[Train] batch 940 loss 11.7191 avg_loss 12.7368\n",
            "[Train] batch 941 loss 12.2536 avg_loss 12.7363\n",
            "[Train] batch 942 loss 12.1804 avg_loss 12.7357\n",
            "[Train] batch 943 loss 12.5186 avg_loss 12.7355\n",
            "[Train] batch 944 loss 10.8601 avg_loss 12.7335\n",
            "[Train] batch 945 loss 12.8982 avg_loss 12.7337\n",
            "[Train] batch 946 loss 11.2905 avg_loss 12.7322\n",
            "[Train] batch 947 loss 12.5808 avg_loss 12.7320\n",
            "[Train] batch 948 loss 9.9285 avg_loss 12.7290\n",
            "[Train] batch 949 loss 9.3186 avg_loss 12.7254\n",
            "[Train] batch 950 loss 12.1012 avg_loss 12.7248\n",
            "[Train] batch 951 loss 12.4900 avg_loss 12.7245\n",
            "[Train] batch 952 loss 14.2394 avg_loss 12.7261\n",
            "[Train] batch 953 loss 13.2962 avg_loss 12.7267\n",
            "[Train] batch 954 loss 13.8922 avg_loss 12.7280\n",
            "[Train] batch 955 loss 12.6625 avg_loss 12.7279\n",
            "[Train] batch 956 loss 13.7770 avg_loss 12.7290\n",
            "[Train] batch 957 loss 11.3930 avg_loss 12.7276\n",
            "[Train] batch 958 loss 12.8059 avg_loss 12.7277\n",
            "[Train] batch 959 loss 13.7324 avg_loss 12.7287\n",
            "[Train] batch 960 loss 13.0062 avg_loss 12.7290\n",
            "[Train] batch 961 loss 10.9832 avg_loss 12.7272\n",
            "[Train] batch 962 loss 11.7319 avg_loss 12.7262\n",
            "[Train] batch 963 loss 11.8462 avg_loss 12.7252\n",
            "[Train] batch 964 loss 11.4898 avg_loss 12.7240\n",
            "[Train] batch 965 loss 12.0889 avg_loss 12.7233\n",
            "[Train] batch 966 loss 12.3237 avg_loss 12.7229\n",
            "[Train] batch 967 loss 12.3266 avg_loss 12.7225\n",
            "[Train] batch 968 loss 13.8136 avg_loss 12.7236\n",
            "[Train] batch 969 loss 11.7147 avg_loss 12.7226\n",
            "[Train] batch 970 loss 12.0429 avg_loss 12.7219\n",
            "[Train] batch 971 loss 12.2406 avg_loss 12.7214\n",
            "[Train] batch 972 loss 12.7584 avg_loss 12.7214\n",
            "[Train] batch 973 loss 10.5009 avg_loss 12.7191\n",
            "[Train] batch 974 loss 11.1419 avg_loss 12.7175\n",
            "[Train] batch 975 loss 11.2484 avg_loss 12.7160\n",
            "[Train] batch 976 loss 12.4227 avg_loss 12.7157\n",
            "[Train] batch 977 loss 14.2439 avg_loss 12.7173\n",
            "[Train] batch 978 loss 13.2956 avg_loss 12.7179\n",
            "[Train] batch 979 loss 11.2271 avg_loss 12.7163\n",
            "[Train] batch 980 loss 10.4762 avg_loss 12.7140\n",
            "[Train] batch 981 loss 12.3735 avg_loss 12.7137\n",
            "[Train] batch 982 loss 13.1743 avg_loss 12.7142\n",
            "[Train] batch 983 loss 12.6545 avg_loss 12.7141\n",
            "[Train] batch 984 loss 13.4966 avg_loss 12.7149\n",
            "[Train] batch 985 loss 10.3772 avg_loss 12.7125\n",
            "[Train] batch 986 loss 12.1566 avg_loss 12.7120\n",
            "[Train] batch 987 loss 13.1477 avg_loss 12.7124\n",
            "[Train] batch 988 loss 12.4921 avg_loss 12.7122\n",
            "[Train] batch 989 loss 12.6101 avg_loss 12.7121\n",
            "[Train] batch 990 loss 10.8595 avg_loss 12.7102\n",
            "[Train] batch 991 loss 12.1085 avg_loss 12.7096\n",
            "[Train] batch 992 loss 11.4644 avg_loss 12.7083\n",
            "[Train] batch 993 loss 11.6884 avg_loss 12.7073\n",
            "[Train] batch 994 loss 10.7533 avg_loss 12.7054\n",
            "[Train] batch 995 loss 10.8466 avg_loss 12.7035\n",
            "[Train] batch 996 loss 11.8386 avg_loss 12.7026\n",
            "[Train] batch 997 loss 12.4421 avg_loss 12.7024\n",
            "[Train] batch 998 loss 12.4512 avg_loss 12.7021\n",
            "[Train] batch 999 loss 11.8144 avg_loss 12.7012\n",
            "[Train] batch 1000 loss 12.0449 avg_loss 12.7006\n",
            "[Train] batch 1001 loss 12.6665 avg_loss 12.7005\n",
            "[Train] batch 1002 loss 12.3601 avg_loss 12.7002\n",
            "[Train] batch 1003 loss 12.5479 avg_loss 12.7000\n",
            "[Train] batch 1004 loss 12.6964 avg_loss 12.7000\n",
            "[Train] batch 1005 loss 12.4162 avg_loss 12.6997\n",
            "[Train] batch 1006 loss 12.1973 avg_loss 12.6992\n",
            "[Train] batch 1007 loss 11.6936 avg_loss 12.6982\n",
            "[Train] batch 1008 loss 12.2341 avg_loss 12.6978\n",
            "[Train] batch 1009 loss 12.6565 avg_loss 12.6977\n",
            "[Train] batch 1010 loss 13.5502 avg_loss 12.6986\n",
            "[Train] batch 1011 loss 12.6391 avg_loss 12.6985\n",
            "[Train] batch 1012 loss 12.3094 avg_loss 12.6981\n",
            "[Train] batch 1013 loss 10.7550 avg_loss 12.6962\n",
            "[Train] batch 1014 loss 13.9498 avg_loss 12.6975\n",
            "[Train] batch 1015 loss 11.5704 avg_loss 12.6964\n",
            "[Train] batch 1016 loss 13.0638 avg_loss 12.6967\n",
            "[Train] batch 1017 loss 13.0137 avg_loss 12.6970\n",
            "[Train] batch 1018 loss 12.7554 avg_loss 12.6971\n",
            "[Train] batch 1019 loss 11.6080 avg_loss 12.6960\n",
            "[Train] batch 1020 loss 13.1920 avg_loss 12.6965\n",
            "[Train] batch 1021 loss 11.5150 avg_loss 12.6953\n",
            "[Train] batch 1022 loss 11.3400 avg_loss 12.6940\n",
            "[Train] batch 1023 loss 11.1810 avg_loss 12.6925\n",
            "[Train] batch 1024 loss 12.2915 avg_loss 12.6921\n",
            "[Train] batch 1025 loss 11.3792 avg_loss 12.6909\n",
            "[Train] batch 1026 loss 11.2028 avg_loss 12.6894\n",
            "[Train] batch 1027 loss 12.1519 avg_loss 12.6889\n",
            "[Train] batch 1028 loss 11.0924 avg_loss 12.6873\n",
            "[Train] batch 1029 loss 12.3841 avg_loss 12.6870\n",
            "[Train] batch 1030 loss 11.9388 avg_loss 12.6863\n",
            "[Train] batch 1031 loss 10.7933 avg_loss 12.6845\n",
            "[Train] batch 1032 loss 12.1403 avg_loss 12.6840\n",
            "[Train] batch 1033 loss 12.5288 avg_loss 12.6838\n",
            "[Train] batch 1034 loss 13.6096 avg_loss 12.6847\n",
            "[Train] batch 1035 loss 11.6366 avg_loss 12.6837\n",
            "[Train] batch 1036 loss 13.1660 avg_loss 12.6842\n",
            "[Train] batch 1037 loss 12.5395 avg_loss 12.6840\n",
            "[Train] batch 1038 loss 11.8921 avg_loss 12.6833\n",
            "[Train] batch 1039 loss 11.0571 avg_loss 12.6817\n",
            "[Train] batch 1040 loss 11.8472 avg_loss 12.6809\n",
            "[Train] batch 1041 loss 10.7928 avg_loss 12.6791\n",
            "[Train] batch 1042 loss 12.4604 avg_loss 12.6789\n",
            "[Train] batch 1043 loss 11.0159 avg_loss 12.6773\n",
            "[Train] batch 1044 loss 12.3084 avg_loss 12.6769\n",
            "[Train] batch 1045 loss 10.1862 avg_loss 12.6745\n",
            "[Train] batch 1046 loss 11.8607 avg_loss 12.6738\n",
            "[Train] batch 1047 loss 11.5597 avg_loss 12.6727\n",
            "[Train] batch 1048 loss 11.7944 avg_loss 12.6718\n",
            "[Train] batch 1049 loss 11.7552 avg_loss 12.6710\n",
            "[Train] batch 1050 loss 10.6631 avg_loss 12.6691\n",
            "[Train] batch 1051 loss 10.5134 avg_loss 12.6670\n",
            "[Train] batch 1052 loss 11.3564 avg_loss 12.6658\n",
            "[Train] batch 1053 loss 12.0553 avg_loss 12.6652\n",
            "[Train] batch 1054 loss 11.6375 avg_loss 12.6642\n",
            "[Train] batch 1055 loss 11.5014 avg_loss 12.6631\n",
            "[Train] batch 1056 loss 12.8389 avg_loss 12.6633\n",
            "[Train] batch 1057 loss 10.7611 avg_loss 12.6615\n",
            "[Train] batch 1058 loss 11.4151 avg_loss 12.6603\n",
            "[Train] batch 1059 loss 11.0564 avg_loss 12.6588\n",
            "[Train] batch 1060 loss 12.9757 avg_loss 12.6591\n",
            "[Train] batch 1061 loss 12.4570 avg_loss 12.6589\n",
            "[Train] batch 1062 loss 13.4627 avg_loss 12.6596\n",
            "[Train] batch 1063 loss 11.6356 avg_loss 12.6587\n",
            "[Train] batch 1064 loss 12.6087 avg_loss 12.6586\n",
            "[Train] batch 1065 loss 11.1169 avg_loss 12.6572\n",
            "[Train] batch 1066 loss 12.9705 avg_loss 12.6575\n",
            "[Train] batch 1067 loss 12.6534 avg_loss 12.6575\n",
            "[Train] batch 1068 loss 11.1307 avg_loss 12.6561\n",
            "[Train] batch 1069 loss 12.7489 avg_loss 12.6561\n",
            "[Train] batch 1070 loss 13.9990 avg_loss 12.6574\n",
            "[Train] batch 1071 loss 11.7887 avg_loss 12.6566\n",
            "[Train] batch 1072 loss 9.9910 avg_loss 12.6541\n",
            "[Train] batch 1073 loss 11.6304 avg_loss 12.6531\n",
            "[Train] batch 1074 loss 11.6052 avg_loss 12.6522\n",
            "[Train] batch 1075 loss 12.8164 avg_loss 12.6523\n",
            "[Train] batch 1076 loss 12.0244 avg_loss 12.6517\n",
            "[Train] batch 1077 loss 12.9589 avg_loss 12.6520\n",
            "[Train] batch 1078 loss 10.4850 avg_loss 12.6500\n",
            "[Train] batch 1079 loss 12.5880 avg_loss 12.6500\n",
            "[Train] batch 1080 loss 10.8218 avg_loss 12.6483\n",
            "[Train] batch 1081 loss 12.2459 avg_loss 12.6479\n",
            "[Train] batch 1082 loss 13.8519 avg_loss 12.6490\n",
            "[Train] batch 1083 loss 12.9462 avg_loss 12.6493\n",
            "[Train] batch 1084 loss 14.4666 avg_loss 12.6510\n",
            "[Train] batch 1085 loss 10.3686 avg_loss 12.6488\n",
            "[Train] batch 1086 loss 10.7007 avg_loss 12.6471\n",
            "[Train] batch 1087 loss 12.4714 avg_loss 12.6469\n",
            "[Train] batch 1088 loss 13.9519 avg_loss 12.6481\n",
            "[Train] batch 1089 loss 11.1397 avg_loss 12.6467\n",
            "[Train] batch 1090 loss 11.2618 avg_loss 12.6454\n",
            "[Train] batch 1091 loss 12.2962 avg_loss 12.6451\n",
            "[Train] batch 1092 loss 12.2534 avg_loss 12.6448\n",
            "[Train] batch 1093 loss 12.1083 avg_loss 12.6443\n",
            "[Train] batch 1094 loss 11.7627 avg_loss 12.6435\n",
            "[Train] batch 1095 loss 11.6428 avg_loss 12.6425\n",
            "[Train] batch 1096 loss 10.2114 avg_loss 12.6403\n",
            "[Train] batch 1097 loss 11.0874 avg_loss 12.6389\n",
            "[Train] batch 1098 loss 11.4220 avg_loss 12.6378\n",
            "[Train] batch 1099 loss 13.6747 avg_loss 12.6387\n",
            "[Train] batch 1100 loss 13.6671 avg_loss 12.6397\n",
            "[Train] batch 1101 loss 9.2505 avg_loss 12.6366\n",
            "[Train] batch 1102 loss 12.3645 avg_loss 12.6364\n",
            "[Train] batch 1103 loss 12.3239 avg_loss 12.6361\n",
            "[Train] batch 1104 loss 12.1618 avg_loss 12.6356\n",
            "[Train] batch 1105 loss 11.1902 avg_loss 12.6343\n",
            "[Train] batch 1106 loss 11.9417 avg_loss 12.6337\n",
            "[Train] batch 1107 loss 13.0700 avg_loss 12.6341\n",
            "[Train] batch 1108 loss 11.7487 avg_loss 12.6333\n",
            "[Train] batch 1109 loss 13.8529 avg_loss 12.6344\n",
            "[Train] batch 1110 loss 12.1354 avg_loss 12.6340\n",
            "[Train] batch 1111 loss 12.1127 avg_loss 12.6335\n",
            "[Train] batch 1112 loss 12.0909 avg_loss 12.6330\n",
            "[Train] batch 1113 loss 12.3272 avg_loss 12.6327\n",
            "[Train] batch 1114 loss 12.1925 avg_loss 12.6323\n",
            "[Train] batch 1115 loss 14.2827 avg_loss 12.6338\n",
            "[Train] batch 1116 loss 13.4390 avg_loss 12.6345\n",
            "[Train] batch 1117 loss 13.3897 avg_loss 12.6352\n",
            "[Train] batch 1118 loss 12.2157 avg_loss 12.6348\n",
            "[Train] batch 1119 loss 11.3185 avg_loss 12.6337\n",
            "[Train] batch 1120 loss 11.8114 avg_loss 12.6329\n",
            "[Train] batch 1121 loss 10.5089 avg_loss 12.6310\n",
            "[Train] batch 1122 loss 12.2653 avg_loss 12.6307\n",
            "[Train] batch 1123 loss 12.5062 avg_loss 12.6306\n",
            "[Train] batch 1124 loss 13.1769 avg_loss 12.6311\n",
            "[Train] batch 1125 loss 11.1312 avg_loss 12.6297\n",
            "[Train] batch 1126 loss 11.7993 avg_loss 12.6290\n",
            "[Train] batch 1127 loss 10.2548 avg_loss 12.6269\n",
            "[Train] batch 1128 loss 10.9498 avg_loss 12.6254\n",
            "[Train] batch 1129 loss 13.0003 avg_loss 12.6257\n",
            "[Train] batch 1130 loss 12.9313 avg_loss 12.6260\n",
            "[Train] batch 1131 loss 12.1762 avg_loss 12.6256\n",
            "[Train] batch 1132 loss 13.1306 avg_loss 12.6261\n",
            "[Train] batch 1133 loss 11.2536 avg_loss 12.6249\n",
            "[Train] batch 1134 loss 12.7205 avg_loss 12.6249\n",
            "[Train] batch 1135 loss 11.6947 avg_loss 12.6241\n",
            "[Train] batch 1136 loss 10.5791 avg_loss 12.6223\n",
            "[Train] batch 1137 loss 11.4738 avg_loss 12.6213\n",
            "[Train] batch 1138 loss 11.5177 avg_loss 12.6203\n",
            "[Train] batch 1139 loss 13.2955 avg_loss 12.6209\n",
            "[Train] batch 1140 loss 11.2466 avg_loss 12.6197\n",
            "[Train] batch 1141 loss 10.8254 avg_loss 12.6182\n",
            "[Train] batch 1142 loss 11.5071 avg_loss 12.6172\n",
            "[Train] batch 1143 loss 9.8014 avg_loss 12.6147\n",
            "[Train] batch 1144 loss 11.2314 avg_loss 12.6135\n",
            "[Train] batch 1145 loss 12.7375 avg_loss 12.6136\n",
            "[Train] batch 1146 loss 12.1913 avg_loss 12.6132\n",
            "[Train] batch 1147 loss 11.8076 avg_loss 12.6125\n",
            "[Train] batch 1148 loss 12.3097 avg_loss 12.6123\n",
            "[Train] batch 1149 loss 11.1958 avg_loss 12.6110\n",
            "[Train] batch 1150 loss 11.7705 avg_loss 12.6103\n",
            "[Train] batch 1151 loss 13.1809 avg_loss 12.6108\n",
            "[Train] batch 1152 loss 12.9220 avg_loss 12.6111\n",
            "[Train] batch 1153 loss 11.4117 avg_loss 12.6100\n",
            "[Train] batch 1154 loss 9.7214 avg_loss 12.6075\n",
            "[Train] batch 1155 loss 9.6279 avg_loss 12.6050\n",
            "[Train] batch 1156 loss 9.6615 avg_loss 12.6024\n",
            "[Train] batch 1157 loss 13.0547 avg_loss 12.6028\n",
            "[Train] batch 1158 loss 10.7101 avg_loss 12.6012\n",
            "[Train] batch 1159 loss 11.0385 avg_loss 12.5998\n",
            "[Train] batch 1160 loss 12.3344 avg_loss 12.5996\n",
            "[Train] batch 1161 loss 12.5223 avg_loss 12.5995\n",
            "[Train] batch 1162 loss 12.9396 avg_loss 12.5998\n",
            "[Train] batch 1163 loss 12.7376 avg_loss 12.5999\n",
            "[Train] batch 1164 loss 12.7044 avg_loss 12.6000\n",
            "[Train] batch 1165 loss 12.9037 avg_loss 12.6003\n",
            "[Train] batch 1166 loss 12.0255 avg_loss 12.5998\n",
            "[Train] batch 1167 loss 13.2204 avg_loss 12.6003\n",
            "[Train] batch 1168 loss 11.6647 avg_loss 12.5995\n",
            "[Train] batch 1169 loss 11.2898 avg_loss 12.5984\n",
            "[Train] batch 1170 loss 12.1611 avg_loss 12.5980\n",
            "[Train] batch 1171 loss 12.9524 avg_loss 12.5983\n",
            "[Train] batch 1172 loss 10.9341 avg_loss 12.5969\n",
            "[Train] batch 1173 loss 10.7981 avg_loss 12.5954\n",
            "[Train] batch 1174 loss 10.4654 avg_loss 12.5936\n",
            "[Train] batch 1175 loss 12.2630 avg_loss 12.5933\n",
            "[Train] batch 1176 loss 12.2186 avg_loss 12.5930\n",
            "[Train] batch 1177 loss 13.1898 avg_loss 12.5935\n",
            "[Train] batch 1178 loss 13.4125 avg_loss 12.5942\n",
            "[Train] batch 1179 loss 11.6607 avg_loss 12.5934\n",
            "[Train] batch 1180 loss 11.8638 avg_loss 12.5928\n",
            "[Train] batch 1181 loss 10.9737 avg_loss 12.5914\n",
            "[Train] batch 1182 loss 12.4929 avg_loss 12.5913\n",
            "[Train] batch 1183 loss 10.8521 avg_loss 12.5898\n",
            "[Train] batch 1184 loss 12.8040 avg_loss 12.5900\n",
            "[Train] batch 1185 loss 12.3680 avg_loss 12.5898\n",
            "[Train] batch 1186 loss 13.5670 avg_loss 12.5906\n",
            "[Train] batch 1187 loss 12.7802 avg_loss 12.5908\n",
            "[Train] batch 1188 loss 13.7279 avg_loss 12.5918\n",
            "[Train] batch 1189 loss 11.7708 avg_loss 12.5911\n",
            "[Train] batch 1190 loss 14.5120 avg_loss 12.5927\n",
            "[Train] batch 1191 loss 10.9135 avg_loss 12.5913\n",
            "[Train] batch 1192 loss 13.0725 avg_loss 12.5917\n",
            "[Train] batch 1193 loss 11.2301 avg_loss 12.5905\n",
            "[Train] batch 1194 loss 11.3048 avg_loss 12.5895\n",
            "[Train] batch 1195 loss 12.2896 avg_loss 12.5892\n",
            "[Train] batch 1196 loss 10.6630 avg_loss 12.5876\n",
            "[Train] batch 1197 loss 12.5774 avg_loss 12.5876\n",
            "[Train] batch 1198 loss 11.8484 avg_loss 12.5870\n",
            "[Train] batch 1199 loss 11.0405 avg_loss 12.5857\n",
            "[Train] batch 1200 loss 11.7164 avg_loss 12.5850\n",
            "[Train] batch 1201 loss 8.4409 avg_loss 12.5815\n",
            "[Train] batch 1202 loss 12.6358 avg_loss 12.5816\n",
            "[Train] batch 1203 loss 12.6455 avg_loss 12.5816\n",
            "[Train] batch 1204 loss 9.6738 avg_loss 12.5792\n",
            "[Train] batch 1205 loss 11.4491 avg_loss 12.5783\n",
            "[Train] batch 1206 loss 11.8612 avg_loss 12.5777\n",
            "[Train] batch 1207 loss 14.1257 avg_loss 12.5789\n",
            "[Train] batch 1208 loss 10.8498 avg_loss 12.5775\n",
            "[Train] batch 1209 loss 10.8612 avg_loss 12.5761\n",
            "[Train] batch 1210 loss 11.3092 avg_loss 12.5750\n",
            "[Train] batch 1211 loss 11.3520 avg_loss 12.5740\n",
            "[Train] batch 1212 loss 12.9104 avg_loss 12.5743\n",
            "[Train] batch 1213 loss 12.8618 avg_loss 12.5746\n",
            "[Train] batch 1214 loss 11.9568 avg_loss 12.5740\n",
            "[Train] batch 1215 loss 13.1169 avg_loss 12.5745\n",
            "[Train] batch 1216 loss 10.0970 avg_loss 12.5725\n",
            "[Train] batch 1217 loss 11.5608 avg_loss 12.5716\n",
            "[Train] batch 1218 loss 11.6723 avg_loss 12.5709\n",
            "[Train] batch 1219 loss 11.4805 avg_loss 12.5700\n",
            "[Train] batch 1220 loss 10.3251 avg_loss 12.5681\n",
            "[Train] batch 1221 loss 13.2074 avg_loss 12.5687\n",
            "[Train] batch 1222 loss 12.5078 avg_loss 12.5686\n",
            "[Train] batch 1223 loss 11.0280 avg_loss 12.5674\n",
            "[Train] batch 1224 loss 12.4055 avg_loss 12.5672\n",
            "[Train] batch 1225 loss 11.5613 avg_loss 12.5664\n",
            "[Train] batch 1226 loss 12.8537 avg_loss 12.5666\n",
            "[Train] batch 1227 loss 10.0794 avg_loss 12.5646\n",
            "[Train] batch 1228 loss 12.7525 avg_loss 12.5648\n",
            "[Train] batch 1229 loss 13.0949 avg_loss 12.5652\n",
            "[Train] batch 1230 loss 11.1959 avg_loss 12.5641\n",
            "[Train] batch 1231 loss 9.9123 avg_loss 12.5619\n",
            "[Train] batch 1232 loss 12.9660 avg_loss 12.5623\n",
            "[Train] batch 1233 loss 12.0988 avg_loss 12.5619\n",
            "[Train] batch 1234 loss 9.4132 avg_loss 12.5593\n",
            "[Train] batch 1235 loss 12.2162 avg_loss 12.5591\n",
            "[Train] batch 1236 loss 11.7105 avg_loss 12.5584\n",
            "[Train] batch 1237 loss 11.2520 avg_loss 12.5573\n",
            "[Train] batch 1238 loss 11.7804 avg_loss 12.5567\n",
            "[Train] batch 1239 loss 13.7764 avg_loss 12.5577\n",
            "[Train] batch 1240 loss 11.8063 avg_loss 12.5571\n",
            "[Train] batch 1241 loss 9.7675 avg_loss 12.5548\n",
            "[Train] batch 1242 loss 11.8343 avg_loss 12.5542\n",
            "[Train] batch 1243 loss 12.3285 avg_loss 12.5541\n",
            "[Train] batch 1244 loss 12.4767 avg_loss 12.5540\n",
            "[Train] batch 1245 loss 12.2472 avg_loss 12.5537\n",
            "[Train] batch 1246 loss 13.0428 avg_loss 12.5541\n",
            "[Train] batch 1247 loss 12.1509 avg_loss 12.5538\n",
            "[Train] batch 1248 loss 11.1737 avg_loss 12.5527\n",
            "[Train] batch 1249 loss 12.6341 avg_loss 12.5528\n",
            "[Train] batch 1250 loss 11.5470 avg_loss 12.5520\n",
            "[Train] batch 1251 loss 12.8686 avg_loss 12.5522\n",
            "[Train] batch 1252 loss 13.2038 avg_loss 12.5527\n",
            "[Train] batch 1253 loss 9.0652 avg_loss 12.5500\n",
            "[Train] batch 1254 loss 11.6787 avg_loss 12.5493\n",
            "[Train] batch 1255 loss 10.9361 avg_loss 12.5480\n",
            "[Train] batch 1256 loss 11.5365 avg_loss 12.5472\n",
            "[Train] batch 1257 loss 13.1552 avg_loss 12.5477\n",
            "[Train] batch 1258 loss 11.7716 avg_loss 12.5470\n",
            "[Train] batch 1259 loss 12.3749 avg_loss 12.5469\n",
            "[Train] batch 1260 loss 11.6583 avg_loss 12.5462\n",
            "[Train] batch 1261 loss 11.1072 avg_loss 12.5451\n",
            "[Train] batch 1262 loss 11.0596 avg_loss 12.5439\n",
            "[Train] batch 1263 loss 12.5249 avg_loss 12.5439\n",
            "[Train] batch 1264 loss 10.6264 avg_loss 12.5424\n",
            "[Train] batch 1265 loss 12.0939 avg_loss 12.5420\n",
            "[Train] batch 1266 loss 12.4715 avg_loss 12.5419\n",
            "[Train] batch 1267 loss 14.4732 avg_loss 12.5435\n",
            "[Train] batch 1268 loss 11.7802 avg_loss 12.5429\n",
            "[Train] batch 1269 loss 11.6290 avg_loss 12.5421\n",
            "[Train] batch 1270 loss 11.9963 avg_loss 12.5417\n",
            "[Train] batch 1271 loss 12.4181 avg_loss 12.5416\n",
            "[Train] batch 1272 loss 10.1833 avg_loss 12.5398\n",
            "[Train] batch 1273 loss 13.1057 avg_loss 12.5402\n",
            "[Train] batch 1274 loss 11.2514 avg_loss 12.5392\n",
            "[Train] batch 1275 loss 12.2871 avg_loss 12.5390\n",
            "[Train] batch 1276 loss 11.5235 avg_loss 12.5382\n",
            "[Train] batch 1277 loss 12.0893 avg_loss 12.5378\n",
            "[Train] batch 1278 loss 11.5375 avg_loss 12.5371\n",
            "[Train] batch 1279 loss 11.0544 avg_loss 12.5359\n",
            "[Train] batch 1280 loss 12.4692 avg_loss 12.5359\n",
            "[Train] batch 1281 loss 11.5681 avg_loss 12.5351\n",
            "[Train] batch 1282 loss 11.5155 avg_loss 12.5343\n",
            "[Train] batch 1283 loss 10.5990 avg_loss 12.5328\n",
            "[Train] batch 1284 loss 11.9795 avg_loss 12.5324\n",
            "[Train] batch 1285 loss 12.8129 avg_loss 12.5326\n",
            "[Train] batch 1286 loss 13.3107 avg_loss 12.5332\n",
            "[Train] batch 1287 loss 12.6464 avg_loss 12.5333\n",
            "[Train] batch 1288 loss 13.5164 avg_loss 12.5340\n",
            "[Train] batch 1289 loss 12.8300 avg_loss 12.5343\n",
            "[Train] batch 1290 loss 10.4660 avg_loss 12.5327\n",
            "[Train] batch 1291 loss 10.5430 avg_loss 12.5311\n",
            "[Train] batch 1292 loss 12.4227 avg_loss 12.5310\n",
            "[Train] batch 1293 loss 11.8260 avg_loss 12.5305\n",
            "[Train] batch 1294 loss 10.3408 avg_loss 12.5288\n",
            "[Train] batch 1295 loss 11.5870 avg_loss 12.5281\n",
            "[Train] batch 1296 loss 10.2903 avg_loss 12.5263\n",
            "[Train] batch 1297 loss 11.5942 avg_loss 12.5256\n",
            "[Train] batch 1298 loss 12.5694 avg_loss 12.5257\n",
            "[Train] batch 1299 loss 13.6965 avg_loss 12.5266\n",
            "[Train] batch 1300 loss 12.8863 avg_loss 12.5268\n",
            "[Train] batch 1301 loss 9.1485 avg_loss 12.5242\n",
            "[Train] batch 1302 loss 13.0770 avg_loss 12.5247\n",
            "[Train] batch 1303 loss 12.7163 avg_loss 12.5248\n",
            "[Train] batch 1304 loss 11.8988 avg_loss 12.5243\n",
            "[Train] batch 1305 loss 13.0753 avg_loss 12.5248\n",
            "[Train] batch 1306 loss 11.3610 avg_loss 12.5239\n",
            "[Train] batch 1307 loss 13.0063 avg_loss 12.5242\n",
            "[Train] batch 1308 loss 12.5654 avg_loss 12.5243\n",
            "[Train] batch 1309 loss 11.7455 avg_loss 12.5237\n",
            "[Train] batch 1310 loss 13.5710 avg_loss 12.5245\n",
            "[Train] batch 1311 loss 12.6517 avg_loss 12.5246\n",
            "[Train] batch 1312 loss 12.9353 avg_loss 12.5249\n",
            "[Train] batch 1313 loss 13.0934 avg_loss 12.5253\n",
            "[Train] batch 1314 loss 12.1657 avg_loss 12.5250\n",
            "[Train] batch 1315 loss 12.2697 avg_loss 12.5248\n",
            "[Train] batch 1316 loss 11.1911 avg_loss 12.5238\n",
            "[Train] batch 1317 loss 12.9404 avg_loss 12.5242\n",
            "[Train] batch 1318 loss 10.9791 avg_loss 12.5230\n",
            "[Train] batch 1319 loss 12.1696 avg_loss 12.5227\n",
            "[Train] batch 1320 loss 12.3477 avg_loss 12.5226\n",
            "[Train] batch 1321 loss 12.8121 avg_loss 12.5228\n",
            "[Train] batch 1322 loss 11.1227 avg_loss 12.5217\n",
            "[Train] batch 1323 loss 11.6795 avg_loss 12.5211\n",
            "[Train] batch 1324 loss 10.7039 avg_loss 12.5197\n",
            "[Train] batch 1325 loss 11.1814 avg_loss 12.5187\n",
            "[Train] batch 1326 loss 9.9142 avg_loss 12.5168\n",
            "[Train] batch 1327 loss 12.3325 avg_loss 12.5166\n",
            "[Train] batch 1328 loss 12.1584 avg_loss 12.5163\n",
            "[Train] batch 1329 loss 12.5351 avg_loss 12.5164\n",
            "[Train] batch 1330 loss 12.4517 avg_loss 12.5163\n",
            "[Train] batch 1331 loss 11.9610 avg_loss 12.5159\n",
            "[Train] batch 1332 loss 12.6395 avg_loss 12.5160\n",
            "[Train] batch 1333 loss 13.0450 avg_loss 12.5164\n",
            "[Train] batch 1334 loss 12.6943 avg_loss 12.5165\n",
            "[Train] batch 1335 loss 11.6807 avg_loss 12.5159\n",
            "[Train] batch 1336 loss 11.9536 avg_loss 12.5155\n",
            "[Train] batch 1337 loss 13.8866 avg_loss 12.5165\n",
            "[Train] batch 1338 loss 13.6262 avg_loss 12.5173\n",
            "[Train] batch 1339 loss 12.2463 avg_loss 12.5171\n",
            "[Train] batch 1340 loss 11.5428 avg_loss 12.5164\n",
            "[Train] batch 1341 loss 11.7811 avg_loss 12.5158\n",
            "[Train] batch 1342 loss 13.3840 avg_loss 12.5165\n",
            "[Train] batch 1343 loss 12.1340 avg_loss 12.5162\n",
            "[Train] batch 1344 loss 11.6962 avg_loss 12.5156\n",
            "[Train] batch 1345 loss 13.2790 avg_loss 12.5162\n",
            "[Train] batch 1346 loss 13.1033 avg_loss 12.5166\n",
            "[Train] batch 1347 loss 12.8228 avg_loss 12.5168\n",
            "[Train] batch 1348 loss 8.9473 avg_loss 12.5142\n",
            "[Train] batch 1349 loss 13.1529 avg_loss 12.5147\n",
            "[Train] batch 1350 loss 12.4440 avg_loss 12.5146\n",
            "[Train] batch 1351 loss 12.0139 avg_loss 12.5142\n",
            "[Train] batch 1352 loss 11.4147 avg_loss 12.5134\n",
            "[Train] batch 1353 loss 12.4141 avg_loss 12.5133\n",
            "[Train] batch 1354 loss 12.4153 avg_loss 12.5133\n",
            "[Train] batch 1355 loss 11.4712 avg_loss 12.5125\n",
            "[Train] batch 1356 loss 12.2354 avg_loss 12.5123\n",
            "[Train] batch 1357 loss 10.5061 avg_loss 12.5108\n",
            "[Train] batch 1358 loss 12.0411 avg_loss 12.5105\n",
            "[Train] batch 1359 loss 12.4519 avg_loss 12.5104\n",
            "[Train] batch 1360 loss 12.0039 avg_loss 12.5101\n",
            "[Train] batch 1361 loss 10.7619 avg_loss 12.5088\n",
            "[Train] batch 1362 loss 11.6167 avg_loss 12.5081\n",
            "[Train] batch 1363 loss 11.8740 avg_loss 12.5077\n",
            "[Train] batch 1364 loss 11.4350 avg_loss 12.5069\n",
            "[Train] batch 1365 loss 12.2289 avg_loss 12.5067\n",
            "[Train] batch 1366 loss 12.8581 avg_loss 12.5069\n",
            "[Train] batch 1367 loss 11.0180 avg_loss 12.5058\n",
            "[Train] batch 1368 loss 11.9106 avg_loss 12.5054\n",
            "[Train] batch 1369 loss 13.8100 avg_loss 12.5064\n",
            "[Train] batch 1370 loss 13.3302 avg_loss 12.5070\n",
            "[Train] batch 1371 loss 12.1062 avg_loss 12.5067\n",
            "[Train] batch 1372 loss 8.7797 avg_loss 12.5039\n",
            "[Train] batch 1373 loss 8.9700 avg_loss 12.5014\n",
            "[Train] batch 1374 loss 10.9677 avg_loss 12.5003\n",
            "[Train] batch 1375 loss 9.3861 avg_loss 12.4980\n",
            "[Train] batch 1376 loss 11.7895 avg_loss 12.4975\n",
            "[Train] batch 1377 loss 13.9215 avg_loss 12.4985\n",
            "[Train] batch 1378 loss 12.6613 avg_loss 12.4986\n",
            "[Train] batch 1379 loss 11.7866 avg_loss 12.4981\n",
            "[Train] batch 1380 loss 12.8245 avg_loss 12.4983\n",
            "[Train] batch 1381 loss 13.0796 avg_loss 12.4988\n",
            "[Train] batch 1382 loss 11.6097 avg_loss 12.4981\n",
            "[Train] batch 1383 loss 10.5131 avg_loss 12.4967\n",
            "[Train] batch 1384 loss 12.6971 avg_loss 12.4968\n",
            "[Train] batch 1385 loss 14.5982 avg_loss 12.4984\n",
            "[Train] batch 1386 loss 10.7059 avg_loss 12.4971\n",
            "[Train] batch 1387 loss 11.3860 avg_loss 12.4963\n",
            "[Train] batch 1388 loss 11.9934 avg_loss 12.4959\n",
            "[Train] batch 1389 loss 11.7576 avg_loss 12.4954\n",
            "[Train] batch 1390 loss 12.0754 avg_loss 12.4951\n",
            "[Train] batch 1391 loss 12.8102 avg_loss 12.4953\n",
            "[Train] batch 1392 loss 11.0525 avg_loss 12.4943\n",
            "[Train] batch 1393 loss 12.1706 avg_loss 12.4940\n",
            "[Train] batch 1394 loss 11.9434 avg_loss 12.4936\n",
            "[Train] batch 1395 loss 11.2436 avg_loss 12.4927\n",
            "[Train] batch 1396 loss 12.1657 avg_loss 12.4925\n",
            "[Train] batch 1397 loss 14.3644 avg_loss 12.4938\n",
            "[Train] batch 1398 loss 14.2930 avg_loss 12.4951\n",
            "[Train] batch 1399 loss 12.7524 avg_loss 12.4953\n",
            "[Train] batch 1400 loss 12.0441 avg_loss 12.4950\n",
            "[Train] batch 1401 loss 11.2490 avg_loss 12.4941\n",
            "[Train] batch 1402 loss 11.9135 avg_loss 12.4937\n",
            "[Train] batch 1403 loss 13.2907 avg_loss 12.4942\n",
            "[Train] batch 1404 loss 9.4412 avg_loss 12.4921\n",
            "[Train] batch 1405 loss 11.4895 avg_loss 12.4914\n",
            "[Train] batch 1406 loss 11.7982 avg_loss 12.4909\n",
            "[Train] batch 1407 loss 10.6740 avg_loss 12.4896\n",
            "[Train] batch 1408 loss 12.9064 avg_loss 12.4899\n",
            "[Train] batch 1409 loss 11.1871 avg_loss 12.4889\n",
            "[Train] batch 1410 loss 10.6400 avg_loss 12.4876\n",
            "[Train] batch 1411 loss 12.5636 avg_loss 12.4877\n",
            "[Train] batch 1412 loss 11.2759 avg_loss 12.4868\n",
            "[Train] batch 1413 loss 11.4600 avg_loss 12.4861\n",
            "[Train] batch 1414 loss 12.5816 avg_loss 12.4862\n",
            "[Train] batch 1415 loss 11.5927 avg_loss 12.4855\n",
            "[Train] batch 1416 loss 13.0712 avg_loss 12.4860\n",
            "[Train] batch 1417 loss 11.8960 avg_loss 12.4855\n",
            "[Train] batch 1418 loss 12.0120 avg_loss 12.4852\n",
            "[Train] batch 1419 loss 12.9144 avg_loss 12.4855\n",
            "[Train] batch 1420 loss 12.1556 avg_loss 12.4853\n",
            "[Train] batch 1421 loss 12.4605 avg_loss 12.4853\n",
            "[Train] batch 1422 loss 12.9675 avg_loss 12.4856\n",
            "[Train] batch 1423 loss 11.4600 avg_loss 12.4849\n",
            "[Train] batch 1424 loss 13.5645 avg_loss 12.4856\n",
            "[Train] batch 1425 loss 14.3576 avg_loss 12.4869\n",
            "[Train] batch 1426 loss 11.9295 avg_loss 12.4866\n",
            "[Train] batch 1427 loss 11.6468 avg_loss 12.4860\n",
            "[Train] batch 1428 loss 10.1054 avg_loss 12.4843\n",
            "[Train] batch 1429 loss 13.3475 avg_loss 12.4849\n",
            "[Train] batch 1430 loss 11.9722 avg_loss 12.4845\n",
            "[Train] batch 1431 loss 12.5897 avg_loss 12.4846\n",
            "[Train] batch 1432 loss 11.9024 avg_loss 12.4842\n",
            "[Train] batch 1433 loss 12.7327 avg_loss 12.4844\n",
            "[Train] batch 1434 loss 11.6902 avg_loss 12.4838\n",
            "[Train] batch 1435 loss 12.2475 avg_loss 12.4837\n",
            "[Train] batch 1436 loss 12.4740 avg_loss 12.4837\n",
            "[Train] batch 1437 loss 11.3421 avg_loss 12.4829\n",
            "[Train] batch 1438 loss 12.1456 avg_loss 12.4826\n",
            "[Train] batch 1439 loss 12.8156 avg_loss 12.4829\n",
            "[Train] batch 1440 loss 9.2000 avg_loss 12.4806\n",
            "[Train] batch 1441 loss 13.8481 avg_loss 12.4815\n",
            "[Train] batch 1442 loss 13.5531 avg_loss 12.4823\n",
            "[Train] batch 1443 loss 11.1737 avg_loss 12.4814\n",
            "[Train] batch 1444 loss 11.0711 avg_loss 12.4804\n",
            "[Train] batch 1445 loss 10.9587 avg_loss 12.4793\n",
            "[Train] batch 1446 loss 11.8854 avg_loss 12.4789\n",
            "[Train] batch 1447 loss 10.9762 avg_loss 12.4779\n",
            "[Train] batch 1448 loss 12.6994 avg_loss 12.4780\n",
            "[Train] batch 1449 loss 11.3511 avg_loss 12.4773\n",
            "[Train] batch 1450 loss 11.5699 avg_loss 12.4766\n",
            "[Train] batch 1451 loss 12.4720 avg_loss 12.4766\n",
            "[Train] batch 1452 loss 11.8004 avg_loss 12.4762\n",
            "[Train] batch 1453 loss 12.0899 avg_loss 12.4759\n",
            "[Train] batch 1454 loss 13.4144 avg_loss 12.4765\n",
            "[Train] batch 1455 loss 11.8965 avg_loss 12.4762\n",
            "[Train] batch 1456 loss 12.2969 avg_loss 12.4760\n",
            "[Train] batch 1457 loss 10.0327 avg_loss 12.4744\n",
            "[Train] batch 1458 loss 10.5505 avg_loss 12.4730\n",
            "[Train] batch 1459 loss 12.0651 avg_loss 12.4728\n",
            "[Train] batch 1460 loss 13.2443 avg_loss 12.4733\n",
            "[Train] batch 1461 loss 12.2560 avg_loss 12.4731\n",
            "[Train] batch 1462 loss 11.5125 avg_loss 12.4725\n",
            "[Train] batch 1463 loss 11.0351 avg_loss 12.4715\n",
            "[Train] batch 1464 loss 10.9957 avg_loss 12.4705\n",
            "[Train] batch 1465 loss 9.7833 avg_loss 12.4686\n",
            "[Train] batch 1466 loss 9.2965 avg_loss 12.4665\n",
            "[Train] batch 1467 loss 12.8508 avg_loss 12.4667\n",
            "[Train] batch 1468 loss 11.2056 avg_loss 12.4659\n",
            "[Train] batch 1469 loss 12.0009 avg_loss 12.4656\n",
            "[Train] batch 1470 loss 12.2283 avg_loss 12.4654\n",
            "[Train] batch 1471 loss 11.0971 avg_loss 12.4645\n",
            "[Train] batch 1472 loss 12.4230 avg_loss 12.4645\n",
            "[Train] batch 1473 loss 11.8773 avg_loss 12.4641\n",
            "[Train] batch 1474 loss 11.8004 avg_loss 12.4636\n",
            "[Train] batch 1475 loss 13.0880 avg_loss 12.4640\n",
            "[Train] batch 1476 loss 10.6150 avg_loss 12.4628\n",
            "[Train] batch 1477 loss 12.0442 avg_loss 12.4625\n",
            "[Train] batch 1478 loss 14.6987 avg_loss 12.4640\n",
            "[Train] batch 1479 loss 11.7575 avg_loss 12.4635\n",
            "[Train] batch 1480 loss 11.7265 avg_loss 12.4630\n",
            "[Train] batch 1481 loss 12.2020 avg_loss 12.4629\n",
            "[Train] batch 1482 loss 11.5135 avg_loss 12.4622\n",
            "[Train] batch 1483 loss 11.8905 avg_loss 12.4618\n",
            "[Train] batch 1484 loss 11.6020 avg_loss 12.4612\n",
            "[Train] batch 1485 loss 11.3644 avg_loss 12.4605\n",
            "[Train] batch 1486 loss 12.2481 avg_loss 12.4604\n",
            "[Train] batch 1487 loss 13.4944 avg_loss 12.4611\n",
            "[Train] batch 1488 loss 13.6228 avg_loss 12.4618\n",
            "[Train] batch 1489 loss 10.5026 avg_loss 12.4605\n",
            "[Train] batch 1490 loss 13.8192 avg_loss 12.4614\n",
            "[Train] batch 1491 loss 12.0210 avg_loss 12.4611\n",
            "[Train] batch 1492 loss 10.9410 avg_loss 12.4601\n",
            "[Train] batch 1493 loss 10.3303 avg_loss 12.4587\n",
            "[Train] batch 1494 loss 9.7037 avg_loss 12.4569\n",
            "[Train] batch 1495 loss 12.9120 avg_loss 12.4572\n",
            "[Train] batch 1496 loss 9.8028 avg_loss 12.4554\n",
            "[Train] batch 1497 loss 12.9924 avg_loss 12.4557\n",
            "[Train] batch 1498 loss 12.0824 avg_loss 12.4555\n",
            "[Train] batch 1499 loss 11.5097 avg_loss 12.4549\n",
            "[Train] batch 1500 loss 11.3615 avg_loss 12.4541\n",
            "[Train] batch 1501 loss 11.6047 avg_loss 12.4536\n",
            "[Train] batch 1502 loss 11.4437 avg_loss 12.4529\n",
            "[Train] batch 1503 loss 11.6611 avg_loss 12.4524\n",
            "[Train] batch 1504 loss 9.9869 avg_loss 12.4507\n",
            "[Train] batch 1505 loss 11.1867 avg_loss 12.4499\n",
            "[Train] batch 1506 loss 11.1791 avg_loss 12.4490\n",
            "[Train] batch 1507 loss 14.0694 avg_loss 12.4501\n",
            "[Train] batch 1508 loss 13.5026 avg_loss 12.4508\n",
            "[Train] batch 1509 loss 11.4094 avg_loss 12.4501\n",
            "[Train] batch 1510 loss 13.7006 avg_loss 12.4510\n",
            "[Train] batch 1511 loss 10.6708 avg_loss 12.4498\n",
            "[Train] batch 1512 loss 11.0382 avg_loss 12.4488\n",
            "[Train] batch 1513 loss 12.5083 avg_loss 12.4489\n",
            "[Train] batch 1514 loss 12.3531 avg_loss 12.4488\n",
            "[Train] batch 1515 loss 11.3606 avg_loss 12.4481\n",
            "[Train] batch 1516 loss 12.9737 avg_loss 12.4484\n",
            "[Train] batch 1517 loss 10.1231 avg_loss 12.4469\n",
            "[Train] batch 1518 loss 14.3063 avg_loss 12.4481\n",
            "[Train] batch 1519 loss 11.5948 avg_loss 12.4476\n",
            "[Train] batch 1520 loss 11.0206 avg_loss 12.4466\n",
            "[Train] batch 1521 loss 12.8463 avg_loss 12.4469\n",
            "[Train] batch 1522 loss 12.4074 avg_loss 12.4469\n",
            "[Train] batch 1523 loss 12.2325 avg_loss 12.4467\n",
            "[Train] batch 1524 loss 10.1138 avg_loss 12.4452\n",
            "[Train] batch 1525 loss 11.3520 avg_loss 12.4445\n",
            "[Train] batch 1526 loss 10.9217 avg_loss 12.4435\n",
            "[Train] batch 1527 loss 11.3303 avg_loss 12.4428\n",
            "[Train] batch 1528 loss 11.5439 avg_loss 12.4422\n",
            "[Train] batch 1529 loss 10.4849 avg_loss 12.4409\n",
            "[Train] batch 1530 loss 12.8632 avg_loss 12.4412\n",
            "[Train] batch 1531 loss 11.2474 avg_loss 12.4404\n",
            "[Train] batch 1532 loss 13.9064 avg_loss 12.4413\n",
            "[Train] batch 1533 loss 10.9289 avg_loss 12.4404\n",
            "[Train] batch 1534 loss 12.7693 avg_loss 12.4406\n",
            "[Train] batch 1535 loss 10.4377 avg_loss 12.4393\n",
            "[Train] batch 1536 loss 10.4479 avg_loss 12.4380\n",
            "[Train] batch 1537 loss 11.7954 avg_loss 12.4376\n",
            "[Train] batch 1538 loss 12.5722 avg_loss 12.4376\n",
            "[Train] batch 1539 loss 12.5804 avg_loss 12.4377\n",
            "[Train] batch 1540 loss 11.8693 avg_loss 12.4374\n",
            "[Train] batch 1541 loss 12.7261 avg_loss 12.4376\n",
            "[Train] batch 1542 loss 12.4333 avg_loss 12.4376\n",
            "[Train] batch 1543 loss 12.0014 avg_loss 12.4373\n",
            "[Train] batch 1544 loss 12.9533 avg_loss 12.4376\n",
            "[Train] batch 1545 loss 14.1913 avg_loss 12.4387\n",
            "[Train] batch 1546 loss 11.6711 avg_loss 12.4382\n",
            "[Train] batch 1547 loss 11.0151 avg_loss 12.4373\n",
            "[Train] batch 1548 loss 11.8898 avg_loss 12.4370\n",
            "[Train] batch 1549 loss 12.2501 avg_loss 12.4368\n",
            "[Train] batch 1550 loss 11.2895 avg_loss 12.4361\n",
            "[Train] batch 1551 loss 11.6614 avg_loss 12.4356\n",
            "[Train] batch 1552 loss 11.7489 avg_loss 12.4352\n",
            "[Train] batch 1553 loss 13.2312 avg_loss 12.4357\n",
            "[Train] batch 1554 loss 13.0670 avg_loss 12.4361\n",
            "[Train] batch 1555 loss 11.8908 avg_loss 12.4357\n",
            "[Train] batch 1556 loss 13.0379 avg_loss 12.4361\n",
            "[Train] batch 1557 loss 11.5205 avg_loss 12.4355\n",
            "[Train] batch 1558 loss 12.9229 avg_loss 12.4358\n",
            "[Train] batch 1559 loss 11.9006 avg_loss 12.4355\n",
            "[Train] batch 1560 loss 11.3111 avg_loss 12.4348\n",
            "[Train] batch 1561 loss 9.7921 avg_loss 12.4331\n",
            "[Train] batch 1562 loss 10.6442 avg_loss 12.4319\n",
            "[Train] batch 1563 loss 12.6852 avg_loss 12.4321\n",
            "[Train] batch 1564 loss 11.2196 avg_loss 12.4313\n",
            "[Train] batch 1565 loss 13.8633 avg_loss 12.4322\n",
            "[Train] batch 1566 loss 13.1036 avg_loss 12.4327\n",
            "[Train] batch 1567 loss 10.9554 avg_loss 12.4317\n",
            "[Train] batch 1568 loss 10.6494 avg_loss 12.4306\n",
            "[Train] batch 1569 loss 12.0221 avg_loss 12.4303\n",
            "[Train] batch 1570 loss 13.1575 avg_loss 12.4308\n",
            "[Train] batch 1571 loss 11.1708 avg_loss 12.4300\n",
            "[Train] batch 1572 loss 12.5901 avg_loss 12.4301\n",
            "[Train] batch 1573 loss 12.4977 avg_loss 12.4301\n",
            "[Train] batch 1574 loss 11.5296 avg_loss 12.4296\n",
            "[Train] batch 1575 loss 10.7597 avg_loss 12.4285\n",
            "[Train] batch 1576 loss 11.3986 avg_loss 12.4279\n",
            "[Train] batch 1577 loss 12.1123 avg_loss 12.4277\n",
            "[Train] batch 1578 loss 12.2983 avg_loss 12.4276\n",
            "[Train] batch 1579 loss 12.7985 avg_loss 12.4278\n",
            "[Train] batch 1580 loss 11.9044 avg_loss 12.4275\n",
            "[Train] batch 1581 loss 13.6375 avg_loss 12.4282\n",
            "[Train] batch 1582 loss 13.0036 avg_loss 12.4286\n",
            "[Train] batch 1583 loss 10.7639 avg_loss 12.4276\n",
            "[Train] batch 1584 loss 11.6876 avg_loss 12.4271\n",
            "[Train] batch 1585 loss 11.5859 avg_loss 12.4266\n",
            "[Train] batch 1586 loss 9.8087 avg_loss 12.4249\n",
            "[Train] batch 1587 loss 13.0635 avg_loss 12.4253\n",
            "[Train] batch 1588 loss 11.1933 avg_loss 12.4245\n",
            "[Train] batch 1589 loss 11.2861 avg_loss 12.4238\n",
            "[Train] batch 1590 loss 11.8578 avg_loss 12.4235\n",
            "[Train] batch 1591 loss 10.8310 avg_loss 12.4225\n",
            "[Train] batch 1592 loss 11.7656 avg_loss 12.4220\n",
            "[Train] batch 1593 loss 11.8556 avg_loss 12.4217\n",
            "[Train] batch 1594 loss 10.5850 avg_loss 12.4205\n",
            "[Train] batch 1595 loss 12.4170 avg_loss 12.4205\n",
            "[Train] batch 1596 loss 13.5149 avg_loss 12.4212\n",
            "[Train] batch 1597 loss 14.4120 avg_loss 12.4225\n",
            "[Train] batch 1598 loss 11.9879 avg_loss 12.4222\n",
            "[Train] batch 1599 loss 11.0746 avg_loss 12.4214\n",
            "[Train] batch 1600 loss 9.4777 avg_loss 12.4195\n",
            "[Train] batch 1601 loss 12.3634 avg_loss 12.4195\n",
            "[Train] batch 1602 loss 12.0711 avg_loss 12.4193\n",
            "[Train] batch 1603 loss 10.7950 avg_loss 12.4182\n",
            "[Train] batch 1604 loss 12.0490 avg_loss 12.4180\n",
            "[Train] batch 1605 loss 11.4800 avg_loss 12.4174\n",
            "[Train] batch 1606 loss 10.3104 avg_loss 12.4161\n",
            "[Train] batch 1607 loss 10.8121 avg_loss 12.4151\n",
            "[Train] batch 1608 loss 12.1805 avg_loss 12.4150\n",
            "[Train] batch 1609 loss 12.4013 avg_loss 12.4150\n",
            "[Train] batch 1610 loss 12.7252 avg_loss 12.4152\n",
            "[Train] batch 1611 loss 12.8817 avg_loss 12.4154\n",
            "[Train] batch 1612 loss 10.1802 avg_loss 12.4141\n",
            "[Train] batch 1613 loss 11.1018 avg_loss 12.4132\n",
            "[Train] batch 1614 loss 13.5087 avg_loss 12.4139\n",
            "[Train] batch 1615 loss 11.4891 avg_loss 12.4134\n",
            "[Train] batch 1616 loss 11.5950 avg_loss 12.4128\n",
            "[Train] batch 1617 loss 11.7599 avg_loss 12.4124\n",
            "[Train] batch 1618 loss 10.7403 avg_loss 12.4114\n",
            "[Train] batch 1619 loss 12.3303 avg_loss 12.4114\n",
            "[Train] batch 1620 loss 12.4792 avg_loss 12.4114\n",
            "[Train] batch 1621 loss 12.7177 avg_loss 12.4116\n",
            "[Train] batch 1622 loss 11.7409 avg_loss 12.4112\n",
            "[Train] batch 1623 loss 11.3958 avg_loss 12.4106\n",
            "[Train] batch 1624 loss 13.5222 avg_loss 12.4112\n",
            "[Train] batch 1625 loss 11.4194 avg_loss 12.4106\n",
            "[Train] batch 1626 loss 12.8644 avg_loss 12.4109\n",
            "[Train] batch 1627 loss 11.8066 avg_loss 12.4105\n",
            "[Train] batch 1628 loss 12.0890 avg_loss 12.4103\n",
            "[Train] batch 1629 loss 10.3227 avg_loss 12.4091\n",
            "[Train] batch 1630 loss 12.6373 avg_loss 12.4092\n",
            "[Train] batch 1631 loss 12.8869 avg_loss 12.4095\n",
            "[Train] batch 1632 loss 11.3916 avg_loss 12.4089\n",
            "[Train] batch 1633 loss 12.5946 avg_loss 12.4090\n",
            "[Train] batch 1634 loss 8.6357 avg_loss 12.4067\n",
            "[Train] batch 1635 loss 11.2846 avg_loss 12.4060\n",
            "[Train] batch 1636 loss 12.3085 avg_loss 12.4059\n",
            "[Train] batch 1637 loss 10.7821 avg_loss 12.4049\n",
            "[Train] batch 1638 loss 11.5914 avg_loss 12.4044\n",
            "[Train] batch 1639 loss 12.5298 avg_loss 12.4045\n",
            "[Train] batch 1640 loss 12.9634 avg_loss 12.4049\n",
            "[Train] batch 1641 loss 11.9732 avg_loss 12.4046\n",
            "[Train] batch 1642 loss 11.0431 avg_loss 12.4038\n",
            "[Train] batch 1643 loss 12.3072 avg_loss 12.4037\n",
            "[Train] batch 1644 loss 13.2998 avg_loss 12.4042\n",
            "[Train] batch 1645 loss 10.7213 avg_loss 12.4032\n",
            "[Train] batch 1646 loss 11.7757 avg_loss 12.4028\n",
            "[Train] batch 1647 loss 12.4394 avg_loss 12.4029\n",
            "[Train] batch 1648 loss 10.7414 avg_loss 12.4019\n",
            "[Train] batch 1649 loss 10.6565 avg_loss 12.4008\n",
            "[Train] batch 1650 loss 12.7740 avg_loss 12.4010\n",
            "[Train] batch 1651 loss 11.8608 avg_loss 12.4007\n",
            "[Train] batch 1652 loss 12.3194 avg_loss 12.4006\n",
            "[Train] batch 1653 loss 13.4366 avg_loss 12.4013\n",
            "[Train] batch 1654 loss 10.9205 avg_loss 12.4004\n",
            "[Train] batch 1655 loss 12.7092 avg_loss 12.4006\n",
            "[Train] batch 1656 loss 12.7681 avg_loss 12.4008\n",
            "[Train] batch 1657 loss 11.2614 avg_loss 12.4001\n",
            "[Train] batch 1658 loss 11.8962 avg_loss 12.3998\n",
            "[Train] batch 1659 loss 12.4373 avg_loss 12.3998\n",
            "[Train] batch 1660 loss 14.1270 avg_loss 12.4009\n",
            "[Train] batch 1661 loss 13.0428 avg_loss 12.4012\n",
            "[Train] batch 1662 loss 10.2726 avg_loss 12.4000\n",
            "[Train] batch 1663 loss 12.5002 avg_loss 12.4000\n",
            "[Train] batch 1664 loss 11.3416 avg_loss 12.3994\n",
            "[Train] batch 1665 loss 10.2317 avg_loss 12.3981\n",
            "[Train] batch 1666 loss 11.0524 avg_loss 12.3973\n",
            "[Train] batch 1667 loss 12.3250 avg_loss 12.3972\n",
            "[Train] batch 1668 loss 12.7837 avg_loss 12.3975\n",
            "[Train] batch 1669 loss 11.8431 avg_loss 12.3971\n",
            "[Train] batch 1670 loss 12.7717 avg_loss 12.3974\n",
            "[Train] batch 1671 loss 11.0423 avg_loss 12.3965\n",
            "[Train] batch 1672 loss 12.1602 avg_loss 12.3964\n",
            "[Train] batch 1673 loss 12.4410 avg_loss 12.3964\n",
            "[Train] batch 1674 loss 10.1094 avg_loss 12.3951\n",
            "[Train] batch 1675 loss 11.2896 avg_loss 12.3944\n",
            "[Train] batch 1676 loss 11.4648 avg_loss 12.3939\n",
            "[Train] batch 1677 loss 13.0510 avg_loss 12.3942\n",
            "[Train] batch 1678 loss 12.0318 avg_loss 12.3940\n",
            "[Train] batch 1679 loss 10.5258 avg_loss 12.3929\n",
            "[Train] batch 1680 loss 11.1425 avg_loss 12.3922\n",
            "[Train] batch 1681 loss 11.4937 avg_loss 12.3916\n",
            "[Train] batch 1682 loss 11.7280 avg_loss 12.3912\n",
            "[Train] batch 1683 loss 12.3968 avg_loss 12.3912\n",
            "[Train] batch 1684 loss 12.5793 avg_loss 12.3914\n",
            "[Train] batch 1685 loss 11.9924 avg_loss 12.3911\n",
            "[Train] batch 1686 loss 10.2263 avg_loss 12.3898\n",
            "[Train] batch 1687 loss 12.4095 avg_loss 12.3898\n",
            "[Train] batch 1688 loss 12.0102 avg_loss 12.3896\n",
            "[Train] batch 1689 loss 12.8220 avg_loss 12.3899\n",
            "[Train] batch 1690 loss 11.5661 avg_loss 12.3894\n",
            "[Train] batch 1691 loss 12.3107 avg_loss 12.3893\n",
            "[Train] batch 1692 loss 12.8647 avg_loss 12.3896\n",
            "[Train] batch 1693 loss 11.2227 avg_loss 12.3889\n",
            "[Train] batch 1694 loss 11.8011 avg_loss 12.3886\n",
            "[Train] batch 1695 loss 13.6607 avg_loss 12.3893\n",
            "[Train] batch 1696 loss 11.6590 avg_loss 12.3889\n",
            "[Train] batch 1697 loss 14.3779 avg_loss 12.3901\n",
            "[Train] batch 1698 loss 11.2560 avg_loss 12.3894\n",
            "[Train] batch 1699 loss 11.4723 avg_loss 12.3889\n",
            "[Train] batch 1700 loss 12.1766 avg_loss 12.3887\n",
            "[Train] batch 1701 loss 11.3918 avg_loss 12.3882\n",
            "[Train] batch 1702 loss 9.4362 avg_loss 12.3864\n",
            "[Train] batch 1703 loss 11.2483 avg_loss 12.3858\n",
            "[Train] batch 1704 loss 11.4472 avg_loss 12.3852\n",
            "[Train] batch 1705 loss 11.4334 avg_loss 12.3847\n",
            "[Train] batch 1706 loss 11.1896 avg_loss 12.3840\n",
            "[Train] batch 1707 loss 10.7137 avg_loss 12.3830\n",
            "[Train] batch 1708 loss 11.8550 avg_loss 12.3827\n",
            "[Train] batch 1709 loss 9.6385 avg_loss 12.3811\n",
            "[Train] batch 1710 loss 12.2941 avg_loss 12.3810\n",
            "[Train] batch 1711 loss 11.0187 avg_loss 12.3802\n",
            "[Train] batch 1712 loss 11.5584 avg_loss 12.3797\n",
            "[Train] batch 1713 loss 9.6131 avg_loss 12.3781\n",
            "[Train] batch 1714 loss 10.5322 avg_loss 12.3770\n",
            "[Train] batch 1715 loss 12.5409 avg_loss 12.3771\n",
            "[Train] batch 1716 loss 12.4131 avg_loss 12.3772\n",
            "[Train] batch 1717 loss 10.5153 avg_loss 12.3761\n",
            "[Train] batch 1718 loss 12.3726 avg_loss 12.3761\n",
            "[Train] batch 1719 loss 12.3140 avg_loss 12.3760\n",
            "[Train] batch 1720 loss 12.3289 avg_loss 12.3760\n",
            "[Train] batch 1721 loss 11.5978 avg_loss 12.3756\n",
            "[Train] batch 1722 loss 11.0975 avg_loss 12.3748\n",
            "[Train] batch 1723 loss 10.0719 avg_loss 12.3735\n",
            "[Train] batch 1724 loss 13.0290 avg_loss 12.3739\n",
            "[Train] batch 1725 loss 12.4446 avg_loss 12.3739\n",
            "[Train] batch 1726 loss 9.8402 avg_loss 12.3724\n",
            "[Train] batch 1727 loss 11.6631 avg_loss 12.3720\n",
            "[Train] batch 1728 loss 11.2865 avg_loss 12.3714\n",
            "[Train] batch 1729 loss 11.4476 avg_loss 12.3709\n",
            "[Train] batch 1730 loss 11.8564 avg_loss 12.3706\n",
            "[Train] batch 1731 loss 12.4849 avg_loss 12.3706\n",
            "[Train] batch 1732 loss 10.6436 avg_loss 12.3696\n",
            "[Train] batch 1733 loss 11.6116 avg_loss 12.3692\n",
            "[Train] batch 1734 loss 13.1088 avg_loss 12.3696\n",
            "[Train] batch 1735 loss 11.9566 avg_loss 12.3694\n",
            "[Train] batch 1736 loss 10.3625 avg_loss 12.3682\n",
            "[Train] batch 1737 loss 11.2889 avg_loss 12.3676\n",
            "[Train] batch 1738 loss 12.3203 avg_loss 12.3676\n",
            "[Train] batch 1739 loss 10.5107 avg_loss 12.3665\n",
            "[Train] batch 1740 loss 12.1939 avg_loss 12.3664\n",
            "[Train] batch 1741 loss 10.8298 avg_loss 12.3655\n",
            "[Train] batch 1742 loss 10.4290 avg_loss 12.3644\n",
            "[Train] batch 1743 loss 12.5673 avg_loss 12.3645\n",
            "[Train] batch 1744 loss 11.1589 avg_loss 12.3638\n",
            "[Train] batch 1745 loss 10.6878 avg_loss 12.3629\n",
            "[Train] batch 1746 loss 11.2590 avg_loss 12.3622\n",
            "[Train] batch 1747 loss 11.1590 avg_loss 12.3616\n",
            "[Train] batch 1748 loss 11.4923 avg_loss 12.3611\n",
            "[Train] batch 1749 loss 13.0580 avg_loss 12.3615\n",
            "[Train] batch 1750 loss 10.7835 avg_loss 12.3606\n",
            "[Train] batch 1751 loss 10.8786 avg_loss 12.3597\n",
            "[Train] batch 1752 loss 10.9781 avg_loss 12.3589\n",
            "[Train] batch 1753 loss 12.6979 avg_loss 12.3591\n",
            "[Train] batch 1754 loss 15.2160 avg_loss 12.3607\n",
            "[Train] batch 1755 loss 12.7354 avg_loss 12.3610\n",
            "[Train] batch 1756 loss 12.5522 avg_loss 12.3611\n",
            "[Train] batch 1757 loss 11.0867 avg_loss 12.3603\n",
            "[Train] batch 1758 loss 11.5968 avg_loss 12.3599\n",
            "[Train] batch 1759 loss 11.6898 avg_loss 12.3595\n",
            "[Train] batch 1760 loss 11.4313 avg_loss 12.3590\n",
            "[Train] batch 1761 loss 10.6277 avg_loss 12.3580\n",
            "[Train] batch 1762 loss 10.5045 avg_loss 12.3570\n",
            "[Train] batch 1763 loss 13.7620 avg_loss 12.3578\n",
            "[Train] batch 1764 loss 12.5744 avg_loss 12.3579\n",
            "[Train] batch 1765 loss 12.4677 avg_loss 12.3579\n",
            "[Train] batch 1766 loss 11.0223 avg_loss 12.3572\n",
            "[Train] batch 1767 loss 10.1955 avg_loss 12.3560\n",
            "[Train] batch 1768 loss 11.0126 avg_loss 12.3552\n",
            "[Train] batch 1769 loss 11.9345 avg_loss 12.3550\n",
            "[Train] batch 1770 loss 10.0656 avg_loss 12.3537\n",
            "[Train] batch 1771 loss 9.6628 avg_loss 12.3522\n",
            "[Train] batch 1772 loss 11.7318 avg_loss 12.3518\n",
            "[Train] batch 1773 loss 11.9661 avg_loss 12.3516\n",
            "[Train] batch 1774 loss 11.8957 avg_loss 12.3513\n",
            "[Train] batch 1775 loss 12.3747 avg_loss 12.3513\n",
            "[Train] batch 1776 loss 10.6242 avg_loss 12.3504\n",
            "[Train] batch 1777 loss 12.0713 avg_loss 12.3502\n",
            "[Train] batch 1778 loss 12.1086 avg_loss 12.3501\n",
            "[Train] batch 1779 loss 10.4901 avg_loss 12.3490\n",
            "[Train] batch 1780 loss 11.5736 avg_loss 12.3486\n",
            "[Train] batch 1781 loss 10.1825 avg_loss 12.3474\n",
            "[Train] batch 1782 loss 11.5631 avg_loss 12.3469\n",
            "[Train] batch 1783 loss 12.4665 avg_loss 12.3470\n",
            "[Train] batch 1784 loss 12.5321 avg_loss 12.3471\n",
            "[Train] batch 1785 loss 11.4058 avg_loss 12.3466\n",
            "[Train] batch 1786 loss 9.0374 avg_loss 12.3447\n",
            "[Train] batch 1787 loss 13.2585 avg_loss 12.3452\n",
            "[Train] batch 1788 loss 11.8120 avg_loss 12.3449\n",
            "[Train] batch 1789 loss 12.0286 avg_loss 12.3448\n",
            "[Train] batch 1790 loss 11.5494 avg_loss 12.3443\n",
            "[Train] batch 1791 loss 11.6807 avg_loss 12.3440\n",
            "[Train] batch 1792 loss 12.9980 avg_loss 12.3443\n",
            "[Train] batch 1793 loss 9.4773 avg_loss 12.3427\n",
            "[Train] batch 1794 loss 12.3693 avg_loss 12.3427\n",
            "[Train] batch 1795 loss 12.0922 avg_loss 12.3426\n",
            "[Train] batch 1796 loss 11.1897 avg_loss 12.3419\n",
            "[Train] batch 1797 loss 11.4757 avg_loss 12.3415\n",
            "[Train] batch 1798 loss 11.7738 avg_loss 12.3412\n",
            "[Train] batch 1799 loss 10.3669 avg_loss 12.3401\n",
            "[Train] batch 1800 loss 11.0737 avg_loss 12.3394\n",
            "[Train] batch 1801 loss 12.3979 avg_loss 12.3394\n",
            "[Train] batch 1802 loss 10.3938 avg_loss 12.3383\n",
            "[Train] batch 1803 loss 13.4438 avg_loss 12.3389\n",
            "[Train] batch 1804 loss 13.2154 avg_loss 12.3394\n",
            "[Train] batch 1805 loss 13.0666 avg_loss 12.3398\n",
            "[Train] batch 1806 loss 11.7243 avg_loss 12.3395\n",
            "[Train] batch 1807 loss 10.8808 avg_loss 12.3387\n",
            "[Train] batch 1808 loss 11.5053 avg_loss 12.3382\n",
            "[Train] batch 1809 loss 11.4234 avg_loss 12.3377\n",
            "[Train] batch 1810 loss 12.1361 avg_loss 12.3376\n",
            "[Train] batch 1811 loss 13.3565 avg_loss 12.3381\n",
            "[Train] batch 1812 loss 13.0843 avg_loss 12.3386\n",
            "[Train] batch 1813 loss 9.4401 avg_loss 12.3370\n",
            "[Train] batch 1814 loss 11.8551 avg_loss 12.3367\n",
            "[Train] batch 1815 loss 10.4700 avg_loss 12.3357\n",
            "[Train] batch 1816 loss 11.8166 avg_loss 12.3354\n",
            "[Train] batch 1817 loss 9.7551 avg_loss 12.3340\n",
            "[Train] batch 1818 loss 13.9669 avg_loss 12.3349\n",
            "[Train] batch 1819 loss 10.8878 avg_loss 12.3341\n",
            "[Train] batch 1820 loss 12.4280 avg_loss 12.3341\n",
            "[Train] batch 1821 loss 10.6656 avg_loss 12.3332\n",
            "[Train] batch 1822 loss 11.9047 avg_loss 12.3330\n",
            "[Train] batch 1823 loss 12.0692 avg_loss 12.3328\n",
            "[Train] batch 1824 loss 11.8844 avg_loss 12.3326\n",
            "[Train] batch 1825 loss 12.6649 avg_loss 12.3327\n",
            "[Train] batch 1826 loss 12.1162 avg_loss 12.3326\n",
            "[Train] batch 1827 loss 11.9217 avg_loss 12.3324\n",
            "[Train] batch 1828 loss 11.9540 avg_loss 12.3322\n",
            "[Train] batch 1829 loss 10.8669 avg_loss 12.3314\n",
            "[Train] batch 1830 loss 11.4894 avg_loss 12.3309\n",
            "[Train] batch 1831 loss 11.7790 avg_loss 12.3306\n",
            "[Train] batch 1832 loss 10.6450 avg_loss 12.3297\n",
            "[Train] batch 1833 loss 11.3199 avg_loss 12.3292\n",
            "[Train] batch 1834 loss 11.0449 avg_loss 12.3285\n",
            "[Train] batch 1835 loss 12.2756 avg_loss 12.3284\n",
            "[Train] batch 1836 loss 11.1822 avg_loss 12.3278\n",
            "[Train] batch 1837 loss 13.4203 avg_loss 12.3284\n",
            "[Train] batch 1838 loss 11.1965 avg_loss 12.3278\n",
            "[Train] batch 1839 loss 13.4526 avg_loss 12.3284\n",
            "[Train] batch 1840 loss 12.0026 avg_loss 12.3282\n",
            "[Train] batch 1841 loss 12.6041 avg_loss 12.3284\n",
            "[Train] batch 1842 loss 9.9901 avg_loss 12.3271\n",
            "[Train] batch 1843 loss 11.6944 avg_loss 12.3268\n",
            "[Train] batch 1844 loss 12.8353 avg_loss 12.3270\n",
            "[Train] batch 1845 loss 11.4951 avg_loss 12.3266\n",
            "[Train] batch 1846 loss 9.2693 avg_loss 12.3249\n",
            "[Train] batch 1847 loss 10.5978 avg_loss 12.3240\n",
            "[Train] batch 1848 loss 10.7441 avg_loss 12.3231\n",
            "[Train] batch 1849 loss 10.6828 avg_loss 12.3223\n",
            "[Train] batch 1850 loss 9.9287 avg_loss 12.3210\n",
            "[Train] batch 1851 loss 12.1547 avg_loss 12.3209\n",
            "[Train] batch 1852 loss 12.9602 avg_loss 12.3212\n",
            "[Train] batch 1853 loss 9.9643 avg_loss 12.3199\n",
            "[Train] batch 1854 loss 11.8322 avg_loss 12.3197\n",
            "[Train] batch 1855 loss 12.5774 avg_loss 12.3198\n",
            "[Train] batch 1856 loss 11.6577 avg_loss 12.3195\n",
            "[Train] batch 1857 loss 11.8388 avg_loss 12.3192\n",
            "[Train] batch 1858 loss 11.4861 avg_loss 12.3188\n",
            "[Train] batch 1859 loss 11.4538 avg_loss 12.3183\n",
            "[Train] batch 1860 loss 11.7674 avg_loss 12.3180\n",
            "[Train] batch 1861 loss 11.5597 avg_loss 12.3176\n",
            "[Train] batch 1862 loss 11.7069 avg_loss 12.3173\n",
            "[Train] batch 1863 loss 11.3227 avg_loss 12.3167\n",
            "[Train] batch 1864 loss 9.3081 avg_loss 12.3151\n",
            "[Train] batch 1865 loss 9.2883 avg_loss 12.3135\n",
            "[Train] batch 1866 loss 12.4957 avg_loss 12.3136\n",
            "[Train] batch 1867 loss 11.6826 avg_loss 12.3132\n",
            "[Train] batch 1868 loss 11.8194 avg_loss 12.3130\n",
            "[Train] batch 1869 loss 13.7703 avg_loss 12.3138\n",
            "[Train] batch 1870 loss 10.5076 avg_loss 12.3128\n",
            "[Train] batch 1871 loss 11.1732 avg_loss 12.3122\n",
            "[Train] batch 1872 loss 11.4482 avg_loss 12.3117\n",
            "[Train] batch 1873 loss 12.5444 avg_loss 12.3119\n",
            "[Train] batch 1874 loss 11.8213 avg_loss 12.3116\n",
            "[Train] batch 1875 loss 10.4919 avg_loss 12.3106\n",
            "[Train] batch 1876 loss 12.2955 avg_loss 12.3106\n",
            "[Train] batch 1877 loss 11.8557 avg_loss 12.3104\n",
            "[Train] batch 1878 loss 12.0388 avg_loss 12.3102\n",
            "[Train] batch 1879 loss 11.0747 avg_loss 12.3096\n",
            "[Train] batch 1880 loss 11.8245 avg_loss 12.3093\n",
            "[Train] batch 1881 loss 10.7962 avg_loss 12.3085\n",
            "[Train] batch 1882 loss 11.0907 avg_loss 12.3079\n",
            "[Train] batch 1883 loss 11.4435 avg_loss 12.3074\n",
            "[Train] batch 1884 loss 13.3274 avg_loss 12.3079\n",
            "[Train] batch 1885 loss 12.3773 avg_loss 12.3080\n",
            "[Train] batch 1886 loss 14.0245 avg_loss 12.3089\n",
            "[Train] batch 1887 loss 11.0133 avg_loss 12.3082\n",
            "[Train] batch 1888 loss 10.8815 avg_loss 12.3074\n",
            "[Train] batch 1889 loss 9.8569 avg_loss 12.3061\n",
            "[Train] batch 1890 loss 12.8036 avg_loss 12.3064\n",
            "[Train] batch 1891 loss 11.0685 avg_loss 12.3058\n",
            "[Train] batch 1892 loss 12.0950 avg_loss 12.3056\n",
            "[Train] batch 1893 loss 12.2901 avg_loss 12.3056\n",
            "[Train] batch 1894 loss 12.5636 avg_loss 12.3058\n",
            "[Train] batch 1895 loss 12.9382 avg_loss 12.3061\n",
            "[Train] batch 1896 loss 11.4904 avg_loss 12.3057\n",
            "[Train] batch 1897 loss 10.0043 avg_loss 12.3045\n",
            "[Train] batch 1898 loss 9.1115 avg_loss 12.3028\n",
            "[Train] batch 1899 loss 14.9894 avg_loss 12.3042\n",
            "[Train] batch 1900 loss 13.1976 avg_loss 12.3047\n",
            "[Train] batch 1901 loss 12.0181 avg_loss 12.3045\n",
            "[Train] batch 1902 loss 11.4561 avg_loss 12.3041\n",
            "[Train] batch 1903 loss 13.6400 avg_loss 12.3048\n",
            "[Train] batch 1904 loss 10.9152 avg_loss 12.3040\n",
            "[Train] batch 1905 loss 11.2205 avg_loss 12.3035\n",
            "[Train] batch 1906 loss 12.0223 avg_loss 12.3033\n",
            "[Train] batch 1907 loss 10.9126 avg_loss 12.3026\n",
            "[Train] batch 1908 loss 11.5016 avg_loss 12.3022\n",
            "[Train] batch 1909 loss 10.5247 avg_loss 12.3012\n",
            "[Train] batch 1910 loss 11.6784 avg_loss 12.3009\n",
            "[Train] batch 1911 loss 11.1704 avg_loss 12.3003\n",
            "[Train] batch 1912 loss 9.7723 avg_loss 12.2990\n",
            "[Train] batch 1913 loss 12.3829 avg_loss 12.2990\n",
            "[Train] batch 1914 loss 11.7394 avg_loss 12.2988\n",
            "[Train] batch 1915 loss 11.8877 avg_loss 12.2985\n",
            "[Train] batch 1916 loss 11.7676 avg_loss 12.2983\n",
            "[Train] batch 1917 loss 13.5605 avg_loss 12.2989\n",
            "[Train] batch 1918 loss 11.0841 avg_loss 12.2983\n",
            "[Train] batch 1919 loss 11.3104 avg_loss 12.2978\n",
            "[Train] batch 1920 loss 9.4228 avg_loss 12.2963\n",
            "[Train] batch 1921 loss 11.1963 avg_loss 12.2957\n",
            "[Train] batch 1922 loss 13.3126 avg_loss 12.2962\n",
            "[Train] batch 1923 loss 13.3091 avg_loss 12.2968\n",
            "[Train] batch 1924 loss 12.7964 avg_loss 12.2970\n",
            "[Train] batch 1925 loss 10.6247 avg_loss 12.2962\n",
            "[Train] batch 1926 loss 11.3183 avg_loss 12.2956\n",
            "[Train] batch 1927 loss 11.7482 avg_loss 12.2954\n",
            "[Train] batch 1928 loss 8.8793 avg_loss 12.2936\n",
            "[Train] batch 1929 loss 11.2778 avg_loss 12.2931\n",
            "[Train] batch 1930 loss 12.0388 avg_loss 12.2929\n",
            "[Train] batch 1931 loss 10.5063 avg_loss 12.2920\n",
            "[Train] batch 1932 loss 10.5566 avg_loss 12.2911\n",
            "[Train] batch 1933 loss 11.6826 avg_loss 12.2908\n",
            "[Train] batch 1934 loss 12.8128 avg_loss 12.2911\n",
            "[Train] batch 1935 loss 12.0184 avg_loss 12.2909\n",
            "[Train] batch 1936 loss 10.2121 avg_loss 12.2898\n",
            "[Train] batch 1937 loss 11.7579 avg_loss 12.2896\n",
            "[Train] batch 1938 loss 13.0819 avg_loss 12.2900\n",
            "[Train] batch 1939 loss 10.8169 avg_loss 12.2892\n",
            "[Train] batch 1940 loss 11.4488 avg_loss 12.2888\n",
            "[Train] batch 1941 loss 13.0226 avg_loss 12.2892\n",
            "[Train] batch 1942 loss 13.9602 avg_loss 12.2900\n",
            "[Train] batch 1943 loss 11.1079 avg_loss 12.2894\n",
            "[Train] batch 1944 loss 12.2689 avg_loss 12.2894\n",
            "[Train] batch 1945 loss 11.5999 avg_loss 12.2891\n",
            "[Train] batch 1946 loss 11.3262 avg_loss 12.2886\n",
            "[Train] batch 1947 loss 11.6489 avg_loss 12.2882\n",
            "[Train] batch 1948 loss 12.9284 avg_loss 12.2886\n",
            "[Train] batch 1949 loss 12.1871 avg_loss 12.2885\n",
            "[Train] batch 1950 loss 10.6784 avg_loss 12.2877\n",
            "[Train] batch 1951 loss 13.7688 avg_loss 12.2884\n",
            "[Train] batch 1952 loss 11.8130 avg_loss 12.2882\n",
            "[Train] batch 1953 loss 10.6910 avg_loss 12.2874\n",
            "[Train] batch 1954 loss 11.0447 avg_loss 12.2867\n",
            "[Train] batch 1955 loss 10.3346 avg_loss 12.2857\n",
            "[Train] batch 1956 loss 10.4960 avg_loss 12.2848\n",
            "[Train] batch 1957 loss 14.5165 avg_loss 12.2860\n",
            "[Train] batch 1958 loss 12.6520 avg_loss 12.2862\n",
            "[Train] batch 1959 loss 12.3186 avg_loss 12.2862\n",
            "[Train] batch 1960 loss 10.3576 avg_loss 12.2852\n",
            "[Train] batch 1961 loss 12.0535 avg_loss 12.2851\n",
            "[Train] batch 1962 loss 12.2655 avg_loss 12.2851\n",
            "[Train] batch 1963 loss 11.7261 avg_loss 12.2848\n",
            "[Train] batch 1964 loss 10.5483 avg_loss 12.2839\n",
            "[Train] batch 1965 loss 13.0078 avg_loss 12.2843\n",
            "[Train] batch 1966 loss 10.0621 avg_loss 12.2831\n",
            "[Train] batch 1967 loss 12.3745 avg_loss 12.2832\n",
            "[Train] batch 1968 loss 11.8591 avg_loss 12.2830\n",
            "[Train] batch 1969 loss 11.5704 avg_loss 12.2826\n",
            "[Train] batch 1970 loss 11.1592 avg_loss 12.2820\n",
            "[Train] batch 1971 loss 11.2019 avg_loss 12.2815\n",
            "[Train] batch 1972 loss 9.6059 avg_loss 12.2801\n",
            "[Train] batch 1973 loss 11.4484 avg_loss 12.2797\n",
            "[Train] batch 1974 loss 10.7206 avg_loss 12.2789\n",
            "[Train] batch 1975 loss 14.8257 avg_loss 12.2802\n",
            "[Train] batch 1976 loss 11.3116 avg_loss 12.2797\n",
            "[Train] batch 1977 loss 11.2171 avg_loss 12.2792\n",
            "[Train] batch 1978 loss 11.2462 avg_loss 12.2787\n",
            "[Train] batch 1979 loss 12.4609 avg_loss 12.2787\n",
            "[Train] batch 1980 loss 10.0981 avg_loss 12.2776\n",
            "[Train] batch 1981 loss 13.9948 avg_loss 12.2785\n",
            "[Train] batch 1982 loss 11.8531 avg_loss 12.2783\n",
            "[Train] batch 1983 loss 11.8410 avg_loss 12.2781\n",
            "[Train] batch 1984 loss 11.6900 avg_loss 12.2778\n",
            "[Train] batch 1985 loss 10.8448 avg_loss 12.2771\n",
            "[Train] batch 1986 loss 10.4351 avg_loss 12.2761\n",
            "[Train] batch 1987 loss 12.8508 avg_loss 12.2764\n",
            "[Train] batch 1988 loss 12.5326 avg_loss 12.2765\n",
            "[Train] batch 1989 loss 8.9835 avg_loss 12.2749\n",
            "[Train] batch 1990 loss 13.2295 avg_loss 12.2754\n",
            "[Train] batch 1991 loss 13.4492 avg_loss 12.2760\n",
            "[Train] batch 1992 loss 11.8410 avg_loss 12.2757\n",
            "[Train] batch 1993 loss 10.5883 avg_loss 12.2749\n",
            "[Train] batch 1994 loss 12.6973 avg_loss 12.2751\n",
            "[Train] batch 1995 loss 11.2396 avg_loss 12.2746\n",
            "[Train] batch 1996 loss 11.4170 avg_loss 12.2742\n",
            "[Train] batch 1997 loss 13.7799 avg_loss 12.2749\n",
            "[Train] batch 1998 loss 12.3344 avg_loss 12.2749\n",
            "[Train] batch 1999 loss 11.7048 avg_loss 12.2747\n",
            "[Train] batch 2000 loss 13.4522 avg_loss 12.2752\n",
            "[Train] batch 2001 loss 12.0154 avg_loss 12.2751\n",
            "[Train] batch 2002 loss 10.2536 avg_loss 12.2741\n",
            "[Train] batch 2003 loss 12.3630 avg_loss 12.2741\n",
            "[Train] batch 2004 loss 11.5612 avg_loss 12.2738\n",
            "[Train] batch 2005 loss 10.0196 avg_loss 12.2727\n",
            "[Train] batch 2006 loss 11.2850 avg_loss 12.2722\n",
            "[Train] batch 2007 loss 11.9115 avg_loss 12.2720\n",
            "[Train] batch 2008 loss 11.2754 avg_loss 12.2715\n",
            "[Train] batch 2009 loss 11.5531 avg_loss 12.2711\n",
            "[Train] batch 2010 loss 13.0353 avg_loss 12.2715\n",
            "[Train] batch 2011 loss 10.9697 avg_loss 12.2709\n",
            "[Train] batch 2012 loss 11.4883 avg_loss 12.2705\n",
            "[Train] batch 2013 loss 11.4298 avg_loss 12.2701\n",
            "[Train] batch 2014 loss 12.6224 avg_loss 12.2702\n",
            "[Train] batch 2015 loss 12.1395 avg_loss 12.2702\n",
            "[Train] batch 2016 loss 11.8467 avg_loss 12.2700\n",
            "[Train] batch 2017 loss 9.4560 avg_loss 12.2686\n",
            "[Train] batch 2018 loss 13.0255 avg_loss 12.2689\n",
            "[Train] batch 2019 loss 10.7609 avg_loss 12.2682\n",
            "[Train] batch 2020 loss 10.9598 avg_loss 12.2676\n",
            "[Train] batch 2021 loss 12.6616 avg_loss 12.2677\n",
            "[Train] batch 2022 loss 13.2647 avg_loss 12.2682\n",
            "[Train] batch 2023 loss 11.5559 avg_loss 12.2679\n",
            "[Train] batch 2024 loss 10.7493 avg_loss 12.2671\n",
            "[Train] batch 2025 loss 11.3560 avg_loss 12.2667\n",
            "[Train] batch 2026 loss 11.5432 avg_loss 12.2663\n",
            "[Train] batch 2027 loss 11.1836 avg_loss 12.2658\n",
            "[Train] batch 2028 loss 12.4440 avg_loss 12.2659\n",
            "[Train] batch 2029 loss 10.7005 avg_loss 12.2651\n",
            "[Train] batch 2030 loss 10.8309 avg_loss 12.2644\n",
            "[Train] batch 2031 loss 11.0252 avg_loss 12.2638\n",
            "[Train] batch 2032 loss 10.8309 avg_loss 12.2631\n",
            "[Train] batch 2033 loss 11.5390 avg_loss 12.2627\n",
            "[Train] batch 2034 loss 11.8938 avg_loss 12.2626\n",
            "[Train] batch 2035 loss 11.0666 avg_loss 12.2620\n",
            "[Train] batch 2036 loss 12.6070 avg_loss 12.2621\n",
            "[Train] batch 2037 loss 10.9911 avg_loss 12.2615\n",
            "[Train] batch 2038 loss 13.1091 avg_loss 12.2619\n",
            "[Train] batch 2039 loss 11.1178 avg_loss 12.2614\n",
            "[Train] batch 2040 loss 11.4701 avg_loss 12.2610\n",
            "[Train] batch 2041 loss 11.3599 avg_loss 12.2605\n",
            "[Train] batch 2042 loss 12.0482 avg_loss 12.2604\n",
            "[Train] batch 2043 loss 11.5420 avg_loss 12.2601\n",
            "[Train] batch 2044 loss 12.2496 avg_loss 12.2601\n",
            "[Train] batch 2045 loss 11.1805 avg_loss 12.2596\n",
            "[Train] batch 2046 loss 12.6500 avg_loss 12.2597\n",
            "[Train] batch 2047 loss 12.5070 avg_loss 12.2599\n",
            "[Train] batch 2048 loss 10.4117 avg_loss 12.2590\n",
            "[Train] batch 2049 loss 11.6740 avg_loss 12.2587\n",
            "[Train] batch 2050 loss 9.4371 avg_loss 12.2573\n",
            "[Train] batch 2051 loss 11.2473 avg_loss 12.2568\n",
            "[Train] batch 2052 loss 11.8130 avg_loss 12.2566\n",
            "[Train] batch 2053 loss 11.2622 avg_loss 12.2561\n",
            "[Train] batch 2054 loss 12.9865 avg_loss 12.2565\n",
            "[Train] batch 2055 loss 12.6421 avg_loss 12.2566\n",
            "[Train] batch 2056 loss 12.9737 avg_loss 12.2570\n",
            "[Train] batch 2057 loss 12.4901 avg_loss 12.2571\n",
            "[Train] batch 2058 loss 11.9183 avg_loss 12.2569\n",
            "[Train] batch 2059 loss 12.1065 avg_loss 12.2569\n",
            "[Train] batch 2060 loss 10.7585 avg_loss 12.2561\n",
            "[Train] batch 2061 loss 9.1650 avg_loss 12.2546\n",
            "[Train] batch 2062 loss 11.5116 avg_loss 12.2543\n",
            "[Train] batch 2063 loss 10.9839 avg_loss 12.2537\n",
            "[Train] batch 2064 loss 11.2777 avg_loss 12.2532\n",
            "[Train] batch 2065 loss 12.9515 avg_loss 12.2535\n",
            "[Train] batch 2066 loss 14.0834 avg_loss 12.2544\n",
            "[Train] batch 2067 loss 11.6705 avg_loss 12.2541\n",
            "[Train] batch 2068 loss 11.7198 avg_loss 12.2539\n",
            "[Train] batch 2069 loss 13.6655 avg_loss 12.2546\n",
            "[Train] batch 2070 loss 10.2633 avg_loss 12.2536\n",
            "[Train] batch 2071 loss 12.2708 avg_loss 12.2536\n",
            "[Train] batch 2072 loss 10.9547 avg_loss 12.2530\n",
            "[Train] batch 2073 loss 10.8196 avg_loss 12.2523\n",
            "[Train] batch 2074 loss 11.6796 avg_loss 12.2520\n",
            "[Train] batch 2075 loss 12.5934 avg_loss 12.2522\n",
            "[Train] batch 2076 loss 12.4719 avg_loss 12.2523\n",
            "[Train] batch 2077 loss 11.1107 avg_loss 12.2517\n",
            "[Train] batch 2078 loss 10.8078 avg_loss 12.2510\n",
            "[Train] batch 2079 loss 11.2029 avg_loss 12.2505\n",
            "[Train] batch 2080 loss 12.1402 avg_loss 12.2505\n",
            "[Train] batch 2081 loss 13.2903 avg_loss 12.2510\n",
            "[Train] batch 2082 loss 12.9436 avg_loss 12.2513\n",
            "[Train] batch 2083 loss 12.1466 avg_loss 12.2513\n",
            "[Train] batch 2084 loss 11.6450 avg_loss 12.2510\n",
            "[Train] batch 2085 loss 12.7245 avg_loss 12.2512\n",
            "[Train] batch 2086 loss 10.9880 avg_loss 12.2506\n",
            "[Train] batch 2087 loss 11.3959 avg_loss 12.2502\n",
            "[Train] batch 2088 loss 11.0183 avg_loss 12.2496\n",
            "[Train] batch 2089 loss 11.1968 avg_loss 12.2491\n",
            "[Train] batch 2090 loss 11.4848 avg_loss 12.2487\n",
            "[Train] batch 2091 loss 11.4023 avg_loss 12.2483\n",
            "[Train] batch 2092 loss 11.5493 avg_loss 12.2480\n",
            "[Train] batch 2093 loss 11.0551 avg_loss 12.2474\n",
            "[Train] batch 2094 loss 12.0728 avg_loss 12.2473\n",
            "[Train] batch 2095 loss 11.3867 avg_loss 12.2469\n",
            "[Train] batch 2096 loss 12.0616 avg_loss 12.2468\n",
            "[Train] batch 2097 loss 13.9643 avg_loss 12.2477\n",
            "[Train] batch 2098 loss 9.2885 avg_loss 12.2462\n",
            "[Train] batch 2099 loss 10.7975 avg_loss 12.2456\n",
            "[Train] batch 2100 loss 11.1133 avg_loss 12.2450\n",
            "[Train] batch 2101 loss 10.9139 avg_loss 12.2444\n",
            "[Train] batch 2102 loss 11.2345 avg_loss 12.2439\n",
            "[Train] batch 2103 loss 11.7424 avg_loss 12.2437\n",
            "[Train] batch 2104 loss 12.7919 avg_loss 12.2439\n",
            "[Train] batch 2105 loss 13.5697 avg_loss 12.2446\n",
            "[Train] batch 2106 loss 10.3838 avg_loss 12.2437\n",
            "[Train] batch 2107 loss 11.3054 avg_loss 12.2432\n",
            "[Train] batch 2108 loss 12.2454 avg_loss 12.2432\n",
            "[Train] batch 2109 loss 11.2101 avg_loss 12.2427\n",
            "[Train] batch 2110 loss 12.3848 avg_loss 12.2428\n",
            "[Train] batch 2111 loss 13.0267 avg_loss 12.2432\n",
            "[Train] batch 2112 loss 10.8064 avg_loss 12.2425\n",
            "[Train] batch 2113 loss 10.6340 avg_loss 12.2417\n",
            "[Train] batch 2114 loss 11.3615 avg_loss 12.2413\n",
            "[Train] batch 2115 loss 12.3464 avg_loss 12.2414\n",
            "[Train] batch 2116 loss 13.0822 avg_loss 12.2418\n",
            "[Train] batch 2117 loss 12.8628 avg_loss 12.2421\n",
            "[Train] batch 2118 loss 12.7650 avg_loss 12.2423\n",
            "[Train] batch 2119 loss 11.5461 avg_loss 12.2420\n",
            "[Train] batch 2120 loss 14.4228 avg_loss 12.2430\n",
            "[Train] batch 2121 loss 10.4798 avg_loss 12.2422\n",
            "[Train] batch 2122 loss 11.9514 avg_loss 12.2420\n",
            "[Train] batch 2123 loss 11.8263 avg_loss 12.2418\n",
            "[Train] batch 2124 loss 9.9022 avg_loss 12.2407\n",
            "[Train] batch 2125 loss 11.8024 avg_loss 12.2405\n",
            "[Train] batch 2126 loss 10.1408 avg_loss 12.2395\n",
            "[Train] batch 2127 loss 11.9953 avg_loss 12.2394\n",
            "[Train] batch 2128 loss 12.0922 avg_loss 12.2394\n",
            "[Train] batch 2129 loss 12.8292 avg_loss 12.2396\n",
            "[Train] batch 2130 loss 10.2925 avg_loss 12.2387\n",
            "[Train] batch 2131 loss 10.4870 avg_loss 12.2379\n",
            "[Train] batch 2132 loss 10.4787 avg_loss 12.2371\n",
            "[Train] batch 2133 loss 10.7888 avg_loss 12.2364\n",
            "[Train] batch 2134 loss 12.1064 avg_loss 12.2363\n",
            "[Train] batch 2135 loss 12.5509 avg_loss 12.2365\n",
            "[Train] batch 2136 loss 13.0290 avg_loss 12.2369\n",
            "[Train] batch 2137 loss 10.8686 avg_loss 12.2362\n",
            "[Train] batch 2138 loss 10.4731 avg_loss 12.2354\n",
            "[Train] batch 2139 loss 13.2711 avg_loss 12.2359\n",
            "[Train] batch 2140 loss 10.2353 avg_loss 12.2349\n",
            "[Train] batch 2141 loss 11.4467 avg_loss 12.2346\n",
            "[Train] batch 2142 loss 11.5719 avg_loss 12.2343\n",
            "[Train] batch 2143 loss 11.2391 avg_loss 12.2338\n",
            "[Train] batch 2144 loss 11.4614 avg_loss 12.2334\n",
            "[Train] batch 2145 loss 10.3339 avg_loss 12.2325\n",
            "[Train] batch 2146 loss 11.5197 avg_loss 12.2322\n",
            "[Train] batch 2147 loss 11.5471 avg_loss 12.2319\n",
            "[Train] batch 2148 loss 11.9480 avg_loss 12.2318\n",
            "[Train] batch 2149 loss 9.5192 avg_loss 12.2305\n",
            "[Train] batch 2150 loss 10.3280 avg_loss 12.2296\n",
            "[Train] batch 2151 loss 9.2667 avg_loss 12.2282\n",
            "[Train] batch 2152 loss 12.3820 avg_loss 12.2283\n",
            "[Train] batch 2153 loss 10.2453 avg_loss 12.2274\n",
            "[Train] batch 2154 loss 11.9335 avg_loss 12.2273\n",
            "[Train] batch 2155 loss 11.1626 avg_loss 12.2268\n",
            "[Train] batch 2156 loss 9.0755 avg_loss 12.2253\n",
            "[Train] batch 2157 loss 10.1485 avg_loss 12.2243\n",
            "[Train] batch 2158 loss 11.2107 avg_loss 12.2239\n",
            "[Train] batch 2159 loss 14.0949 avg_loss 12.2247\n",
            "[Train] batch 2160 loss 13.6371 avg_loss 12.2254\n",
            "[Train] batch 2161 loss 11.4179 avg_loss 12.2250\n",
            "[Train] batch 2162 loss 11.7495 avg_loss 12.2248\n",
            "[Train] batch 2163 loss 13.5941 avg_loss 12.2254\n",
            "[Train] batch 2164 loss 10.2312 avg_loss 12.2245\n",
            "[Train] batch 2165 loss 12.4716 avg_loss 12.2246\n",
            "[Train] batch 2166 loss 11.8815 avg_loss 12.2245\n",
            "[Train] batch 2167 loss 11.8503 avg_loss 12.2243\n",
            "[Train] batch 2168 loss 10.3745 avg_loss 12.2234\n",
            "[Train] batch 2169 loss 12.6286 avg_loss 12.2236\n",
            "[Train] batch 2170 loss 11.2353 avg_loss 12.2232\n",
            "[Train] batch 2171 loss 12.6066 avg_loss 12.2233\n",
            "[Train] batch 2172 loss 12.0787 avg_loss 12.2233\n",
            "[Train] batch 2173 loss 13.4449 avg_loss 12.2238\n",
            "[Train] batch 2174 loss 11.6882 avg_loss 12.2236\n",
            "[Train] batch 2175 loss 13.1939 avg_loss 12.2240\n",
            "[Train] batch 2176 loss 12.3209 avg_loss 12.2241\n",
            "[Train] batch 2177 loss 11.1932 avg_loss 12.2236\n",
            "[Train] batch 2178 loss 12.9798 avg_loss 12.2240\n",
            "[Train] batch 2179 loss 10.1751 avg_loss 12.2230\n",
            "[Train] batch 2180 loss 12.2189 avg_loss 12.2230\n",
            "[Train] batch 2181 loss 11.6874 avg_loss 12.2228\n",
            "[Train] batch 2182 loss 11.4469 avg_loss 12.2224\n",
            "[Train] batch 2183 loss 10.8087 avg_loss 12.2218\n",
            "[Train] batch 2184 loss 12.3538 avg_loss 12.2218\n",
            "[Train] batch 2185 loss 11.9803 avg_loss 12.2217\n",
            "[Train] batch 2186 loss 9.9097 avg_loss 12.2207\n",
            "[Train] batch 2187 loss 11.9586 avg_loss 12.2205\n",
            "[Train] batch 2188 loss 8.8667 avg_loss 12.2190\n",
            "[Train] batch 2189 loss 11.0962 avg_loss 12.2185\n",
            "[Train] batch 2190 loss 10.5433 avg_loss 12.2177\n",
            "[Train] batch 2191 loss 11.2969 avg_loss 12.2173\n",
            "[Train] batch 2192 loss 11.8916 avg_loss 12.2172\n",
            "[Train] batch 2193 loss 10.4512 avg_loss 12.2164\n",
            "[Train] batch 2194 loss 10.6801 avg_loss 12.2157\n",
            "[Train] batch 2195 loss 10.1592 avg_loss 12.2147\n",
            "[Train] batch 2196 loss 12.0073 avg_loss 12.2146\n",
            "[Train] batch 2197 loss 12.2483 avg_loss 12.2146\n",
            "[Train] batch 2198 loss 10.7487 avg_loss 12.2140\n",
            "[Train] batch 2199 loss 11.7621 avg_loss 12.2138\n",
            "[Train] batch 2200 loss 12.0296 avg_loss 12.2137\n",
            "[Train] batch 2201 loss 11.2050 avg_loss 12.2132\n",
            "[Train] batch 2202 loss 11.6641 avg_loss 12.2130\n",
            "[Train] batch 2203 loss 9.0939 avg_loss 12.2116\n",
            "[Train] batch 2204 loss 11.4512 avg_loss 12.2112\n",
            "[Train] batch 2205 loss 11.9493 avg_loss 12.2111\n",
            "[Train] batch 2206 loss 12.8866 avg_loss 12.2114\n",
            "[Train] batch 2207 loss 9.7957 avg_loss 12.2103\n",
            "[Train] batch 2208 loss 12.9936 avg_loss 12.2107\n",
            "[Train] batch 2209 loss 11.6711 avg_loss 12.2104\n",
            "[Train] batch 2210 loss 12.4473 avg_loss 12.2105\n",
            "[Train] batch 2211 loss 13.5107 avg_loss 12.2111\n",
            "[Train] batch 2212 loss 11.9317 avg_loss 12.2110\n",
            "[Train] batch 2213 loss 13.0238 avg_loss 12.2114\n",
            "[Train] batch 2214 loss 10.4550 avg_loss 12.2106\n",
            "[Train] batch 2215 loss 10.5896 avg_loss 12.2098\n",
            "[Train] batch 2216 loss 10.4090 avg_loss 12.2090\n",
            "[Train] batch 2217 loss 11.8509 avg_loss 12.2089\n",
            "[Train] batch 2218 loss 13.5486 avg_loss 12.2095\n",
            "[Train] batch 2219 loss 11.6244 avg_loss 12.2092\n",
            "[Train] batch 2220 loss 14.9960 avg_loss 12.2104\n",
            "[Train] batch 2221 loss 11.6614 avg_loss 12.2102\n",
            "[Train] batch 2222 loss 12.5464 avg_loss 12.2104\n",
            "[Train] batch 2223 loss 11.1694 avg_loss 12.2099\n",
            "[Train] batch 2224 loss 11.7165 avg_loss 12.2097\n",
            "[Train] batch 2225 loss 11.4179 avg_loss 12.2093\n",
            "[Train] batch 2226 loss 12.7450 avg_loss 12.2095\n",
            "[Train] batch 2227 loss 9.6896 avg_loss 12.2084\n",
            "[Train] batch 2228 loss 11.5051 avg_loss 12.2081\n",
            "[Train] batch 2229 loss 12.5480 avg_loss 12.2083\n",
            "[Train] batch 2230 loss 12.8574 avg_loss 12.2085\n",
            "[Train] batch 2231 loss 12.5061 avg_loss 12.2087\n",
            "[Train] batch 2232 loss 13.0797 avg_loss 12.2091\n",
            "[Train] batch 2233 loss 10.2401 avg_loss 12.2082\n",
            "[Train] batch 2234 loss 11.1906 avg_loss 12.2077\n",
            "[Train] batch 2235 loss 12.5609 avg_loss 12.2079\n",
            "[Train] batch 2236 loss 12.8592 avg_loss 12.2082\n",
            "[Train] batch 2237 loss 11.6396 avg_loss 12.2079\n",
            "[Train] batch 2238 loss 10.6114 avg_loss 12.2072\n",
            "[Train] batch 2239 loss 12.0856 avg_loss 12.2072\n",
            "[Train] batch 2240 loss 10.5148 avg_loss 12.2064\n",
            "[Train] batch 2241 loss 11.5293 avg_loss 12.2061\n",
            "[Train] batch 2242 loss 11.3817 avg_loss 12.2057\n",
            "[Train] batch 2243 loss 12.4698 avg_loss 12.2058\n",
            "[Train] batch 2244 loss 11.0084 avg_loss 12.2053\n",
            "[Train] batch 2245 loss 10.1596 avg_loss 12.2044\n",
            "[Train] batch 2246 loss 11.2393 avg_loss 12.2040\n",
            "[Train] batch 2247 loss 12.5293 avg_loss 12.2041\n",
            "[Train] batch 2248 loss 10.4968 avg_loss 12.2034\n",
            "[Train] batch 2249 loss 12.6516 avg_loss 12.2036\n",
            "[Train] batch 2250 loss 12.4960 avg_loss 12.2037\n",
            "[Train] batch 2251 loss 9.9110 avg_loss 12.2027\n",
            "[Train] batch 2252 loss 12.2938 avg_loss 12.2027\n",
            "[Train] batch 2253 loss 10.9154 avg_loss 12.2021\n",
            "[Train] batch 2254 loss 11.5495 avg_loss 12.2019\n",
            "[Train] batch 2255 loss 12.0144 avg_loss 12.2018\n",
            "[Train] batch 2256 loss 11.2341 avg_loss 12.2013\n",
            "[Train] batch 2257 loss 9.7540 avg_loss 12.2003\n",
            "[Train] batch 2258 loss 10.9064 avg_loss 12.1997\n",
            "[Train] batch 2259 loss 11.0469 avg_loss 12.1992\n",
            "[Train] batch 2260 loss 11.4361 avg_loss 12.1988\n",
            "[Train] batch 2261 loss 11.6698 avg_loss 12.1986\n",
            "[Train] batch 2262 loss 10.6498 avg_loss 12.1979\n",
            "[Train] batch 2263 loss 10.5026 avg_loss 12.1972\n",
            "[Train] batch 2264 loss 12.9164 avg_loss 12.1975\n",
            "[Train] batch 2265 loss 9.5632 avg_loss 12.1963\n",
            "[Train] batch 2266 loss 10.9695 avg_loss 12.1958\n",
            "[Train] batch 2267 loss 11.9317 avg_loss 12.1957\n",
            "[Train] batch 2268 loss 14.0059 avg_loss 12.1965\n",
            "[Train] batch 2269 loss 9.7865 avg_loss 12.1954\n",
            "[Train] batch 2270 loss 11.6927 avg_loss 12.1952\n",
            "[Train] batch 2271 loss 12.2953 avg_loss 12.1952\n",
            "[Train] batch 2272 loss 10.5884 avg_loss 12.1945\n",
            "[Train] batch 2273 loss 12.8876 avg_loss 12.1948\n",
            "[Train] batch 2274 loss 9.2517 avg_loss 12.1935\n",
            "[Train] batch 2275 loss 12.4932 avg_loss 12.1937\n",
            "[Train] batch 2276 loss 12.2973 avg_loss 12.1937\n",
            "[Train] batch 2277 loss 12.1515 avg_loss 12.1937\n",
            "[Train] batch 2278 loss 11.5389 avg_loss 12.1934\n",
            "[Train] batch 2279 loss 11.0582 avg_loss 12.1929\n",
            "[Train] batch 2280 loss 10.5543 avg_loss 12.1922\n",
            "[Train] batch 2281 loss 12.9687 avg_loss 12.1925\n",
            "[Train] batch 2282 loss 11.1860 avg_loss 12.1921\n",
            "[Train] batch 2283 loss 11.3271 avg_loss 12.1917\n",
            "[Train] batch 2284 loss 11.3380 avg_loss 12.1913\n",
            "[Train] batch 2285 loss 11.0766 avg_loss 12.1908\n",
            "[Train] batch 2286 loss 11.6085 avg_loss 12.1906\n",
            "[Train] batch 2287 loss 11.6407 avg_loss 12.1903\n",
            "[Train] batch 2288 loss 11.1886 avg_loss 12.1899\n",
            "[Train] batch 2289 loss 10.3187 avg_loss 12.1891\n",
            "[Train] batch 2290 loss 12.2846 avg_loss 12.1891\n",
            "[Train] batch 2291 loss 14.7742 avg_loss 12.1903\n",
            "[Train] batch 2292 loss 11.2200 avg_loss 12.1898\n",
            "[Train] batch 2293 loss 11.6752 avg_loss 12.1896\n",
            "[Train] batch 2294 loss 12.9280 avg_loss 12.1899\n",
            "[Train] batch 2295 loss 11.9995 avg_loss 12.1898\n",
            "[Train] batch 2296 loss 11.0260 avg_loss 12.1893\n",
            "[Train] batch 2297 loss 10.9851 avg_loss 12.1888\n",
            "[Train] batch 2298 loss 10.3966 avg_loss 12.1880\n",
            "[Train] batch 2299 loss 11.8267 avg_loss 12.1879\n",
            "[Train] batch 2300 loss 13.0006 avg_loss 12.1882\n",
            "[Train] batch 2301 loss 11.2085 avg_loss 12.1878\n",
            "[Train] batch 2302 loss 10.8939 avg_loss 12.1872\n",
            "[Train] batch 2303 loss 11.4847 avg_loss 12.1869\n",
            "[Train] batch 2304 loss 10.5926 avg_loss 12.1862\n",
            "[Train] batch 2305 loss 10.9636 avg_loss 12.1857\n",
            "[Train] batch 2306 loss 11.8579 avg_loss 12.1856\n",
            "[Train] batch 2307 loss 11.6565 avg_loss 12.1853\n",
            "[Train] batch 2308 loss 12.8887 avg_loss 12.1857\n",
            "[Train] batch 2309 loss 10.8575 avg_loss 12.1851\n",
            "[Train] batch 2310 loss 12.7760 avg_loss 12.1853\n",
            "[Train] batch 2311 loss 9.3783 avg_loss 12.1841\n",
            "[Train] batch 2312 loss 10.7353 avg_loss 12.1835\n",
            "[Train] batch 2313 loss 11.1284 avg_loss 12.1830\n",
            "[Train] batch 2314 loss 12.5882 avg_loss 12.1832\n",
            "[Train] batch 2315 loss 13.0697 avg_loss 12.1836\n",
            "[Train] batch 2316 loss 13.4921 avg_loss 12.1842\n",
            "[Train] batch 2317 loss 10.8513 avg_loss 12.1836\n",
            "[Train] batch 2318 loss 11.2758 avg_loss 12.1832\n",
            "[Train] batch 2319 loss 11.2403 avg_loss 12.1828\n",
            "[Train] batch 2320 loss 12.6343 avg_loss 12.1830\n",
            "[Train] batch 2321 loss 11.1346 avg_loss 12.1825\n",
            "[Train] batch 2322 loss 11.5489 avg_loss 12.1823\n",
            "[Train] batch 2323 loss 11.8521 avg_loss 12.1821\n",
            "[Train] batch 2324 loss 10.2798 avg_loss 12.1813\n",
            "[Train] batch 2325 loss 10.7547 avg_loss 12.1807\n",
            "[Train] batch 2326 loss 10.8429 avg_loss 12.1801\n",
            "[Train] batch 2327 loss 11.6918 avg_loss 12.1799\n",
            "[Train] batch 2328 loss 11.8615 avg_loss 12.1798\n",
            "[Train] batch 2329 loss 11.4281 avg_loss 12.1794\n",
            "[Train] batch 2330 loss 12.3653 avg_loss 12.1795\n",
            "[Train] batch 2331 loss 11.5471 avg_loss 12.1792\n",
            "[Train] batch 2332 loss 12.0880 avg_loss 12.1792\n",
            "[Train] batch 2333 loss 10.2570 avg_loss 12.1784\n",
            "[Train] batch 2334 loss 10.3714 avg_loss 12.1776\n",
            "[Train] batch 2335 loss 13.0261 avg_loss 12.1780\n",
            "[Train] batch 2336 loss 10.4465 avg_loss 12.1772\n",
            "[Train] batch 2337 loss 10.4128 avg_loss 12.1765\n",
            "[Train] batch 2338 loss 13.5396 avg_loss 12.1771\n",
            "[Train] batch 2339 loss 12.1338 avg_loss 12.1770\n",
            "[Train] batch 2340 loss 11.5075 avg_loss 12.1768\n",
            "[Train] batch 2341 loss 11.3885 avg_loss 12.1764\n",
            "[Train] batch 2342 loss 9.7017 avg_loss 12.1754\n",
            "[Train] batch 2343 loss 10.0936 avg_loss 12.1745\n",
            "[Train] batch 2344 loss 10.5598 avg_loss 12.1738\n",
            "[Train] batch 2345 loss 12.7835 avg_loss 12.1740\n",
            "[Train] batch 2346 loss 11.0410 avg_loss 12.1736\n",
            "[Train] batch 2347 loss 10.7975 avg_loss 12.1730\n",
            "[Train] batch 2348 loss 11.6894 avg_loss 12.1728\n",
            "[Train] batch 2349 loss 10.4033 avg_loss 12.1720\n",
            "[Train] batch 2350 loss 11.3374 avg_loss 12.1717\n",
            "[Train] batch 2351 loss 10.8527 avg_loss 12.1711\n",
            "[Train] batch 2352 loss 11.9858 avg_loss 12.1710\n",
            "[Train] batch 2353 loss 12.4514 avg_loss 12.1711\n",
            "[Train] batch 2354 loss 10.8317 avg_loss 12.1706\n",
            "[Train] batch 2355 loss 12.6362 avg_loss 12.1708\n",
            "[Train] batch 2356 loss 10.3295 avg_loss 12.1700\n",
            "[Train] batch 2357 loss 10.6564 avg_loss 12.1693\n",
            "[Train] batch 2358 loss 12.6763 avg_loss 12.1696\n",
            "[Train] batch 2359 loss 11.4920 avg_loss 12.1693\n",
            "[Train] batch 2360 loss 10.8542 avg_loss 12.1687\n",
            "[Train] batch 2361 loss 11.6260 avg_loss 12.1685\n",
            "[Train] batch 2362 loss 11.8216 avg_loss 12.1683\n",
            "[Train] batch 2363 loss 10.3563 avg_loss 12.1676\n",
            "[Train] batch 2364 loss 13.2420 avg_loss 12.1680\n",
            "[Train] batch 2365 loss 12.9829 avg_loss 12.1684\n",
            "[Train] batch 2366 loss 12.6213 avg_loss 12.1686\n",
            "[Train] batch 2367 loss 12.5449 avg_loss 12.1687\n",
            "[Train] batch 2368 loss 11.7486 avg_loss 12.1685\n",
            "[Train] batch 2369 loss 13.2843 avg_loss 12.1690\n",
            "[Train] batch 2370 loss 11.9843 avg_loss 12.1689\n",
            "[Train] batch 2371 loss 11.1336 avg_loss 12.1685\n",
            "[Train] batch 2372 loss 11.8107 avg_loss 12.1683\n",
            "[Train] batch 2373 loss 11.8646 avg_loss 12.1682\n",
            "[Train] batch 2374 loss 11.0091 avg_loss 12.1677\n",
            "[Train] batch 2375 loss 11.7519 avg_loss 12.1676\n",
            "[Train] batch 2376 loss 11.5549 avg_loss 12.1673\n",
            "[Train] batch 2377 loss 10.2598 avg_loss 12.1665\n",
            "[Train] batch 2378 loss 12.2281 avg_loss 12.1665\n",
            "[Train] batch 2379 loss 10.4876 avg_loss 12.1658\n",
            "[Train] batch 2380 loss 11.3096 avg_loss 12.1655\n",
            "[Train] batch 2381 loss 10.7792 avg_loss 12.1649\n",
            "[Train] batch 2382 loss 10.3940 avg_loss 12.1641\n",
            "[Train] batch 2383 loss 10.7839 avg_loss 12.1636\n",
            "[Train] batch 2384 loss 10.8405 avg_loss 12.1630\n",
            "[Train] batch 2385 loss 11.7929 avg_loss 12.1628\n",
            "[Train] batch 2386 loss 10.0463 avg_loss 12.1620\n",
            "[Train] batch 2387 loss 11.8589 avg_loss 12.1618\n",
            "[Train] batch 2388 loss 12.2454 avg_loss 12.1619\n",
            "[Train] batch 2389 loss 10.2777 avg_loss 12.1611\n",
            "[Train] batch 2390 loss 12.0576 avg_loss 12.1610\n",
            "[Train] batch 2391 loss 13.0079 avg_loss 12.1614\n",
            "[Train] batch 2392 loss 12.7348 avg_loss 12.1616\n",
            "[Train] batch 2393 loss 11.0006 avg_loss 12.1611\n",
            "[Train] batch 2394 loss 12.1147 avg_loss 12.1611\n",
            "[Train] batch 2395 loss 11.9098 avg_loss 12.1610\n",
            "[Train] batch 2396 loss 11.1348 avg_loss 12.1606\n",
            "[Train] batch 2397 loss 10.7496 avg_loss 12.1600\n",
            "[Train] batch 2398 loss 14.4642 avg_loss 12.1610\n",
            "[Train] batch 2399 loss 9.9116 avg_loss 12.1600\n",
            "[Train] batch 2400 loss 10.8157 avg_loss 12.1595\n",
            "[Train] batch 2401 loss 10.0071 avg_loss 12.1586\n",
            "[Train] batch 2402 loss 11.0074 avg_loss 12.1581\n",
            "[Train] batch 2403 loss 11.2916 avg_loss 12.1577\n",
            "[Train] batch 2404 loss 11.5554 avg_loss 12.1575\n",
            "[Train] batch 2405 loss 10.8866 avg_loss 12.1569\n",
            "[Train] batch 2406 loss 11.1613 avg_loss 12.1565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 회고\n",
        "\n",
        "LMS에서 하려고 했는데 메모리 부족으로 데이터셋 다운로드가 안돼서 코랩에서 했습니다.  \n",
        "코랩에서는 경로 문제가 있어서 경로는 직접 수정해 해결 했습니다.\n",
        "그 후 잘 하다가 학습을 돌렸는데, 한시간이나 걸린다는걸 노드를 통해 알았습니다.  \n",
        "그런데 데이터셋도 용량이 커서 다운로드, 압축 해제에만 시간이 꽤 걸리고 학습 코드 외에도 실행하는데 시간이 좀 걸리는 코드를이 있어서 학습 1에폭이 끝나자마자 T4그래픽카드 사용 시간이 끝났습니다.  \n",
        "  \n",
        "레이 라이브러리를 임포트 하는데 한번 안되더라구요. pip를 안해서 그런가 했는데 pip도 이미 했었는데도 안되는거더라구요. 이때가 LMS에서 하고 있었을 때였는데 데이터셋도 하리, 라이브러리 다운로드도 하리 해서 메모리가 터지려고 하는거 같습니다. 콟에서 하면 잘 됐습니다.\n",
        "\n",
        "근데 그래픽카드를 사용시간이 다 돼서 따로 코드를 방법이 없어서 여기서 더 하지는 못했습니다."
      ],
      "metadata": {
        "id": "8CBN3FeM5NWy"
      }
    }
  ]
}